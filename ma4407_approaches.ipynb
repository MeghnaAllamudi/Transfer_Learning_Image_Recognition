{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3",
      "metadata": {
        "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c",
      "metadata": {
        "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c"
      },
      "outputs": [],
      "source": [
        "# Create Dataset class for multilabel classification\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.ann_df = ann_df\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.ann_df['image'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        super_idx = self.ann_df['superclass_index'][idx]\n",
        "        super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "        sub_idx = self.ann_df['subclass_index'][idx]\n",
        "        sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, super_idx, super_label, sub_idx, sub_label\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): # Count files in img_dir\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "60tzy4N0CoKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86997947-c73c-4317-8421-57b17d87ad02"
      },
      "id": "60tzy4N0CoKA",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ann_df = pd.read_csv('/content/drive/My Drive/train_data.csv')\n",
        "super_map_df = pd.read_csv('/content/drive/My Drive/superclass_mapping.csv')\n",
        "sub_map_df = pd.read_csv('/content/drive/My Drive/subclass_mapping.csv')"
      ],
      "metadata": {
        "id": "a6LV0RMPE2HJ"
      },
      "id": "a6LV0RMPE2HJ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e7398553-8842-4ad8-b348-767921a22482",
      "metadata": {
        "id": "e7398553-8842-4ad8-b348-767921a22482",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d4bd6d-47b9-42fa-8d07-de1b4bc18895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching images locally from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6288/6288 [01:39<00:00, 63.25it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached 6288 images to local storage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Test Dataset\n",
        "#test_ann_df = pd.read_csv('/content/drive/My Drive/test_data.csv')\n",
        "\n",
        "train_img_dir = '/content/drive/My Drive/train_images/train_images/'\n",
        "test_img_dir = '/content/drive/My Drive/test_images/test_images/'\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "# Create a local cache directory\n",
        "local_cache_dir = \"/content/local_train_image_cache\"\n",
        "os.makedirs(local_cache_dir, exist_ok=True)\n",
        "\n",
        "# Copy your dataset from Google Drive to local storage once\n",
        "if len(os.listdir(local_cache_dir)) == 0:  # Only copy if cache is empty\n",
        "    print(\"Caching images locally from Google Drive...\")\n",
        "    source_dir = train_img_dir\n",
        "\n",
        "    # Get list of image files\n",
        "    image_files = [f for f in os.listdir(source_dir)\n",
        "                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # Copy files with progress bar\n",
        "    for img in tqdm(image_files):\n",
        "        shutil.copy(os.path.join(source_dir, img),\n",
        "                   os.path.join(local_cache_dir, img))\n",
        "\n",
        "    print(f\"Cached {len(image_files)} images to local storage\")\n",
        "\n",
        "\n",
        "image_preprocessing = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0), std=(1)),\n",
        "])\n",
        "\n",
        "# Update this in your original code where you define image_preprocessing\n",
        "# image_preprocessing = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),  # Resize to 224x224 for ViT\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
        "# ])\n",
        "\n",
        "# Create train and val split\n",
        "full_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, local_cache_dir, transform=image_preprocessing)\n",
        "train_dataset, val_dataset = random_split(full_dataset, [0.9, 0.1])\n",
        "\n",
        "#Create test dataset\n",
        "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=1,\n",
        "                         shuffle=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Novelty Detection with Energy Stats And Best Balanced Threshold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size=64, num_superclasses=4, num_subclasses=88):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_size = input_size // (2**3)\n",
        "\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(self.feature_size * self.feature_size * 128, 256)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "\n",
        "        self.fc3a = nn.Linear(128, num_superclasses)\n",
        "        self.fc3b = nn.Linear(128, num_subclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "\n",
        "        return super_out, sub_out\n",
        "\n",
        "    def get_features(self, x):\n",
        "        \"\"\"Extract features before the final classification layer\"\"\"\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class NoveltyDetectionTrainer:\n",
        "    def __init__(self, full_dataset, image_preprocessing, device='cuda', batch_size=64):\n",
        "        self.full_dataset = full_dataset\n",
        "        self.image_preprocessing = image_preprocessing\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "        self.energy_mean = 0\n",
        "        self.energy_std = 1\n",
        "\n",
        "\n",
        "        self.superclass_indices = set()\n",
        "        for i in range(len(full_dataset)):\n",
        "            _, super_idx, _, _, _ = full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "            self.superclass_indices.add(super_idx)\n",
        "\n",
        "        self.superclass_indices = sorted(list(self.superclass_indices))\n",
        "        print(f\"Found superclasses with indices: {self.superclass_indices}\")\n",
        "\n",
        "    def cross_validate_novelty_detection(self, epochs=5, confidence_threshold=0.0):\n",
        "        results = []\n",
        "\n",
        "        for fold, novel_idx in enumerate(self.superclass_indices):\n",
        "            print(f\"\\n=== Fold {fold+1}/{len(self.superclass_indices)}: Treating superclass {novel_idx} as novel ===\")\n",
        "\n",
        "            known_indices, novel_indices = self._split_by_superclass(novel_idx)\n",
        "\n",
        "            np.random.shuffle(known_indices)\n",
        "            train_size = int(0.9 * len(known_indices))\n",
        "            train_indices = known_indices[:train_size]\n",
        "            val_known_indices = known_indices[train_size:]\n",
        "\n",
        "            train_dataset = Subset(self.full_dataset, train_indices)\n",
        "            val_known_dataset = Subset(self.full_dataset, val_known_indices)\n",
        "            val_novel_dataset = Subset(self.full_dataset, novel_indices)\n",
        "\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "            val_known_loader = DataLoader(val_known_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "            val_novel_loader = DataLoader(val_novel_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "            model = CNN(input_size=64, num_superclasses=len(self.superclass_indices)+1).to(self.device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "            self._train_model(model, criterion, optimizer, train_loader, epochs)\n",
        "\n",
        "\n",
        "            self._calibrate_energy_stats(model, train_loader)\n",
        "\n",
        "\n",
        "            metrics = self._evaluate_novelty_detection(model, val_known_loader, val_novel_loader, confidence_threshold)\n",
        "            results.append(metrics)\n",
        "\n",
        "            print(f\"Fold {fold+1} results:\")\n",
        "            for key, value in metrics.items():\n",
        "                print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "\n",
        "        avg_results = {}\n",
        "        for key in results[0].keys():\n",
        "            avg_results[key] = sum(r[key] for r in results) / len(results)\n",
        "\n",
        "\n",
        "        for key, value in avg_results.items():\n",
        "            print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "        return avg_results, results\n",
        "\n",
        "    def find_optimal_threshold(self, fold_index=0, threshold_range=np.arange(-3.0, 3.0, 0.1)):\n",
        "        novel_idx = self.superclass_indices[fold_index]\n",
        "        print(f\"\\n=== Finding optimal threshold for fold {fold_index+1}: Superclass {novel_idx} as novel ===\")\n",
        "\n",
        "\n",
        "        known_indices, novel_indices = self._split_by_superclass(novel_idx)\n",
        "\n",
        "\n",
        "        np.random.shuffle(known_indices)\n",
        "        train_size = int(0.9 * len(known_indices))\n",
        "        train_indices = known_indices[:train_size]\n",
        "        val_known_indices = known_indices[train_size:]\n",
        "\n",
        "\n",
        "        train_dataset = Subset(self.full_dataset, train_indices)\n",
        "        val_known_dataset = Subset(self.full_dataset, val_known_indices)\n",
        "        val_novel_dataset = Subset(self.full_dataset, novel_indices)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_known_loader = DataLoader(val_known_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        val_novel_loader = DataLoader(val_novel_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "        model = CNN(input_size=64, num_superclasses=len(self.superclass_indices)+1).to(self.device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "        self._train_model(model, criterion, optimizer, train_loader, epochs=5)\n",
        "\n",
        "\n",
        "        self._calibrate_energy_stats(model, train_loader)\n",
        "\n",
        "\n",
        "        known_energies, novel_energies = self._collect_energies(model, val_known_loader, val_novel_loader)\n",
        "\n",
        "\n",
        "        results = []\n",
        "        for threshold in threshold_range:\n",
        "\n",
        "            known_correct = sum(1 for e in known_energies if e <= threshold)\n",
        "            known_accuracy = known_correct / len(known_energies) if known_energies else 0\n",
        "\n",
        "\n",
        "            novel_correct = sum(1 for e in novel_energies if e > threshold)\n",
        "            novel_accuracy = novel_correct / len(novel_energies) if novel_energies else 0\n",
        "\n",
        "            balanced_accuracy = (known_accuracy + novel_accuracy) / 2\n",
        "\n",
        "            results.append({\n",
        "                'threshold': threshold,\n",
        "                'known_accuracy': known_accuracy,\n",
        "                'novel_accuracy': novel_accuracy,\n",
        "                'balanced_accuracy': balanced_accuracy\n",
        "            })\n",
        "\n",
        "            print(f\"Threshold {threshold:.2f}: Known Acc={known_accuracy:.4f}, Novel Acc={novel_accuracy:.4f}, Balanced Acc={balanced_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "        best_result = max(results, key=lambda x: x['balanced_accuracy'])\n",
        "\n",
        "        print(f\"\\nBest threshold: {best_result['threshold']:.2f}\")\n",
        "        print(f\"Known accuracy: {best_result['known_accuracy']:.4f}\")\n",
        "        print(f\"Novel accuracy: {best_result['novel_accuracy']:.4f}\")\n",
        "        print(f\"Balanced accuracy: {best_result['balanced_accuracy']:.4f}\")\n",
        "\n",
        "        return best_result['threshold'], results\n",
        "\n",
        "    def _calibrate_energy_stats(self, model, loader):\n",
        "      \"\"\"Calculate energy statistics on a dataset for normalization\"\"\"\n",
        "      model.eval()\n",
        "      all_energies = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for data in loader:\n",
        "              inputs = data[0].to(self.device)\n",
        "\n",
        "\n",
        "              super_outputs, _ = model(inputs)\n",
        "\n",
        "\n",
        "              energies = -torch.logsumexp(super_outputs, dim=1)\n",
        "              all_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "      all_energies = np.array(all_energies)\n",
        "      self.energy_mean = float(np.mean(all_energies))\n",
        "      self.energy_std = float(np.std(all_energies) + 1e-6)  # Add epsilon to avoid division by zero\n",
        "\n",
        "      print(f\"Calibrated energy statistics: mean={self.energy_mean:.4f}, std={self.energy_std:.4f}\")\n",
        "\n",
        "    def _compute_normalized_energy(self, logits):\n",
        "\n",
        "\n",
        "      raw_energy = -torch.logsumexp(logits, dim=1)\n",
        "\n",
        "\n",
        "      normalized_energy = (raw_energy - self.energy_mean) / self.energy_std\n",
        "\n",
        "      return normalized_energy\n",
        "\n",
        "    def _split_by_superclass(self, novel_superclass_idx):\n",
        "        \"\"\"Split dataset indices into known and novel based on superclass\"\"\"\n",
        "        known_indices = []\n",
        "        novel_indices = []\n",
        "\n",
        "        for i in range(len(self.full_dataset)):\n",
        "            _, super_idx, _, _, _ = self.full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "\n",
        "            if super_idx == novel_superclass_idx:\n",
        "                novel_indices.append(i)\n",
        "            else:\n",
        "                known_indices.append(i)\n",
        "\n",
        "        return known_indices, novel_indices\n",
        "\n",
        "    def _train_model(self, model, criterion, optimizer, train_loader, epochs):\n",
        "        \"\"\"Train the model on known classes\"\"\"\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(train_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                super_labels = super_labels.to(self.device)\n",
        "                sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                super_outputs, sub_outputs = model(inputs)\n",
        "                loss = criterion(super_outputs, super_labels) + criterion(sub_outputs, sub_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "    def _evaluate_novelty_detection(self, model, known_loader, novel_loader, threshold):\n",
        "      \"\"\"Evaluate novelty detection performance using balanced ensemble approach.\"\"\"\n",
        "      model.eval()\n",
        "\n",
        "\n",
        "      self._calibrate_energy_stats(model, known_loader)\n",
        "\n",
        "      def eval_loader(loader, is_novel):\n",
        "          super_correct, sub_correct = 0, 0\n",
        "          super_total, sub_total = 0, 0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for data in loader:\n",
        "                  inputs, _, _, _, _ = data\n",
        "                  inputs = inputs.to(self.device)\n",
        "\n",
        "                  super_outputs, sub_outputs = model(inputs)\n",
        "\n",
        "\n",
        "                  super_energies = self._compute_normalized_energy(super_outputs)\n",
        "                  energy_novel = super_energies > threshold\n",
        "\n",
        "\n",
        "                  super_probs = F.softmax(super_outputs, dim=1)\n",
        "                  super_confidences, _ = torch.max(super_probs, dim=1)\n",
        "                  confidence_novel = super_confidences < 0.7\n",
        "\n",
        "\n",
        "                  energy_weight = 0.6\n",
        "                  confidence_weight = 0.4\n",
        "\n",
        "                  novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "\n",
        "                  is_novel_super = novelty_score > 0.5\n",
        "\n",
        "                  sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "                  sub_confidences, _ = torch.max(sub_probs, dim=1)\n",
        "                  is_novel_sub = sub_confidences < 0.5\n",
        "\n",
        "                  if is_novel:\n",
        "                      super_correct += is_novel_super.sum().item()\n",
        "                      sub_correct += is_novel_sub.sum().item()\n",
        "                  else:\n",
        "                      super_correct += (~is_novel_super).sum().item()\n",
        "                      sub_correct += (~is_novel_sub).sum().item()\n",
        "\n",
        "                  super_total += inputs.size(0)\n",
        "                  sub_total += inputs.size(0)\n",
        "\n",
        "          return (\n",
        "              super_correct / super_total if super_total else 0,\n",
        "              sub_correct / sub_total if sub_total else 0\n",
        "          )\n",
        "\n",
        "\n",
        "      known_super_acc, known_sub_acc = eval_loader(known_loader, is_novel=False)\n",
        "      novel_super_acc, novel_sub_acc = eval_loader(novel_loader, is_novel=True)\n",
        "\n",
        "      balanced_super_acc = (known_super_acc + novel_super_acc) / 2\n",
        "      balanced_sub_acc = (known_sub_acc + novel_sub_acc) / 2\n",
        "\n",
        "      return {\n",
        "          'known_superclass_accuracy': known_super_acc,\n",
        "          'novel_superclass_accuracy': novel_super_acc,\n",
        "          'balanced_superclass_accuracy': balanced_super_acc,\n",
        "          'known_subclass_accuracy': known_sub_acc,\n",
        "          'novel_subclass_accuracy': novel_sub_acc,\n",
        "          'balanced_subclass_accuracy': balanced_sub_acc\n",
        "      }\n",
        "\n",
        "\n",
        "    def _collect_energies(self, model, known_loader, novel_loader):\n",
        "        \"\"\"Collect normalized energy scores for known and novel classes\"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        known_energies = []\n",
        "        novel_energies = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for data in known_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                super_outputs, _ = model(inputs)\n",
        "                energies = self._compute_normalized_energy(super_outputs)\n",
        "                known_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "\n",
        "            for data in novel_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                super_outputs, _ = model(inputs)\n",
        "                energies = self._compute_normalized_energy(super_outputs)\n",
        "                novel_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "        return known_energies, novel_energies\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        self.energy_mean = 0\n",
        "        self.energy_std = 1\n",
        "        self.energy_calibrated = False\n",
        "\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "        )\n",
        "\n",
        "\n",
        "        self.temperature = 1.5\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/(i+1):.3f}')\n",
        "        avg_loss = running_loss/(i+1)\n",
        "        self.scheduler.step(avg_loss)\n",
        "        return avg_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self._calibrate_energy_stats()\n",
        "\n",
        "    def _calibrate_energy_stats(self):\n",
        "        \"\"\"Calculate energy statistics on training data for normalization\"\"\"\n",
        "        self.model.eval()\n",
        "        all_energies = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in self.train_loader:\n",
        "                inputs = data[0].to(self.device)\n",
        "\n",
        "\n",
        "                super_outputs, _ = self.model(inputs)\n",
        "\n",
        "\n",
        "                energies = -torch.logsumexp(super_outputs, dim=1)\n",
        "                all_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "\n",
        "        all_energies = np.array(all_energies)\n",
        "        self.energy_mean = float(np.mean(all_energies))\n",
        "        self.energy_std = float(np.std(all_energies) + 1e-6)\n",
        "        self.energy_calibrated = True\n",
        "\n",
        "        print(f\"Calibrated energy statistics: mean={self.energy_mean:.4f}, std={self.energy_std:.4f}\")\n",
        "\n",
        "    def compute_normalized_energy(self, logits):\n",
        "\n",
        "\n",
        "        raw_energy = -torch.logsumexp(logits, dim=1)\n",
        "\n",
        "\n",
        "        if not self.energy_calibrated:\n",
        "\n",
        "            print(\"Warning: Energy statistics not calibrated, using raw energy\")\n",
        "            return raw_energy\n",
        "\n",
        "        normalized_energy = (raw_energy - self.energy_mean) / self.energy_std\n",
        "\n",
        "        return normalized_energy\n",
        "\n",
        "    def validate_epoch(self, novel_superclass_idx=3, novel_subclass_idx=87, confidence_threshold=0.0, temperature=1.0):\n",
        "\n",
        "      # Make sure energy statistics are calibrated\n",
        "      if not self.energy_calibrated:\n",
        "          self._calibrate_energy_stats()\n",
        "\n",
        "      self.model.eval()\n",
        "\n",
        "      # Metrics to track\n",
        "      correct_with_novelty = 0\n",
        "      super_correct_standard = 0\n",
        "      sub_correct = 0\n",
        "\n",
        "      novel_total = 0\n",
        "      known_total = 0\n",
        "      novel_correct = 0\n",
        "      known_correct = 0\n",
        "\n",
        "      total = 0\n",
        "\n",
        "      novel_super_predictions = 0\n",
        "      novel_sub_predictions = 0\n",
        "\n",
        "      all_super_energies = []\n",
        "      all_sub_confidences = []\n",
        "\n",
        "      running_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for i, data in enumerate(self.val_loader):\n",
        "              inputs, super_labels, _, sub_labels, _ = data\n",
        "              inputs = inputs.to(self.device)\n",
        "              super_labels = super_labels.to(self.device)\n",
        "              sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "              super_outputs, sub_outputs = self.model(inputs)\n",
        "\n",
        "\n",
        "              super_energies = self.compute_normalized_energy(super_outputs)\n",
        "              energy_novel = super_energies > confidence_threshold\n",
        "\n",
        "\n",
        "              super_probs = F.softmax(super_outputs, dim=1)\n",
        "              super_confidences, super_predicted = torch.max(super_probs, dim=1)\n",
        "\n",
        "              conf_threshold = 0.7\n",
        "              confidence_novel = super_confidences < conf_threshold\n",
        "\n",
        "              energy_weight = 0.6\n",
        "              confidence_weight = 0.4\n",
        "\n",
        "\n",
        "              novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "\n",
        "\n",
        "              decision_threshold = 0.5\n",
        "              novel_super_mask = novelty_score > decision_threshold\n",
        "\n",
        "\n",
        "              final_super_preds = torch.where(\n",
        "                  novel_super_mask,\n",
        "                  torch.full_like(super_predicted, novel_superclass_idx),\n",
        "                  super_predicted\n",
        "              )\n",
        "\n",
        "\n",
        "              sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "              sub_confidences, sub_predicted = torch.max(sub_probs, dim=1)\n",
        "              sub_threshold = 0.5\n",
        "              novel_sub_mask = sub_confidences < sub_threshold\n",
        "\n",
        "              final_sub_preds = torch.where(\n",
        "                  novel_sub_mask,\n",
        "                  torch.full_like(sub_predicted, novel_subclass_idx),\n",
        "                  sub_predicted\n",
        "              )\n",
        "\n",
        "\n",
        "              total += super_labels.size(0)\n",
        "\n",
        "\n",
        "              correct_with_novelty += (final_super_preds == super_labels).sum().item()\n",
        "              super_correct_standard += (super_predicted == super_labels).sum().item()\n",
        "              sub_correct += (final_sub_preds == sub_labels).sum().item()\n",
        "\n",
        "\n",
        "              is_novel_label = super_labels == novel_superclass_idx\n",
        "              novel_total += is_novel_label.sum().item()\n",
        "              known_total += (~is_novel_label).sum().item()\n",
        "\n",
        "              novel_correct += ((final_super_preds == super_labels) & is_novel_label).sum().item()\n",
        "              known_correct += ((final_super_preds == super_labels) & ~is_novel_label).sum().item()\n",
        "\n",
        "\n",
        "              novel_super_predictions += novel_super_mask.sum().item()\n",
        "              novel_sub_predictions += novel_sub_mask.sum().item()\n",
        "\n",
        "\n",
        "              all_super_energies.extend(super_energies.cpu().numpy())\n",
        "              all_sub_confidences.extend(sub_confidences.cpu().numpy())\n",
        "\n",
        "\n",
        "              loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "              running_loss += loss.item()\n",
        "\n",
        "\n",
        "      super_acc = 100 * correct_with_novelty / total if total > 0 else 0\n",
        "      sub_acc = 100 * sub_correct / total if total > 0 else 0\n",
        "\n",
        "      novel_acc = 100 * novel_correct / novel_total if novel_total > 0 else 0\n",
        "      known_acc = 100 * known_correct / known_total if known_total > 0 else 0\n",
        "      balanced_acc = (novel_acc + known_acc) / 2 if novel_total > 0 and known_total > 0 else 0\n",
        "\n",
        "      avg_super_energy = sum(all_super_energies) / len(all_super_energies) if all_super_energies else 0\n",
        "      avg_sub_conf = sum(all_sub_confidences) / len(all_sub_confidences) if all_sub_confidences else 0\n",
        "\n",
        "      novel_super_perc = 100 * novel_super_predictions / total if total > 0 else 0\n",
        "      novel_sub_perc = 100 * novel_sub_predictions / total if total > 0 else 0\n",
        "\n",
        "      # Display metrics\n",
        "      print(f'Validation loss: {running_loss/(i+1):.3f}')\n",
        "      print(f'Validation superclass acc: {super_acc:.2f}%')\n",
        "      print(f'Validation subclass acc: {sub_acc:.2f}%')\n",
        "      print(f'Novel superclass acc: {novel_acc:.2f}%, Known superclass acc: {known_acc:.2f}%')\n",
        "      print(f'Balanced superclass acc: {balanced_acc:.2f}%')\n",
        "      print(f'Average normalized superclass energy: {avg_super_energy:.4f}')\n",
        "      print(f'Average subclass confidence: {avg_sub_conf:.4f}')\n",
        "      print(f'Samples predicted as novel superclass: {novel_super_predictions} ({novel_super_perc:.2f}%)')\n",
        "      print(f'Samples predicted as novel subclass: {novel_sub_predictions} ({novel_sub_perc:.2f}%)')\n",
        "\n",
        "      return {\n",
        "          'loss': running_loss/(i+1),\n",
        "          'accuracy': super_acc,\n",
        "          'novel_acc': novel_acc,\n",
        "          'known_acc': known_acc,\n",
        "          'balanced_acc': balanced_acc\n",
        "      }\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False, confidence_threshold=0.0):\n",
        "      if not self.test_loader:\n",
        "          raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "      # Make sure energy statistics are calibrated\n",
        "      if not self.energy_calibrated:\n",
        "          self._calibrate_energy_stats()\n",
        "\n",
        "      self.model.eval()\n",
        "      novel_superclass_idx = 3  # Index for novel superclass\n",
        "      novel_subclass_idx = 87   # Index for novel subclass\n",
        "\n",
        "      # Create full data structure for internal use\n",
        "      full_test_predictions = {\n",
        "          'image': [],\n",
        "          'superclass_index': [],\n",
        "          'subclass_index': [],\n",
        "          'superclass_energy': [],\n",
        "          'subclass_confidence': [],\n",
        "          'novelty_score': []\n",
        "      }\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for i, data in enumerate(self.test_loader):\n",
        "              inputs, img_name = data[0].to(self.device), data[1]\n",
        "\n",
        "              super_outputs, sub_outputs = self.model(inputs)\n",
        "\n",
        "\n",
        "              super_energies = self.compute_normalized_energy(super_outputs)\n",
        "              energy_novel = super_energies > confidence_threshold\n",
        "\n",
        "\n",
        "              super_probs = F.softmax(super_outputs, dim=1)\n",
        "              super_confidences, super_predicted = torch.max(super_probs, dim=1)\n",
        "\n",
        "\n",
        "              conf_threshold = 0.7\n",
        "              confidence_novel = super_confidences < conf_threshold\n",
        "\n",
        "\n",
        "              energy_weight = 0.6\n",
        "              confidence_weight = 0.4\n",
        "\n",
        "              novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "              decision_threshold = 0.5\n",
        "              novel_super_mask = novelty_score > decision_threshold\n",
        "\n",
        "\n",
        "              sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "              sub_confidences, sub_predicted = torch.max(sub_probs, dim=1)\n",
        "              sub_threshold = 0.5\n",
        "              novel_sub_mask = sub_confidences < sub_threshold\n",
        "\n",
        "              for j in range(inputs.size(0)):\n",
        "                  img = img_name[j] if isinstance(img_name, list) else img_name[0]\n",
        "\n",
        "\n",
        "                  super_pred = novel_superclass_idx if novel_super_mask[j] else super_predicted[j].item()\n",
        "                  sub_pred = novel_subclass_idx if novel_sub_mask[j] else sub_predicted[j].item()\n",
        "\n",
        "                  full_test_predictions['image'].append(img)\n",
        "                  full_test_predictions['superclass_index'].append(super_pred)\n",
        "                  full_test_predictions['subclass_index'].append(sub_pred)\n",
        "                  full_test_predictions['superclass_energy'].append(super_energies[j].item())\n",
        "                  full_test_predictions['subclass_confidence'].append(sub_confidences[j].item())\n",
        "                  full_test_predictions['novelty_score'].append(novelty_score[j].item())\n",
        "\n",
        "      full_predictions_df = pd.DataFrame(data=full_test_predictions)\n",
        "\n",
        "\n",
        "      simplified_test_predictions = {\n",
        "          'image': full_test_predictions['image'],\n",
        "          'superclass_index': full_test_predictions['superclass_index'],\n",
        "          'subclass_index': full_test_predictions['subclass_index']\n",
        "      }\n",
        "      simplified_predictions_df = pd.DataFrame(data=simplified_test_predictions)\n",
        "\n",
        "\n",
        "      novel_super_count = sum(1 for idx in full_test_predictions['superclass_index'] if idx == novel_superclass_idx)\n",
        "      novel_sub_count = sum(1 for idx in full_test_predictions['subclass_index'] if idx == novel_subclass_idx)\n",
        "\n",
        "      total_count = len(full_test_predictions['image'])\n",
        "      novel_super_perc = 100 * novel_super_count / total_count if total_count > 0 else 0\n",
        "      novel_sub_perc = 100 * novel_sub_count / total_count if total_count > 0 else 0\n",
        "\n",
        "      print(f'Test set predictions:')\n",
        "      print(f'Images predicted as novel superclass: {novel_super_count} ({novel_super_perc:.2f}%)')\n",
        "      print(f'Images predicted as novel subclass: {novel_sub_count} ({novel_sub_perc:.2f}%)')\n",
        "\n",
        "\n",
        "      print(f'Novelty score distribution:')\n",
        "      bins = [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
        "      for i in range(len(bins)-1):\n",
        "          count = sum(1 for score in full_test_predictions['novelty_score']\n",
        "                    if bins[i] <= score < bins[i+1])\n",
        "          print(f'  {bins[i]:.1f}-{bins[i+1]:.1f}: {count} ({100*count/total_count:.2f}%)')\n",
        "\n",
        "      if save_to_csv:\n",
        "\n",
        "          simplified_predictions_df.to_csv('example_test_predictions.csv', index=False)\n",
        "          print(\"Predictions saved to 'example_test_predictions.csv'\")\n",
        "\n",
        "      if return_predictions:\n",
        "\n",
        "          return full_predictions_df\n",
        "\n",
        "\n",
        "def train_with_novelty_detection(full_dataset, image_preprocessing, device='cuda', batch_size=64, epochs=5):\n",
        "\n",
        "    novelty_trainer = NoveltyDetectionTrainer(\n",
        "        full_dataset=full_dataset,\n",
        "        image_preprocessing=image_preprocessing,\n",
        "        device=device,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"Running cross-validation for novelty detection...\")\n",
        "    avg_results, fold_results = novelty_trainer.cross_validate_novelty_detection(epochs=epochs)\n",
        "\n",
        "\n",
        "    print(\"\\nFinding optimal energy threshold...\")\n",
        "    best_threshold, threshold_results = novelty_trainer.find_optimal_threshold()\n",
        "\n",
        "    return avg_results, best_threshold\n"
      ],
      "metadata": {
        "id": "20bsRDmhGSLM"
      },
      "id": "20bsRDmhGSLM",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. First run cross-validation to find the optimal threshold\n",
        "results, threshold = train_with_novelty_detection(full_dataset, image_preprocessing)\n",
        "device = 'cuda'\n",
        "\n",
        "# 2. Then train your final model on all data and use the threshold for inference\n",
        "model = CNN(input_size=64, num_superclasses=3, num_subclasses=87).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader, device=device)\n",
        "\n",
        "# Initialize energy normalization parameters\n",
        "trainer.energy_mean = 0\n",
        "trainer.energy_std = 1\n",
        "\n",
        "for epoch in range(20):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    trainer.train_epoch()\n",
        "    trainer.validate_epoch(confidence_threshold=threshold)  # Use optimized threshold\n",
        "    print('')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Test with optimized threshold for novel class detection\n",
        "predictions = trainer.test(save_to_csv=True, confidence_threshold=threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmR10iTb3RjY",
        "outputId": "ec4b8883-7a36-412b-e26c-d6d468bfc04e"
      },
      "id": "cmR10iTb3RjY",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found superclasses with indices: [0, 1, 2]\n",
            "Running cross-validation for novelty detection...\n",
            "\n",
            "=== Fold 1/3: Treating superclass 0 as novel ===\n",
            "Epoch 1/5, Loss: 3.7903\n",
            "Epoch 2/5, Loss: 2.2881\n",
            "Epoch 3/5, Loss: 1.6160\n",
            "Epoch 4/5, Loss: 1.2769\n",
            "Epoch 5/5, Loss: 1.0051\n",
            "Calibrated energy statistics: mean=-7.6329, std=2.4059\n",
            "Calibrated energy statistics: mean=-7.4353, std=2.4509\n",
            "Fold 1 results:\n",
            "  known_superclass_accuracy: 0.4820\n",
            "  novel_superclass_accuracy: 0.9011\n",
            "  balanced_superclass_accuracy: 0.6915\n",
            "  known_subclass_accuracy: 0.7342\n",
            "  novel_subclass_accuracy: 0.5411\n",
            "  balanced_subclass_accuracy: 0.6377\n",
            "\n",
            "=== Fold 2/3: Treating superclass 1 as novel ===\n",
            "Epoch 1/5, Loss: 3.8059\n",
            "Epoch 2/5, Loss: 2.4237\n",
            "Epoch 3/5, Loss: 1.7160\n",
            "Epoch 4/5, Loss: 1.3096\n",
            "Epoch 5/5, Loss: 1.0380\n",
            "Calibrated energy statistics: mean=-7.3479, std=3.1703\n",
            "Calibrated energy statistics: mean=-7.4792, std=3.3093\n",
            "Fold 2 results:\n",
            "  known_superclass_accuracy: 0.4204\n",
            "  novel_superclass_accuracy: 0.9726\n",
            "  balanced_superclass_accuracy: 0.6965\n",
            "  known_subclass_accuracy: 0.7245\n",
            "  novel_subclass_accuracy: 0.5302\n",
            "  balanced_subclass_accuracy: 0.6273\n",
            "\n",
            "=== Fold 3/3: Treating superclass 2 as novel ===\n",
            "Epoch 1/5, Loss: 3.4744\n",
            "Epoch 2/5, Loss: 1.9913\n",
            "Epoch 3/5, Loss: 1.3808\n",
            "Epoch 4/5, Loss: 1.0229\n",
            "Epoch 5/5, Loss: 0.7405\n",
            "Calibrated energy statistics: mean=-8.4050, std=3.0736\n",
            "Calibrated energy statistics: mean=-8.3637, std=3.1729\n",
            "Fold 3 results:\n",
            "  known_superclass_accuracy: 0.4772\n",
            "  novel_superclass_accuracy: 0.7175\n",
            "  balanced_superclass_accuracy: 0.5973\n",
            "  known_subclass_accuracy: 0.8503\n",
            "  novel_subclass_accuracy: 0.4630\n",
            "  balanced_subclass_accuracy: 0.6566\n",
            "known_superclass_accuracy: 0.4599\n",
            "novel_superclass_accuracy: 0.8637\n",
            "balanced_superclass_accuracy: 0.6618\n",
            "known_subclass_accuracy: 0.7697\n",
            "novel_subclass_accuracy: 0.5115\n",
            "balanced_subclass_accuracy: 0.6406\n",
            "\n",
            "Finding optimal energy threshold...\n",
            "\n",
            "=== Finding optimal threshold for fold 1: Superclass 0 as novel ===\n",
            "Epoch 1/5, Loss: 3.5671\n",
            "Epoch 2/5, Loss: 2.2134\n",
            "Epoch 3/5, Loss: 1.5683\n",
            "Epoch 4/5, Loss: 1.2113\n",
            "Epoch 5/5, Loss: 0.9002\n",
            "Calibrated energy statistics: mean=-7.2572, std=2.3472\n",
            "Threshold -3.00: Known Acc=0.0023, Novel Acc=0.9973, Balanced Acc=0.4998\n",
            "Threshold -2.90: Known Acc=0.0045, Novel Acc=0.9968, Balanced Acc=0.5006\n",
            "Threshold -2.80: Known Acc=0.0045, Novel Acc=0.9962, Balanced Acc=0.5004\n",
            "Threshold -2.70: Known Acc=0.0068, Novel Acc=0.9957, Balanced Acc=0.5012\n",
            "Threshold -2.60: Known Acc=0.0068, Novel Acc=0.9957, Balanced Acc=0.5012\n",
            "Threshold -2.50: Known Acc=0.0113, Novel Acc=0.9951, Balanced Acc=0.5032\n",
            "Threshold -2.40: Known Acc=0.0113, Novel Acc=0.9941, Balanced Acc=0.5027\n",
            "Threshold -2.30: Known Acc=0.0135, Novel Acc=0.9930, Balanced Acc=0.5032\n",
            "Threshold -2.20: Known Acc=0.0180, Novel Acc=0.9914, Balanced Acc=0.5047\n",
            "Threshold -2.10: Known Acc=0.0270, Novel Acc=0.9881, Balanced Acc=0.5076\n",
            "Threshold -2.00: Known Acc=0.0293, Novel Acc=0.9859, Balanced Acc=0.5076\n",
            "Threshold -1.90: Known Acc=0.0338, Novel Acc=0.9843, Balanced Acc=0.5091\n",
            "Threshold -1.80: Known Acc=0.0450, Novel Acc=0.9822, Balanced Acc=0.5136\n",
            "Threshold -1.70: Known Acc=0.0541, Novel Acc=0.9795, Balanced Acc=0.5168\n",
            "Threshold -1.60: Known Acc=0.0698, Novel Acc=0.9773, Balanced Acc=0.5236\n",
            "Threshold -1.50: Known Acc=0.0743, Novel Acc=0.9751, Balanced Acc=0.5247\n",
            "Threshold -1.40: Known Acc=0.0833, Novel Acc=0.9714, Balanced Acc=0.5273\n",
            "Threshold -1.30: Known Acc=0.1014, Novel Acc=0.9692, Balanced Acc=0.5353\n",
            "Threshold -1.20: Known Acc=0.1149, Novel Acc=0.9643, Balanced Acc=0.5396\n",
            "Threshold -1.10: Known Acc=0.1419, Novel Acc=0.9611, Balanced Acc=0.5515\n",
            "Threshold -1.00: Known Acc=0.1509, Novel Acc=0.9546, Balanced Acc=0.5527\n",
            "Threshold -0.90: Known Acc=0.1622, Novel Acc=0.9449, Balanced Acc=0.5535\n",
            "Threshold -0.80: Known Acc=0.1779, Novel Acc=0.9373, Balanced Acc=0.5576\n",
            "Threshold -0.70: Known Acc=0.2095, Novel Acc=0.9319, Balanced Acc=0.5707\n",
            "Threshold -0.60: Known Acc=0.2365, Novel Acc=0.9232, Balanced Acc=0.5799\n",
            "Threshold -0.50: Known Acc=0.2613, Novel Acc=0.9135, Balanced Acc=0.5874\n",
            "Threshold -0.40: Known Acc=0.2860, Novel Acc=0.9059, Balanced Acc=0.5960\n",
            "Threshold -0.30: Known Acc=0.3153, Novel Acc=0.8973, Balanced Acc=0.6063\n",
            "Threshold -0.20: Known Acc=0.3446, Novel Acc=0.8876, Balanced Acc=0.6161\n",
            "Threshold -0.10: Known Acc=0.3739, Novel Acc=0.8697, Balanced Acc=0.6218\n",
            "Threshold 0.00: Known Acc=0.4009, Novel Acc=0.8530, Balanced Acc=0.6269\n",
            "Threshold 0.10: Known Acc=0.4369, Novel Acc=0.8357, Balanced Acc=0.6363\n",
            "Threshold 0.20: Known Acc=0.4932, Novel Acc=0.8216, Balanced Acc=0.6574\n",
            "Threshold 0.30: Known Acc=0.5473, Novel Acc=0.7968, Balanced Acc=0.6720\n",
            "Threshold 0.40: Known Acc=0.5946, Novel Acc=0.7827, Balanced Acc=0.6886\n",
            "Threshold 0.50: Known Acc=0.6194, Novel Acc=0.7616, Balanced Acc=0.6905\n",
            "Threshold 0.60: Known Acc=0.6712, Novel Acc=0.7351, Balanced Acc=0.7032\n",
            "Threshold 0.70: Known Acc=0.7185, Novel Acc=0.7103, Balanced Acc=0.7144\n",
            "Threshold 0.80: Known Acc=0.7658, Novel Acc=0.6816, Balanced Acc=0.7237\n",
            "Threshold 0.90: Known Acc=0.7973, Novel Acc=0.6476, Balanced Acc=0.7224\n",
            "Threshold 1.00: Known Acc=0.8198, Novel Acc=0.6168, Balanced Acc=0.7183\n",
            "Threshold 1.10: Known Acc=0.8423, Novel Acc=0.5886, Balanced Acc=0.7155\n",
            "Threshold 1.20: Known Acc=0.8559, Novel Acc=0.5541, Balanced Acc=0.7050\n",
            "Threshold 1.30: Known Acc=0.8806, Novel Acc=0.5205, Balanced Acc=0.7006\n",
            "Threshold 1.40: Known Acc=0.8941, Novel Acc=0.4827, Balanced Acc=0.6884\n",
            "Threshold 1.50: Known Acc=0.9099, Novel Acc=0.4405, Balanced Acc=0.6752\n",
            "Threshold 1.60: Known Acc=0.9459, Novel Acc=0.3989, Balanced Acc=0.6724\n",
            "Threshold 1.70: Known Acc=0.9550, Novel Acc=0.3524, Balanced Acc=0.6537\n",
            "Threshold 1.80: Known Acc=0.9707, Novel Acc=0.3049, Balanced Acc=0.6378\n",
            "Threshold 1.90: Known Acc=0.9730, Novel Acc=0.2616, Balanced Acc=0.6173\n",
            "Threshold 2.00: Known Acc=0.9775, Novel Acc=0.2173, Balanced Acc=0.5974\n",
            "Threshold 2.10: Known Acc=0.9797, Novel Acc=0.1805, Balanced Acc=0.5801\n",
            "Threshold 2.20: Known Acc=0.9820, Novel Acc=0.1378, Balanced Acc=0.5599\n",
            "Threshold 2.30: Known Acc=0.9887, Novel Acc=0.1049, Balanced Acc=0.5468\n",
            "Threshold 2.40: Known Acc=0.9955, Novel Acc=0.0719, Balanced Acc=0.5337\n",
            "Threshold 2.50: Known Acc=0.9955, Novel Acc=0.0470, Balanced Acc=0.5213\n",
            "Threshold 2.60: Known Acc=0.9955, Novel Acc=0.0259, Balanced Acc=0.5107\n",
            "Threshold 2.70: Known Acc=0.9955, Novel Acc=0.0157, Balanced Acc=0.5056\n",
            "Threshold 2.80: Known Acc=1.0000, Novel Acc=0.0054, Balanced Acc=0.5027\n",
            "Threshold 2.90: Known Acc=1.0000, Novel Acc=0.0027, Balanced Acc=0.5014\n",
            "\n",
            "Best threshold: 0.80\n",
            "Known accuracy: 0.7658\n",
            "Novel accuracy: 0.6816\n",
            "Balanced accuracy: 0.7237\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.583\n",
            "Calibrated energy statistics: mean=-3.6228, std=2.3014\n",
            "Validation loss: 3.327\n",
            "Validation superclass acc: 74.52%\n",
            "Validation subclass acc: 10.67%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 74.52%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -0.0121\n",
            "Average subclass confidence: 0.3697\n",
            "Samples predicted as novel superclass: 139 (22.13%)\n",
            "Samples predicted as novel subclass: 459 (73.09%)\n",
            "\n",
            "Epoch 2\n",
            "Training loss: 1.951\n",
            "Validation loss: 2.178\n",
            "Validation superclass acc: 83.12%\n",
            "Validation subclass acc: 32.64%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 83.12%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -0.4585\n",
            "Average subclass confidence: 0.5510\n",
            "Samples predicted as novel superclass: 88 (14.01%)\n",
            "Samples predicted as novel subclass: 299 (47.61%)\n",
            "\n",
            "Epoch 3\n",
            "Training loss: 1.427\n",
            "Validation loss: 1.218\n",
            "Validation superclass acc: 92.20%\n",
            "Validation subclass acc: 53.50%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.20%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -0.7972\n",
            "Average subclass confidence: 0.6492\n",
            "Samples predicted as novel superclass: 37 (5.89%)\n",
            "Samples predicted as novel subclass: 204 (32.48%)\n",
            "\n",
            "Epoch 4\n",
            "Training loss: 1.025\n",
            "Validation loss: 1.266\n",
            "Validation superclass acc: 94.11%\n",
            "Validation subclass acc: 53.03%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 94.11%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.1334\n",
            "Average subclass confidence: 0.6900\n",
            "Samples predicted as novel superclass: 31 (4.94%)\n",
            "Samples predicted as novel subclass: 178 (28.34%)\n",
            "\n",
            "Epoch 5\n",
            "Training loss: 0.796\n",
            "Validation loss: 1.205\n",
            "Validation superclass acc: 91.88%\n",
            "Validation subclass acc: 60.83%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 91.88%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.1031\n",
            "Average subclass confidence: 0.7568\n",
            "Samples predicted as novel superclass: 44 (7.01%)\n",
            "Samples predicted as novel subclass: 118 (18.79%)\n",
            "\n",
            "Epoch 6\n",
            "Training loss: 0.672\n",
            "Validation loss: 1.535\n",
            "Validation superclass acc: 92.99%\n",
            "Validation subclass acc: 58.28%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.99%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.6414\n",
            "Average subclass confidence: 0.7726\n",
            "Samples predicted as novel superclass: 22 (3.50%)\n",
            "Samples predicted as novel subclass: 95 (15.13%)\n",
            "\n",
            "Epoch 7\n",
            "Training loss: 0.571\n",
            "Validation loss: 1.108\n",
            "Validation superclass acc: 94.43%\n",
            "Validation subclass acc: 68.95%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 94.43%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.5593\n",
            "Average subclass confidence: 0.8159\n",
            "Samples predicted as novel superclass: 23 (3.66%)\n",
            "Samples predicted as novel subclass: 79 (12.58%)\n",
            "\n",
            "Epoch 8\n",
            "Training loss: 0.486\n",
            "Validation loss: 0.909\n",
            "Validation superclass acc: 94.11%\n",
            "Validation subclass acc: 70.22%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 94.11%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.4929\n",
            "Average subclass confidence: 0.8159\n",
            "Samples predicted as novel superclass: 33 (5.25%)\n",
            "Samples predicted as novel subclass: 74 (11.78%)\n",
            "\n",
            "Epoch 9\n",
            "Training loss: 0.429\n",
            "Validation loss: 1.441\n",
            "Validation superclass acc: 93.95%\n",
            "Validation subclass acc: 61.46%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 93.95%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.5386\n",
            "Average subclass confidence: 0.8165\n",
            "Samples predicted as novel superclass: 30 (4.78%)\n",
            "Samples predicted as novel subclass: 67 (10.67%)\n",
            "\n",
            "Epoch 10\n",
            "Training loss: 0.395\n",
            "Validation loss: 1.053\n",
            "Validation superclass acc: 95.38%\n",
            "Validation subclass acc: 70.54%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 95.38%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.1554\n",
            "Average subclass confidence: 0.8527\n",
            "Samples predicted as novel superclass: 16 (2.55%)\n",
            "Samples predicted as novel subclass: 47 (7.48%)\n",
            "\n",
            "Epoch 11\n",
            "Training loss: 0.358\n",
            "Validation loss: 1.105\n",
            "Validation superclass acc: 96.02%\n",
            "Validation subclass acc: 71.34%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.02%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.9737\n",
            "Average subclass confidence: 0.8581\n",
            "Samples predicted as novel superclass: 18 (2.87%)\n",
            "Samples predicted as novel subclass: 52 (8.28%)\n",
            "\n",
            "Epoch 12\n",
            "Training loss: 0.306\n",
            "Validation loss: 1.023\n",
            "Validation superclass acc: 95.86%\n",
            "Validation subclass acc: 76.75%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 95.86%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.3213\n",
            "Average subclass confidence: 0.8904\n",
            "Samples predicted as novel superclass: 18 (2.87%)\n",
            "Samples predicted as novel subclass: 31 (4.94%)\n",
            "\n",
            "Epoch 13\n",
            "Training loss: 0.269\n",
            "Validation loss: 1.254\n",
            "Validation superclass acc: 93.15%\n",
            "Validation subclass acc: 75.96%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 93.15%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.3488\n",
            "Average subclass confidence: 0.8903\n",
            "Samples predicted as novel superclass: 32 (5.10%)\n",
            "Samples predicted as novel subclass: 34 (5.41%)\n",
            "\n",
            "Epoch 14\n",
            "Training loss: 0.256\n",
            "Validation loss: 1.238\n",
            "Validation superclass acc: 93.63%\n",
            "Validation subclass acc: 74.36%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 93.63%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.4046\n",
            "Average subclass confidence: 0.8849\n",
            "Samples predicted as novel superclass: 29 (4.62%)\n",
            "Samples predicted as novel subclass: 35 (5.57%)\n",
            "\n",
            "Epoch 15\n",
            "Training loss: 0.233\n",
            "Validation loss: 1.222\n",
            "Validation superclass acc: 96.50%\n",
            "Validation subclass acc: 75.32%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.50%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.5064\n",
            "Average subclass confidence: 0.8913\n",
            "Samples predicted as novel superclass: 12 (1.91%)\n",
            "Samples predicted as novel subclass: 36 (5.73%)\n",
            "\n",
            "Epoch 16\n",
            "Training loss: 0.209\n",
            "Validation loss: 1.385\n",
            "Validation superclass acc: 95.22%\n",
            "Validation subclass acc: 71.50%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 95.22%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.6383\n",
            "Average subclass confidence: 0.8814\n",
            "Samples predicted as novel superclass: 21 (3.34%)\n",
            "Samples predicted as novel subclass: 40 (6.37%)\n",
            "\n",
            "Epoch 17\n",
            "Training loss: 0.207\n",
            "Validation loss: 0.978\n",
            "Validation superclass acc: 96.18%\n",
            "Validation subclass acc: 78.50%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.18%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.6469\n",
            "Average subclass confidence: 0.9065\n",
            "Samples predicted as novel superclass: 19 (3.03%)\n",
            "Samples predicted as novel subclass: 24 (3.82%)\n",
            "\n",
            "Epoch 18\n",
            "Training loss: 0.188\n",
            "Validation loss: 1.280\n",
            "Validation superclass acc: 96.66%\n",
            "Validation subclass acc: 73.73%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.66%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.6045\n",
            "Average subclass confidence: 0.8964\n",
            "Samples predicted as novel superclass: 13 (2.07%)\n",
            "Samples predicted as novel subclass: 35 (5.57%)\n",
            "\n",
            "Epoch 19\n",
            "Training loss: 0.181\n",
            "Validation loss: 1.168\n",
            "Validation superclass acc: 96.34%\n",
            "Validation subclass acc: 75.48%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.34%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -3.1540\n",
            "Average subclass confidence: 0.9040\n",
            "Samples predicted as novel superclass: 10 (1.59%)\n",
            "Samples predicted as novel subclass: 27 (4.30%)\n",
            "\n",
            "Epoch 20\n",
            "Training loss: 0.174\n",
            "Validation loss: 1.032\n",
            "Validation superclass acc: 95.54%\n",
            "Validation subclass acc: 77.87%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 95.54%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -3.1112\n",
            "Average subclass confidence: 0.9057\n",
            "Samples predicted as novel superclass: 19 (3.03%)\n",
            "Samples predicted as novel subclass: 25 (3.98%)\n",
            "\n",
            "Finished Training\n",
            "Test set predictions:\n",
            "Images predicted as novel superclass: 665 (5.95%)\n",
            "Images predicted as novel subclass: 1295 (11.58%)\n",
            "Novelty score distribution:\n",
            "  0.0-0.2: 10410 (93.11%)\n",
            "  0.2-0.4: 0 (0.00%)\n",
            "  0.4-0.5: 105 (0.94%)\n",
            "  0.5-0.6: 0 (0.00%)\n",
            "  0.6-0.8: 494 (4.42%)\n",
            "  0.8-1.0: 0 (0.00%)\n",
            "Predictions saved to 'example_test_predictions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Open Max For Novelty Detection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import weibull_min\n",
        "from sklearn.preprocessing import normalize\n",
        "from collections import defaultdict\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size=64, num_superclasses=4, num_subclasses=88):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_size = input_size // (2**3)\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(self.feature_size * self.feature_size * 128, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3a = nn.Linear(128, num_superclasses)\n",
        "        self.fc3b = nn.Linear(128, num_subclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "        return super_out, sub_out\n",
        "\n",
        "    def get_features(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def get_logits(self, features):\n",
        "\n",
        "        super_out = self.fc3a(features)\n",
        "        sub_out = self.fc3b(features)\n",
        "        return super_out, sub_out\n",
        "\n",
        "\n",
        "class OpenMaxModel:\n",
        "    def __init__(self, model, num_superclasses=3, num_subclasses=87, tailsize=20, alpha=10):\n",
        "\n",
        "        self.model = model\n",
        "        self.num_superclasses = num_superclasses\n",
        "        self.num_subclasses = num_subclasses\n",
        "        self.tailsize = tailsize\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Storage for class means and Weibull models\n",
        "        self.super_means = None\n",
        "        self.sub_means = None\n",
        "        self.super_weibull_models = None\n",
        "        self.sub_weibull_models = None\n",
        "\n",
        "    def fit(self, train_loader, device):\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "        super_activations = defaultdict(list)\n",
        "        sub_activations = defaultdict(list)\n",
        "\n",
        "        # Collect activations for each class\n",
        "        with torch.no_grad():\n",
        "            for data in train_loader:\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                # Get features\n",
        "                features = self.model.get_features(inputs)\n",
        "                super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "                # Store activations for each class\n",
        "                for i in range(inputs.size(0)):\n",
        "                    super_class = super_labels[i].item()\n",
        "                    sub_class = sub_labels[i].item()\n",
        "\n",
        "                    super_activations[super_class].append(super_logits[i].cpu().numpy())\n",
        "                    sub_activations[sub_class].append(sub_logits[i].cpu().numpy())\n",
        "\n",
        "\n",
        "        self.super_means = {}\n",
        "        self.sub_means = {}\n",
        "\n",
        "        for c in range(self.num_superclasses):\n",
        "            if c in super_activations and len(super_activations[c]) > 0:\n",
        "                self.super_means[c] = np.mean(super_activations[c], axis=0)\n",
        "\n",
        "        for c in range(self.num_subclasses):\n",
        "            if c in sub_activations and len(sub_activations[c]) > 0:\n",
        "                self.sub_means[c] = np.mean(sub_activations[c], axis=0)\n",
        "\n",
        "\n",
        "        super_dists = defaultdict(list)\n",
        "        sub_dists = defaultdict(list)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in train_loader:\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "\n",
        "                features = self.model.get_features(inputs)\n",
        "                super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "\n",
        "                for i in range(inputs.size(0)):\n",
        "                    super_class = super_labels[i].item()\n",
        "                    sub_class = sub_labels[i].item()\n",
        "\n",
        "                    if super_class in self.super_means:\n",
        "                        super_mean = self.super_means[super_class]\n",
        "                        super_logit = super_logits[i].cpu().numpy()\n",
        "                        super_dist = np.linalg.norm(super_logit - super_mean)\n",
        "                        super_dists[super_class].append(super_dist)\n",
        "\n",
        "                    if sub_class in self.sub_means:\n",
        "                        sub_mean = self.sub_means[sub_class]\n",
        "                        sub_logit = sub_logits[i].cpu().numpy()\n",
        "                        sub_dist = np.linalg.norm(sub_logit - sub_mean)\n",
        "                        sub_dists[sub_class].append(sub_dist)\n",
        "\n",
        "\n",
        "        self.super_weibull_models = {}\n",
        "        self.sub_weibull_models = {}\n",
        "\n",
        "        for c in range(self.num_superclasses):\n",
        "            if c in super_dists and len(super_dists[c]) > self.tailsize:\n",
        "\n",
        "                sorted_dists = sorted(super_dists[c])\n",
        "                tail_dists = sorted_dists[-self.tailsize:]\n",
        "\n",
        "\n",
        "                try:\n",
        "                    shape, loc, scale = weibull_min.fit(tail_dists, floc=0)\n",
        "                    self.super_weibull_models[c] = (shape, loc, scale)\n",
        "                except:\n",
        "                    print(f\"Warning: Failed to fit Weibull for superclass {c}\")\n",
        "\n",
        "        for c in range(self.num_subclasses):\n",
        "            if c in sub_dists and len(sub_dists[c]) > self.tailsize:\n",
        "\n",
        "                sorted_dists = sorted(sub_dists[c])\n",
        "                tail_dists = sorted_dists[-self.tailsize:]\n",
        "\n",
        "\n",
        "                try:\n",
        "                    shape, loc, scale = weibull_min.fit(tail_dists, floc=0)\n",
        "                    self.sub_weibull_models[c] = (shape, loc, scale)\n",
        "                except:\n",
        "                    print(f\"Warning: Failed to fit Weibull for subclass {c}\")\n",
        "\n",
        "    def predict(self, inputs, device):\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            features = self.model.get_features(inputs)\n",
        "            super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "\n",
        "            super_logits_np = super_logits.cpu().numpy()\n",
        "            sub_logits_np = sub_logits.cpu().numpy()\n",
        "\n",
        "\n",
        "            super_preds = []\n",
        "            sub_preds = []\n",
        "\n",
        "            for i in range(inputs.size(0)):\n",
        "\n",
        "                super_logit = super_logits_np[i]\n",
        "                super_pred = self._recalibrate_sample(super_logit, self.super_means, self.super_weibull_models, self.num_superclasses)\n",
        "                super_preds.append(super_pred)\n",
        "\n",
        "\n",
        "                sub_logit = sub_logits_np[i]\n",
        "                sub_pred = self._recalibrate_sample(sub_logit, self.sub_means, self.sub_weibull_models, self.num_subclasses)\n",
        "                sub_preds.append(sub_pred)\n",
        "\n",
        "\n",
        "            super_preds = torch.tensor(super_preds, device=device)\n",
        "            sub_preds = torch.tensor(sub_preds, device=device)\n",
        "\n",
        "        return super_preds, sub_preds\n",
        "\n",
        "    def _recalibrate_sample(self, logits, means, weibull_models, num_classes):\n",
        "\n",
        "\n",
        "        top_alpha_idx = np.argsort(logits)[-self.alpha:]\n",
        "\n",
        "\n",
        "        distances = {}\n",
        "        for c in range(num_classes):\n",
        "            if c in means:\n",
        "                distances[c] = np.linalg.norm(logits - means[c])\n",
        "\n",
        "\n",
        "        recalibrated = np.copy(logits)\n",
        "        for c in top_alpha_idx:\n",
        "            if c < num_classes and c in weibull_models:\n",
        "\n",
        "                shape, loc, scale = weibull_models[c]\n",
        "\n",
        "\n",
        "                dist = distances.get(c, 0)\n",
        "                weibull_score = 1 - weibull_min.cdf(dist, shape, loc, scale)\n",
        "\n",
        "\n",
        "                recalibrated[c] = logits[c] * (1 - weibull_score)\n",
        "\n",
        "\n",
        "        recalibrated_probs = np.exp(recalibrated) / np.sum(np.exp(recalibrated))\n",
        "\n",
        "\n",
        "        unknown_prob = 1.0 - np.sum(recalibrated_probs)\n",
        "\n",
        "\n",
        "        if unknown_prob > 0.5:\n",
        "            return num_classes\n",
        "        else:\n",
        "            return np.argmax(recalibrated_probs)\n",
        "\n",
        "\n",
        "class OpenMaxTrainer:\n",
        "    def __init__(self, model, openmax_model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n",
        "        self.model = model\n",
        "        self.openmax_model = openmax_model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, _, sub_labels, _ = data\n",
        "            inputs = inputs.to(self.device)\n",
        "            super_labels = super_labels.to(self.device)\n",
        "            sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/(i+1):.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                super_labels = super_labels.to(self.device)\n",
        "                sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "                # Standard forward pass for loss calculation\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, sub_preds = self.openmax_model.predict(inputs, self.device)\n",
        "\n",
        "                total += super_labels.size(0)\n",
        "                super_correct += (super_preds == super_labels).sum().item()\n",
        "                sub_correct += (sub_preds == sub_labels).sum().item()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'Validation loss: {running_loss/(i+1):.3f}')\n",
        "        print(f'Validation superclass acc: {100 * super_correct / total:.2f}%')\n",
        "        print(f'Validation subclass acc: {100 * sub_correct / total:.2f}%')\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False):\n",
        "        if not self.test_loader:\n",
        "            raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Evaluate on test set with OpenMax\n",
        "        test_predictions = {\n",
        "            'image': [],\n",
        "            'superclass_index': [],\n",
        "            'subclass_index': []\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(self.device), data[1]\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, sub_preds = self.openmax_model.predict(inputs, self.device)\n",
        "\n",
        "                for j in range(inputs.size(0)):\n",
        "                    img = img_name[j] if isinstance(img_name, list) else img_name[0]\n",
        "\n",
        "                    test_predictions['image'].append(img)\n",
        "                    test_predictions['superclass_index'].append(super_preds[j].item())\n",
        "                    test_predictions['subclass_index'].append(sub_preds[j].item())\n",
        "\n",
        "        test_predictions = pd.DataFrame(data=test_predictions)\n",
        "\n",
        "        # Print summary of novel predictions\n",
        "        novel_super_count = sum(1 for idx in test_predictions['superclass_index'] if idx == self.openmax_model.num_superclasses)\n",
        "        novel_sub_count = sum(1 for idx in test_predictions['subclass_index'] if idx == self.openmax_model.num_subclasses)\n",
        "\n",
        "        print(f'Test set predictions:')\n",
        "        print(f'Images predicted as novel superclass: {novel_super_count} ({100*novel_super_count/len(test_predictions):.2f}%)')\n",
        "        print(f'Images predicted as novel subclass: {novel_sub_count} ({100*novel_sub_count/len(test_predictions):.2f}%)')\n",
        "\n",
        "        if save_to_csv:\n",
        "            test_predictions.to_csv('openmax_test_predictions.csv', index=False)\n",
        "\n",
        "        if return_predictions:\n",
        "            return test_predictions\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "def train_with_openmax(full_dataset, device='cuda', batch_size=64, num_epochs=20):\n",
        "    # Create cross-validation split\n",
        "    from torch.utils.data import random_split\n",
        "\n",
        "    # Split into train and validation\n",
        "    train_size = int(0.9 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = CNN(input_size=64, num_superclasses=4, num_subclasses=88).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Train standard model first\n",
        "    trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, device=device)\n",
        "\n",
        "    print(\"Training standard model...\")\n",
        "    for epoch in range(10):  # Pre-train for 10 epochs\n",
        "        print(f'Epoch {epoch+1}/10')\n",
        "        trainer.train_epoch()\n",
        "\n",
        "    # Initialize OpenMax model\n",
        "    openmax_model = OpenMaxModel(model, num_superclasses=3, num_subclasses=87)\n",
        "\n",
        "    # Fit OpenMax parameters\n",
        "    print(\"\\nFitting OpenMax parameters...\")\n",
        "    openmax_model.fit(train_loader, device)\n",
        "\n",
        "    # Continue training with OpenMax\n",
        "    openmax_trainer = OpenMaxTrainer(model, openmax_model, criterion, optimizer, train_loader, val_loader, device=device)\n",
        "\n",
        "    print(\"\\nTraining with OpenMax...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        openmax_trainer.train_epoch()\n",
        "        openmax_trainer.validate_epoch()\n",
        "        print()\n",
        "\n",
        "    print(\"Finished Training\")\n",
        "\n",
        "    return model, openmax_model\n",
        "\n",
        "\n",
        "\n",
        "def openmax_cross_validation(full_dataset, image_preprocessing, device='cuda', batch_size=64):\n",
        "    \"\"\"Run cross-validation for OpenMax novel detection\"\"\"\n",
        "    # Get all unique superclass indices\n",
        "    superclass_indices = set()\n",
        "    for i in range(len(full_dataset)):\n",
        "        _, super_idx, _, _, _ = full_dataset[i]\n",
        "        if hasattr(super_idx, 'item'):\n",
        "            super_idx = super_idx.item()\n",
        "        superclass_indices.add(super_idx)\n",
        "\n",
        "    superclass_indices = sorted(list(superclass_indices))\n",
        "    print(f\"Found superclasses with indices: {superclass_indices}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "\n",
        "    for fold, novel_idx in enumerate(superclass_indices):\n",
        "        print(f\"\\n=== Fold {fold+1}/{len(superclass_indices)}: Treating superclass {novel_idx} as novel ===\")\n",
        "\n",
        "\n",
        "        known_indices = []\n",
        "        novel_indices = []\n",
        "\n",
        "        for i in range(len(full_dataset)):\n",
        "            _, super_idx, _, _, _ = full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "\n",
        "            if super_idx == novel_idx:\n",
        "                novel_indices.append(i)\n",
        "            else:\n",
        "                known_indices.append(i)\n",
        "\n",
        "\n",
        "        np.random.shuffle(known_indices)\n",
        "        train_size = int(0.9 * len(known_indices))\n",
        "        train_indices = known_indices[:train_size]\n",
        "        val_known_indices = known_indices[train_size:]\n",
        "\n",
        "\n",
        "        train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "        val_known_dataset = torch.utils.data.Subset(full_dataset, val_known_indices)\n",
        "        val_novel_dataset = torch.utils.data.Subset(full_dataset, novel_indices)\n",
        "\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_known_loader = torch.utils.data.DataLoader(val_known_dataset, batch_size=batch_size, shuffle=False)\n",
        "        val_novel_loader = torch.utils.data.DataLoader(val_novel_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "        model = CNN(input_size=64, num_superclasses=len(superclass_indices)+1).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "        print(\"Training standard model...\")\n",
        "        for epoch in range(5):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(train_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                super_outputs, sub_outputs = model(inputs)\n",
        "                loss = criterion(super_outputs, super_labels) + criterion(sub_outputs, sub_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/5, Loss: {running_loss/(i+1):.4f}')\n",
        "\n",
        "        # Initialize OpenMax model\n",
        "        num_known_classes = len(superclass_indices) - 1  # Excluding the novel class\n",
        "        openmax_model = OpenMaxModel(model, num_superclasses=num_known_classes)\n",
        "\n",
        "        # Fit OpenMax parameters\n",
        "        print(\"\\nFitting OpenMax parameters...\")\n",
        "        openmax_model.fit(train_loader, device)\n",
        "\n",
        "        # Evaluate OpenMax on known and novel samples\n",
        "        model.eval()\n",
        "\n",
        "        # Test on known classes\n",
        "        known_correct = 0\n",
        "        known_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in val_known_loader:\n",
        "                inputs, super_labels, _, _, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, _ = openmax_model.predict(inputs, device)\n",
        "\n",
        "                known_total += super_labels.size(0)\n",
        "                known_correct += (super_preds == super_labels).sum().item()\n",
        "\n",
        "        # Test on novel classes\n",
        "        novel_correct = 0\n",
        "        novel_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in val_novel_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, _ = openmax_model.predict(inputs, device)\n",
        "\n",
        "                # For novel classes, prediction should be the novel class index\n",
        "                novel_total += inputs.size(0)\n",
        "                novel_correct += (super_preds == num_known_classes).sum().item()\n",
        "\n",
        "        known_acc = known_correct / known_total if known_total > 0 else 0\n",
        "        novel_acc = novel_correct / novel_total if novel_total > 0 else 0\n",
        "        balanced_acc = (known_acc + novel_acc) / 2\n",
        "\n",
        "        results.append({\n",
        "            'fold': fold,\n",
        "            'novel_class': novel_idx,\n",
        "            'known_accuracy': known_acc,\n",
        "            'novel_accuracy': novel_acc,\n",
        "            'balanced_accuracy': balanced_acc\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold+1} results:\")\n",
        "        print(f\"  Known class accuracy: {known_acc:.4f}\")\n",
        "        print(f\"  Novel class accuracy: {novel_acc:.4f}\")\n",
        "        print(f\"  Balanced accuracy: {balanced_acc:.4f}\")\n",
        "\n",
        "    # Calculate average results\n",
        "    avg_known_acc = sum(r['known_accuracy'] for r in results) / len(results)\n",
        "    avg_novel_acc = sum(r['novel_accuracy'] for r in results) / len(results)\n",
        "    avg_balanced_acc = sum(r['balanced_accuracy'] for r in results) / len(results)\n",
        "\n",
        "    print(\"\\n=== OpenMax Cross-Validation Summary ===\")\n",
        "    print(f\"Average known accuracy: {avg_known_acc:.4f}\")\n",
        "    print(f\"Average novel accuracy: {avg_novel_acc:.4f}\")\n",
        "    print(f\"Average balanced accuracy: {avg_balanced_acc:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "LqTJlERxTGkF"
      },
      "id": "LqTJlERxTGkF",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. First, cross-validate to evaluate OpenMax performance\n",
        "results = openmax_cross_validation(full_dataset, image_preprocessing)\n",
        "\n",
        "# 2. Then train your final model with OpenMax\n",
        "model, openmax_model = train_with_openmax(full_dataset)\n",
        "\n",
        "# 3. Test with OpenMax on the test set\n",
        "openmax_trainer = OpenMaxTrainer(model, openmax_model, criterion, optimizer, train_loader, val_loader, test_loader, device)\n",
        "predictions = openmax_trainer.test(save_to_csv=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw01ohW8JCH5",
        "outputId": "9b72ba0f-457d-4e0e-99c6-08cca735f1e2"
      },
      "id": "Aw01ohW8JCH5",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found superclasses with indices: [0, 1, 2]\n",
            "\n",
            "=== Fold 1/3: Treating superclass 0 as novel ===\n",
            "Training standard model...\n",
            "Epoch 1/5, Loss: 3.1934\n",
            "Epoch 2/5, Loss: 1.5236\n",
            "Epoch 3/5, Loss: 0.9673\n",
            "Epoch 4/5, Loss: 0.6727\n",
            "Epoch 5/5, Loss: 0.4073\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "Fold 1 results:\n",
            "  Known class accuracy: 0.6824\n",
            "  Novel class accuracy: 0.9892\n",
            "  Balanced accuracy: 0.8358\n",
            "\n",
            "=== Fold 2/3: Treating superclass 1 as novel ===\n",
            "Training standard model...\n",
            "Epoch 1/5, Loss: 2.9958\n",
            "Epoch 2/5, Loss: 1.3994\n",
            "Epoch 3/5, Loss: 0.9239\n",
            "Epoch 4/5, Loss: 0.6459\n",
            "Epoch 5/5, Loss: 0.4183\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "Fold 2 results:\n",
            "  Known class accuracy: 0.6105\n",
            "  Novel class accuracy: 0.9870\n",
            "  Balanced accuracy: 0.7987\n",
            "\n",
            "=== Fold 3/3: Treating superclass 2 as novel ===\n",
            "Training standard model...\n",
            "Epoch 1/5, Loss: 2.9762\n",
            "Epoch 2/5, Loss: 1.3099\n",
            "Epoch 3/5, Loss: 0.7494\n",
            "Epoch 4/5, Loss: 0.4948\n",
            "Epoch 5/5, Loss: 0.3059\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "Fold 3 results:\n",
            "  Known class accuracy: 0.3274\n",
            "  Novel class accuracy: 0.0000\n",
            "  Balanced accuracy: 0.1637\n",
            "\n",
            "=== OpenMax Cross-Validation Summary ===\n",
            "Average known accuracy: 0.5401\n",
            "Average novel accuracy: 0.6587\n",
            "Average balanced accuracy: 0.5994\n",
            "Training standard model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.074\n",
            "Epoch 2/10\n",
            "Training loss: 1.544\n",
            "Epoch 3/10\n",
            "Training loss: 0.991\n",
            "Epoch 4/10\n",
            "Training loss: 0.694\n",
            "Epoch 5/10\n",
            "Training loss: 0.493\n",
            "Epoch 6/10\n",
            "Training loss: 0.386\n",
            "Epoch 7/10\n",
            "Training loss: 0.287\n",
            "Epoch 8/10\n",
            "Training loss: 0.239\n",
            "Epoch 9/10\n",
            "Training loss: 0.200\n",
            "Epoch 10/10\n",
            "Training loss: 0.185\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "\n",
            "Training with OpenMax...\n",
            "Epoch 1/20\n",
            "Training loss: 1.211\n",
            "Validation loss: 1.782\n",
            "Validation superclass acc: 77.58%\n",
            "Validation subclass acc: 46.26%\n",
            "\n",
            "Epoch 2/20\n",
            "Training loss: 0.438\n",
            "Validation loss: 1.177\n",
            "Validation superclass acc: 74.09%\n",
            "Validation subclass acc: 45.95%\n",
            "\n",
            "Epoch 3/20\n",
            "Training loss: 0.163\n",
            "Validation loss: 1.371\n",
            "Validation superclass acc: 81.40%\n",
            "Validation subclass acc: 44.20%\n",
            "\n",
            "Epoch 4/20\n",
            "Training loss: 0.075\n",
            "Validation loss: 1.141\n",
            "Validation superclass acc: 89.03%\n",
            "Validation subclass acc: 51.03%\n",
            "\n",
            "Epoch 5/20\n",
            "Training loss: 0.046\n",
            "Validation loss: 1.355\n",
            "Validation superclass acc: 87.92%\n",
            "Validation subclass acc: 53.42%\n",
            "\n",
            "Epoch 6/20\n",
            "Training loss: 0.030\n",
            "Validation loss: 1.215\n",
            "Validation superclass acc: 85.37%\n",
            "Validation subclass acc: 57.39%\n",
            "\n",
            "Epoch 7/20\n",
            "Training loss: 0.030\n",
            "Validation loss: 1.253\n",
            "Validation superclass acc: 92.05%\n",
            "Validation subclass acc: 58.35%\n",
            "\n",
            "Epoch 8/20\n",
            "Training loss: 0.014\n",
            "Validation loss: 1.272\n",
            "Validation superclass acc: 89.83%\n",
            "Validation subclass acc: 63.75%\n",
            "\n",
            "Epoch 9/20\n",
            "Training loss: 0.027\n",
            "Validation loss: 1.611\n",
            "Validation superclass acc: 89.19%\n",
            "Validation subclass acc: 61.05%\n",
            "\n",
            "Epoch 10/20\n",
            "Training loss: 0.084\n",
            "Validation loss: 1.484\n",
            "Validation superclass acc: 85.06%\n",
            "Validation subclass acc: 56.44%\n",
            "\n",
            "Epoch 11/20\n",
            "Training loss: 0.066\n",
            "Validation loss: 1.576\n",
            "Validation superclass acc: 89.67%\n",
            "Validation subclass acc: 62.80%\n",
            "\n",
            "Epoch 12/20\n",
            "Training loss: 0.055\n",
            "Validation loss: 1.384\n",
            "Validation superclass acc: 86.33%\n",
            "Validation subclass acc: 65.34%\n",
            "\n",
            "Epoch 13/20\n",
            "Training loss: 0.045\n",
            "Validation loss: 1.552\n",
            "Validation superclass acc: 84.74%\n",
            "Validation subclass acc: 62.80%\n",
            "\n",
            "Epoch 14/20\n",
            "Training loss: 0.073\n",
            "Validation loss: 2.132\n",
            "Validation superclass acc: 85.37%\n",
            "Validation subclass acc: 57.23%\n",
            "\n",
            "Epoch 15/20\n",
            "Training loss: 0.102\n",
            "Validation loss: 1.508\n",
            "Validation superclass acc: 86.17%\n",
            "Validation subclass acc: 60.41%\n",
            "\n",
            "Epoch 16/20\n",
            "Training loss: 0.075\n",
            "Validation loss: 1.616\n",
            "Validation superclass acc: 88.08%\n",
            "Validation subclass acc: 62.16%\n",
            "\n",
            "Epoch 17/20\n",
            "Training loss: 0.058\n",
            "Validation loss: 1.533\n",
            "Validation superclass acc: 91.26%\n",
            "Validation subclass acc: 67.25%\n",
            "\n",
            "Epoch 18/20\n",
            "Training loss: 0.058\n",
            "Validation loss: 1.752\n",
            "Validation superclass acc: 87.12%\n",
            "Validation subclass acc: 67.09%\n",
            "\n",
            "Epoch 19/20\n",
            "Training loss: 0.050\n",
            "Validation loss: 1.613\n",
            "Validation superclass acc: 90.30%\n",
            "Validation subclass acc: 68.20%\n",
            "\n",
            "Epoch 20/20\n",
            "Training loss: 0.055\n",
            "Validation loss: 1.777\n",
            "Validation superclass acc: 92.37%\n",
            "Validation subclass acc: 70.59%\n",
            "\n",
            "Finished Training\n",
            "Test set predictions:\n",
            "Images predicted as novel superclass: 0 (0.00%)\n",
            "Images predicted as novel subclass: 0 (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Novelty Detection with Enhanced Threshold Selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size=64, num_superclasses=4, num_subclasses=88):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.feature_size = input_size // (2**3)\n",
        "\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(self.feature_size * self.feature_size * 128, 256)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "        # Classification heads\n",
        "        self.fc3a = nn.Linear(128, num_superclasses)\n",
        "        self.fc3b = nn.Linear(128, num_subclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        # Pass through convolutional blocks\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        # Flatten for fully connected layers\n",
        "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)  # Apply dropout after activation\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)  # Apply dropout after activation\n",
        "\n",
        "        # Classification heads\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "\n",
        "        return super_out, sub_out\n",
        "\n",
        "    def get_features(self, x):\n",
        "        \"\"\"Extract features before the final classification layer\"\"\"\n",
        "        # Pass through convolutional blocks\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        # Flatten and pass through FC layers (without final classification)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)  # Apply dropout\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)  # Apply dropout\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class NoveltyDetectionTrainer:\n",
        "    def __init__(self, full_dataset, image_preprocessing, device='cuda', batch_size=64,\n",
        "                 min_known_acc=95, min_novel_acc=20):\n",
        "        self.full_dataset = full_dataset\n",
        "        self.image_preprocessing = image_preprocessing\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Store energy normalization parameters\n",
        "        self.energy_mean = 0\n",
        "        self.energy_std = 1\n",
        "\n",
        "        # Required accuracy thresholds\n",
        "        self.min_known_acc = min_known_acc\n",
        "        self.min_novel_acc = min_novel_acc\n",
        "\n",
        "        # Fixed energy threshold (based on cross-validation) for high known accuracy (≥95%)\n",
        "        # and reasonable novel detection (≥20%)\n",
        "        self.energy_threshold = 1.9  # Fixed threshold from cross-validation\n",
        "\n",
        "        # Get all unique superclass indices\n",
        "        self.superclass_indices = set()\n",
        "        for i in range(len(full_dataset)):\n",
        "            _, super_idx, _, _, _ = full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "            self.superclass_indices.add(super_idx)\n",
        "\n",
        "        self.superclass_indices = sorted(list(self.superclass_indices))\n",
        "        print(f\"Found superclasses with indices: {self.superclass_indices}\")\n",
        "\n",
        "    def cross_validate_novelty_detection(self, epochs=5, confidence_threshold=0.0):\n",
        "        results = []\n",
        "\n",
        "        # For each superclass, treat it as novel and others as known\n",
        "        for fold, novel_idx in enumerate(self.superclass_indices):\n",
        "            print(f\"\\n=== Fold {fold+1}/{len(self.superclass_indices)}: Treating superclass {novel_idx} as novel ===\")\n",
        "\n",
        "            # Create data splits\n",
        "            known_indices, novel_indices = self._split_by_superclass(novel_idx)\n",
        "\n",
        "            # Further split known indices into train/validation\n",
        "            np.random.shuffle(known_indices)\n",
        "            train_size = int(0.9 * len(known_indices))\n",
        "            train_indices = known_indices[:train_size]\n",
        "            val_known_indices = known_indices[train_size:]\n",
        "\n",
        "            # Create datasets\n",
        "            train_dataset = Subset(self.full_dataset, train_indices)\n",
        "            val_known_dataset = Subset(self.full_dataset, val_known_indices)\n",
        "            val_novel_dataset = Subset(self.full_dataset, novel_indices)\n",
        "\n",
        "            # Create dataloaders\n",
        "            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "            val_known_loader = DataLoader(val_known_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "            val_novel_loader = DataLoader(val_novel_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "            # Initialize model, loss, optimizer\n",
        "            model = CNN(input_size=64, num_superclasses=len(self.superclass_indices)+1).to(self.device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "            # Train the model\n",
        "            self._train_model(model, criterion, optimizer, train_loader, epochs)\n",
        "\n",
        "            # Calibrate energy statistics on training data\n",
        "            self._calibrate_energy_stats(model, train_loader)\n",
        "\n",
        "            # Evaluate novelty detection\n",
        "            metrics = self._evaluate_novelty_detection(model, val_known_loader, val_novel_loader, confidence_threshold)\n",
        "            results.append(metrics)\n",
        "\n",
        "            print(f\"Fold {fold+1} results:\")\n",
        "            for key, value in metrics.items():\n",
        "                print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "        # Calculate average results across folds\n",
        "        avg_results = {}\n",
        "        for key in results[0].keys():\n",
        "            avg_results[key] = sum(r[key] for r in results) / len(results)\n",
        "\n",
        "        print(\"\\n=== Cross-Validation Summary ===\")\n",
        "        for key, value in avg_results.items():\n",
        "            print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "        return avg_results, results\n",
        "\n",
        "    def find_optimal_threshold(self, fold_index=0, threshold_range=np.arange(-3.0, 3.0, 0.1)):\n",
        "        novel_idx = self.superclass_indices[fold_index]\n",
        "        print(f\"\\n=== Finding optimal threshold for fold {fold_index+1}: Superclass {novel_idx} as novel ===\")\n",
        "\n",
        "        # Create data splits\n",
        "        known_indices, novel_indices = self._split_by_superclass(novel_idx)\n",
        "\n",
        "        # Further split known indices into train/validation\n",
        "        np.random.shuffle(known_indices)\n",
        "        train_size = int(0.9 * len(known_indices))\n",
        "        train_indices = known_indices[:train_size]\n",
        "        val_known_indices = known_indices[train_size:]\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = Subset(self.full_dataset, train_indices)\n",
        "        val_known_dataset = Subset(self.full_dataset, val_known_indices)\n",
        "        val_novel_dataset = Subset(self.full_dataset, novel_indices)\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_known_loader = DataLoader(val_known_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        val_novel_loader = DataLoader(val_novel_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        # Initialize model, loss, optimizer\n",
        "        model = CNN(input_size=64, num_superclasses=len(self.superclass_indices)+1).to(self.device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "        # Train the model\n",
        "        self._train_model(model, criterion, optimizer, train_loader, epochs=5)\n",
        "\n",
        "        # Calibrate energy statistics on training data\n",
        "        self._calibrate_energy_stats(model, train_loader)\n",
        "\n",
        "        # Collect all normalized energy scores\n",
        "        known_energies, novel_energies = self._collect_energies(model, val_known_loader, val_novel_loader)\n",
        "\n",
        "        # Evaluate different thresholds\n",
        "        results = []\n",
        "        for threshold in threshold_range:\n",
        "\n",
        "            known_correct = sum(1 for e in known_energies if e <= threshold)\n",
        "            known_accuracy = known_correct / len(known_energies) if known_energies else 0\n",
        "\n",
        "\n",
        "            novel_correct = sum(1 for e in novel_energies if e > threshold)\n",
        "            novel_accuracy = novel_correct / len(novel_energies) if novel_energies else 0\n",
        "\n",
        "\n",
        "            balanced_accuracy = (known_accuracy + novel_accuracy) / 2\n",
        "\n",
        "            results.append({\n",
        "                'threshold': threshold,\n",
        "                'known_accuracy': known_accuracy * 100,\n",
        "                'novel_accuracy': novel_accuracy * 100,\n",
        "                'balanced_accuracy': balanced_accuracy * 100\n",
        "            })\n",
        "\n",
        "            print(f\"Threshold {threshold:.2f}: Known Acc={known_accuracy:.4f}, Novel Acc={novel_accuracy:.4f}, Balanced Acc={balanced_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "        valid_thresholds = []\n",
        "        for result in results:\n",
        "\n",
        "            if result['known_accuracy'] >= self.min_known_acc and result['novel_accuracy'] >= self.min_novel_acc:\n",
        "                valid_thresholds.append(result)\n",
        "\n",
        "        if valid_thresholds:\n",
        "\n",
        "            best_result = max(valid_thresholds, key=lambda x: x['balanced_accuracy'])\n",
        "            print(f\"\\nFound threshold meeting criteria (known ≥{self.min_known_acc}%, novel ≥{self.min_novel_acc}%):\")\n",
        "        else:\n",
        "\n",
        "            print(f\"\\nNo threshold meets both criteria (known ≥{self.min_known_acc}%, novel ≥{self.min_novel_acc}%)\")\n",
        "            print(\"Using pre-selected threshold of 1.9 from cross-validation...\")\n",
        "\n",
        "            # Find result closest to threshold 1.9\n",
        "            best_result = min(results, key=lambda x: abs(x['threshold'] - 1.9))\n",
        "\n",
        "        print(f\"Best threshold: {best_result['threshold']:.2f}\")\n",
        "        print(f\"Known accuracy: {best_result['known_accuracy']:.4f}\")\n",
        "        print(f\"Novel accuracy: {best_result['novel_accuracy']:.4f}\")\n",
        "        print(f\"Balanced accuracy: {best_result['balanced_accuracy']:.4f}\")\n",
        "\n",
        "        return best_result['threshold'], results\n",
        "\n",
        "    def _calibrate_energy_stats(self, model, loader):\n",
        "      \"\"\"Calculate energy statistics on a dataset for normalization\"\"\"\n",
        "      model.eval()\n",
        "      all_energies = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for data in loader:\n",
        "              inputs = data[0].to(self.device)\n",
        "\n",
        "              # Get model outputs\n",
        "              super_outputs, _ = model(inputs)\n",
        "\n",
        "              # Calculate raw energy\n",
        "              energies = -torch.logsumexp(super_outputs, dim=1)\n",
        "              all_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "      all_energies = np.array(all_energies)\n",
        "      self.energy_mean = float(np.mean(all_energies))\n",
        "      self.energy_std = float(np.std(all_energies) + 1e-6)\n",
        "\n",
        "      print(f\"Calibrated energy statistics: mean={self.energy_mean:.4f}, std={self.energy_std:.4f}\")\n",
        "\n",
        "    def _compute_normalized_energy(self, logits):\n",
        "      \"\"\"Compute normalized energy scores\"\"\"\n",
        "\n",
        "      raw_energy = -torch.logsumexp(logits, dim=1)\n",
        "\n",
        "\n",
        "      normalized_energy = (raw_energy - self.energy_mean) / self.energy_std\n",
        "\n",
        "      return normalized_energy\n",
        "\n",
        "    def _split_by_superclass(self, novel_superclass_idx):\n",
        "        \"\"\"Split dataset indices into known and novel based on superclass\"\"\"\n",
        "        known_indices = []\n",
        "        novel_indices = []\n",
        "\n",
        "        for i in range(len(self.full_dataset)):\n",
        "            _, super_idx, _, _, _ = self.full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "\n",
        "            if super_idx == novel_superclass_idx:\n",
        "                novel_indices.append(i)\n",
        "            else:\n",
        "                known_indices.append(i)\n",
        "\n",
        "        return known_indices, novel_indices\n",
        "\n",
        "    def _train_model(self, model, criterion, optimizer, train_loader, epochs):\n",
        "        \"\"\"Train the model on known classes\"\"\"\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(train_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                super_labels = super_labels.to(self.device)\n",
        "                sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                super_outputs, sub_outputs = model(inputs)\n",
        "                loss = criterion(super_outputs, super_labels) + criterion(sub_outputs, sub_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "    def _evaluate_novelty_detection(self, model, known_loader, novel_loader, threshold):\n",
        "      \"\"\"Evaluate novelty detection performance using balanced ensemble approach.\"\"\"\n",
        "      model.eval()\n",
        "\n",
        "      # First calibrate energy statistics on known data\n",
        "      self._calibrate_energy_stats(model, known_loader)\n",
        "\n",
        "      def eval_loader(loader, is_novel):\n",
        "          super_correct, sub_correct = 0, 0\n",
        "          super_total, sub_total = 0, 0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for data in loader:\n",
        "                  inputs, _, _, _, _ = data\n",
        "                  inputs = inputs.to(self.device)\n",
        "\n",
        "                  super_outputs, sub_outputs = model(inputs)\n",
        "\n",
        "                  # --- Energy-based detection with FIXED threshold ---\n",
        "                  super_energies = self._compute_normalized_energy(super_outputs)\n",
        "                  # Use the fixed threshold of 1.9 to maintain high known accuracy\n",
        "                  energy_novel = super_energies > self.energy_threshold  # self.energy_threshold is 1.9\n",
        "\n",
        "\n",
        "                  super_probs = F.softmax(super_outputs, dim=1)\n",
        "                  super_confidences, _ = torch.max(super_probs, dim=1)\n",
        "                  confidence_novel = super_confidences < 0.7  # Adjust this threshold\n",
        "\n",
        "\n",
        "                  energy_weight = 0.6\n",
        "                  confidence_weight = 0.4\n",
        "\n",
        "\n",
        "                  novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "\n",
        "                  is_novel_super = novelty_score > 0.5\n",
        "\n",
        "\n",
        "                  sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "                  sub_confidences, _ = torch.max(sub_probs, dim=1)\n",
        "                  is_novel_sub = sub_confidences < 0.5\n",
        "\n",
        "\n",
        "                  if is_novel:\n",
        "                      super_correct += is_novel_super.sum().item()\n",
        "                      sub_correct += is_novel_sub.sum().item()\n",
        "                  else:\n",
        "                      super_correct += (~is_novel_super).sum().item()\n",
        "                      sub_correct += (~is_novel_sub).sum().item()\n",
        "\n",
        "                  super_total += inputs.size(0)\n",
        "                  sub_total += inputs.size(0)\n",
        "\n",
        "          return (\n",
        "              super_correct / super_total if super_total else 0,\n",
        "              sub_correct / sub_total if sub_total else 0\n",
        "          )\n",
        "\n",
        "\n",
        "      known_super_acc, known_sub_acc = eval_loader(known_loader, is_novel=False)\n",
        "      novel_super_acc, novel_sub_acc = eval_loader(novel_loader, is_novel=True)\n",
        "\n",
        "      balanced_super_acc = (known_super_acc + novel_super_acc) / 2\n",
        "      balanced_sub_acc = (known_sub_acc + novel_sub_acc) / 2\n",
        "\n",
        "\n",
        "      known_req_met = known_super_acc * 100 >= self.min_known_acc\n",
        "      novel_req_met = novel_super_acc * 100 >= self.min_novel_acc\n",
        "\n",
        "      if known_req_met and novel_req_met:\n",
        "          print(f\"✓ Requirements met: known={known_super_acc*100:.2f}%, novel={novel_super_acc*100:.2f}%\")\n",
        "      else:\n",
        "          print(f\"✗ Requirements not met:\")\n",
        "          if not known_req_met:\n",
        "              print(f\"  Known accuracy {known_super_acc*100:.2f}% < {self.min_known_acc}% requirement\")\n",
        "          if not novel_req_met:\n",
        "              print(f\"  Novel accuracy {novel_super_acc*100:.2f}% < {self.min_novel_acc}% requirement\")\n",
        "\n",
        "      return {\n",
        "          'known_superclass_accuracy': known_super_acc,\n",
        "          'novel_superclass_accuracy': novel_super_acc,\n",
        "          'balanced_superclass_accuracy': balanced_super_acc,\n",
        "          'known_subclass_accuracy': known_sub_acc,\n",
        "          'novel_subclass_accuracy': novel_sub_acc,\n",
        "          'balanced_subclass_accuracy': balanced_sub_acc\n",
        "      }\n",
        "\n",
        "    def _collect_energies(self, model, known_loader, novel_loader):\n",
        "        \"\"\"Collect normalized energy scores for known and novel classes\"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        known_energies = []\n",
        "        novel_energies = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Known classes\n",
        "            for data in known_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                super_outputs, _ = model(inputs)\n",
        "                energies = self._compute_normalized_energy(super_outputs)\n",
        "                known_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "            # Novel classes\n",
        "            for data in novel_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                super_outputs, _ = model(inputs)\n",
        "                energies = self._compute_normalized_energy(super_outputs)\n",
        "                novel_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "        return known_energies, novel_energies\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda',\n",
        "                min_known_acc=95, min_novel_acc=20):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        self.energy_mean = 0\n",
        "        self.energy_std = 1\n",
        "        self.energy_calibrated = False\n",
        "\n",
        "\n",
        "        self.min_known_acc = min_known_acc\n",
        "        self.min_novel_acc = min_novel_acc\n",
        "\n",
        "        self.energy_threshold = 1.9\n",
        "\n",
        "        # Add scheduler\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "        )\n",
        "\n",
        "        # Store temperature parameter\n",
        "        self.temperature = 1.5\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/(i+1):.3f}')\n",
        "        avg_loss = running_loss/(i+1)\n",
        "        self.scheduler.step(avg_loss)\n",
        "\n",
        "        # Recalibrate energy statistics after each epoch\n",
        "        self._calibrate_energy_stats()\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "    def _calibrate_energy_stats(self):\n",
        "        \"\"\"Calculate energy statistics on training data for normalization\"\"\"\n",
        "        self.model.eval()\n",
        "        all_energies = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in self.train_loader:\n",
        "                inputs = data[0].to(self.device)\n",
        "\n",
        "                # Get model outputs\n",
        "                super_outputs, _ = self.model(inputs)\n",
        "\n",
        "                # Calculate raw energy\n",
        "                energies = -torch.logsumexp(super_outputs, dim=1)\n",
        "                all_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "\n",
        "        all_energies = np.array(all_energies)\n",
        "        self.energy_mean = float(np.mean(all_energies))\n",
        "        self.energy_std = float(np.std(all_energies) + 1e-6)\n",
        "        self.energy_calibrated = True\n",
        "\n",
        "        print(f\"Calibrated energy statistics: mean={self.energy_mean:.4f}, std={self.energy_std:.4f}\")\n",
        "\n",
        "    def compute_normalized_energy(self, logits):\n",
        "        \"\"\"Compute normalized energy scores\"\"\"\n",
        "        # Calculate raw energy\n",
        "        raw_energy = -torch.logsumexp(logits, dim=1)\n",
        "\n",
        "        # Normalize using stored statistics\n",
        "        if not self.energy_calibrated:\n",
        "            # If not calibrated, just return raw energy\n",
        "            print(\"Warning: Energy statistics not calibrated, using raw energy\")\n",
        "            return raw_energy\n",
        "\n",
        "        normalized_energy = (raw_energy - self.energy_mean) / self.energy_std\n",
        "\n",
        "        return normalized_energy\n",
        "\n",
        "    def validate_epoch(self, novel_superclass_idx=3, novel_subclass_idx=87):\n",
        "      \"\"\"\n",
        "      Validate the model with balanced ensemble novelty detection approach\n",
        "      using FIXED threshold of 1.9 for better known accuracy.\n",
        "      \"\"\"\n",
        "      # Make sure energy statistics are calibrated\n",
        "      if not self.energy_calibrated:\n",
        "          self._calibrate_energy_stats()\n",
        "\n",
        "      self.model.eval()\n",
        "\n",
        "      # Metrics to track\n",
        "      correct_with_novelty = 0\n",
        "      super_correct_standard = 0\n",
        "      sub_correct = 0\n",
        "\n",
        "      novel_total = 0\n",
        "      known_total = 0\n",
        "      novel_correct = 0\n",
        "      known_correct = 0\n",
        "\n",
        "      total = 0\n",
        "\n",
        "      novel_super_predictions = 0\n",
        "      novel_sub_predictions = 0\n",
        "\n",
        "      all_super_energies = []\n",
        "      all_sub_confidences = []\n",
        "\n",
        "      running_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for i, data in enumerate(self.val_loader):\n",
        "              inputs, super_labels, _, sub_labels, _ = data\n",
        "              inputs = inputs.to(self.device)\n",
        "              super_labels = super_labels.to(self.device)\n",
        "              sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "              super_outputs, sub_outputs = self.model(inputs)\n",
        "\n",
        "              # Normalized energy scores with FIXED threshold (1.9)\n",
        "              super_energies = self.compute_normalized_energy(super_outputs)\n",
        "              energy_novel = super_energies > self.energy_threshold  # Using fixed threshold 1.9\n",
        "\n",
        "              # Confidence scores\n",
        "              super_probs = F.softmax(super_outputs, dim=1)\n",
        "              super_confidences, super_predicted = torch.max(super_probs, dim=1)\n",
        "\n",
        "              # Confidence threshold can be adjusted\n",
        "              conf_threshold = 0.7\n",
        "              confidence_novel = super_confidences < conf_threshold\n",
        "\n",
        "\n",
        "              energy_weight = 0.6\n",
        "              confidence_weight = 0.4\n",
        "\n",
        "              # Calculate weighted novelty score (0-1 range)\n",
        "              novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "\n",
        "\n",
        "              decision_threshold = 0.5  # 0.5 is balanced\n",
        "              novel_super_mask = novelty_score > decision_threshold\n",
        "\n",
        "              # Create final predictions\n",
        "              final_super_preds = torch.where(\n",
        "                  novel_super_mask,\n",
        "                  torch.full_like(super_predicted, novel_superclass_idx),\n",
        "                  super_predicted\n",
        "              )\n",
        "\n",
        "              # Subclass confidence-based detection\n",
        "              sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "              sub_confidences, sub_predicted = torch.max(sub_probs, dim=1)\n",
        "              sub_threshold = 0.5\n",
        "              novel_sub_mask = sub_confidences < sub_threshold\n",
        "\n",
        "              final_sub_preds = torch.where(\n",
        "                  novel_sub_mask,\n",
        "                  torch.full_like(sub_predicted, novel_subclass_idx),\n",
        "                  sub_predicted\n",
        "              )\n",
        "\n",
        "\n",
        "              total += super_labels.size(0)\n",
        "\n",
        "\n",
        "              correct_with_novelty += (final_super_preds == super_labels).sum().item()\n",
        "              super_correct_standard += (super_predicted == super_labels).sum().item()\n",
        "              sub_correct += (final_sub_preds == sub_labels).sum().item()\n",
        "\n",
        "\n",
        "              is_novel_label = super_labels == novel_superclass_idx\n",
        "              novel_total += is_novel_label.sum().item()\n",
        "              known_total += (~is_novel_label).sum().item()\n",
        "\n",
        "              novel_correct += ((final_super_preds == super_labels) & is_novel_label).sum().item()\n",
        "              known_correct += ((final_super_preds == super_labels) & ~is_novel_label).sum().item()\n",
        "\n",
        "\n",
        "              novel_super_predictions += novel_super_mask.sum().item()\n",
        "              novel_sub_predictions += novel_sub_mask.sum().item()\n",
        "\n",
        "\n",
        "              all_super_energies.extend(super_energies.cpu().numpy())\n",
        "              all_sub_confidences.extend(sub_confidences.cpu().numpy())\n",
        "\n",
        "              # Calculate loss\n",
        "              loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "              running_loss += loss.item()\n",
        "\n",
        "\n",
        "      super_acc = 100 * correct_with_novelty / total if total > 0 else 0\n",
        "      sub_acc = 100 * sub_correct / total if total > 0 else 0\n",
        "\n",
        "      novel_acc = 100 * novel_correct / novel_total if novel_total > 0 else 0\n",
        "      known_acc = 100 * known_correct / known_total if known_total > 0 else 0\n",
        "      balanced_acc = (novel_acc + known_acc) / 2 if novel_total > 0 and known_total > 0 else 0\n",
        "\n",
        "      avg_super_energy = sum(all_super_energies) / len(all_super_energies) if all_super_energies else 0\n",
        "      avg_sub_conf = sum(all_sub_confidences) / len(all_sub_confidences) if all_sub_confidences else 0\n",
        "\n",
        "      novel_super_perc = 100 * novel_super_predictions / total if total > 0 else 0\n",
        "      novel_sub_perc = 100 * novel_sub_predictions / total if total > 0 else 0\n",
        "\n",
        "      # Display metrics\n",
        "      print(f'Validation loss: {running_loss/(i+1):.3f}')\n",
        "      print(f'Validation superclass acc: {super_acc:.2f}%')\n",
        "      print(f'Validation subclass acc: {sub_acc:.2f}%')\n",
        "      print(f'Novel superclass acc: {novel_acc:.2f}%, Known superclass acc: {known_acc:.2f}%')\n",
        "      print(f'Balanced superclass acc: {balanced_acc:.2f}%')\n",
        "      print(f'Average normalized superclass energy: {avg_super_energy:.4f}')\n",
        "      print(f'Average subclass confidence: {avg_sub_conf:.4f}')\n",
        "      print(f'Samples predicted as novel superclass: {novel_super_predictions} ({novel_super_perc:.2f}%)')\n",
        "      print(f'Samples predicted as novel subclass: {novel_sub_predictions} ({novel_sub_perc:.2f}%)')\n",
        "\n",
        "      # Check if requirements are met\n",
        "      requirements_met = known_acc >= self.min_known_acc and novel_acc >= self.min_novel_acc\n",
        "\n",
        "      if requirements_met:\n",
        "          print(f\"✓ REQUIREMENTS MET: known={known_acc:.2f}% ≥ {self.min_known_acc}%, novel={novel_acc:.2f}% ≥ {self.min_novel_acc}%\")\n",
        "      else:\n",
        "          print(f\"✗ REQUIREMENTS NOT MET:\")\n",
        "          if known_acc < self.min_known_acc:\n",
        "              print(f\"  Known accuracy {known_acc:.2f}% < {self.min_known_acc}% requirement\")\n",
        "          if novel_acc < self.min_novel_acc:\n",
        "              print(f\"  Novel accuracy {novel_acc:.2f}% < {self.min_novel_acc}% requirement\")\n",
        "\n",
        "      return {\n",
        "          'loss': running_loss/(i+1),\n",
        "          'accuracy': super_acc,\n",
        "          'novel_acc': novel_acc,\n",
        "          'known_acc': known_acc,\n",
        "          'balanced_acc': balanced_acc\n",
        "      }\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False, output_file='example_test_predictions.csv'):\n",
        "      \"\"\"\n",
        "      Test the model with fixed threshold of 1.9 for higher known accuracy\n",
        "      \"\"\"\n",
        "      if not self.test_loader:\n",
        "          raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "      # Make sure energy statistics are calibrated\n",
        "      if not self.energy_calibrated:\n",
        "          self._calibrate_energy_stats()\n",
        "\n",
        "      self.model.eval()\n",
        "      novel_superclass_idx = 3  # Index for novel superclass\n",
        "      novel_subclass_idx = 87   # Index for novel subclass\n",
        "\n",
        "      # Create full data structure for internal use\n",
        "      full_test_predictions = {\n",
        "          'image': [],\n",
        "          'superclass_index': [],\n",
        "          'subclass_index': [],\n",
        "          'superclass_energy': [],\n",
        "          'subclass_confidence': [],\n",
        "          'novelty_score': []\n",
        "      }\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for i, data in enumerate(self.test_loader):\n",
        "              inputs, img_name = data[0].to(self.device), data[1]\n",
        "\n",
        "              super_outputs, sub_outputs = self.model(inputs)\n",
        "\n",
        "              # Normalized energy with FIXED threshold (1.9)\n",
        "              super_energies = self.compute_normalized_energy(super_outputs)\n",
        "              energy_novel = super_energies > self.energy_threshold  # Fixed at 1.9\n",
        "\n",
        "              # Confidence scores for superclasses\n",
        "              super_probs = F.softmax(super_outputs, dim=1)\n",
        "              super_confidences, super_predicted = torch.max(super_probs, dim=1)\n",
        "\n",
        "              # Confidence threshold\n",
        "              conf_threshold = 0.7\n",
        "              confidence_novel = super_confidences < conf_threshold\n",
        "\n",
        "              # BALANCED APPROACH - weighted voting\n",
        "              energy_weight = 0.6\n",
        "              confidence_weight = 0.4\n",
        "\n",
        "              novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "              decision_threshold = 0.5\n",
        "              novel_super_mask = novelty_score > decision_threshold\n",
        "\n",
        "              # Subclass confidence-based detection\n",
        "              sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "              sub_confidences, sub_predicted = torch.max(sub_probs, dim=1)\n",
        "              sub_threshold = 0.5\n",
        "              novel_sub_mask = sub_confidences < sub_threshold\n",
        "\n",
        "              for j in range(inputs.size(0)):\n",
        "                  img = img_name[j] if isinstance(img_name, list) else img_name[0]\n",
        "\n",
        "                  # Apply novelty detection\n",
        "                  super_pred = novel_superclass_idx if novel_super_mask[j] else super_predicted[j].item()\n",
        "                  sub_pred = novel_subclass_idx if novel_sub_mask[j] else sub_predicted[j].item()\n",
        "\n",
        "                  full_test_predictions['image'].append(img)\n",
        "                  full_test_predictions['superclass_index'].append(super_pred)\n",
        "                  full_test_predictions['subclass_index'].append(sub_pred)\n",
        "                  full_test_predictions['superclass_energy'].append(super_energies[j].item())\n",
        "                  full_test_predictions['subclass_confidence'].append(sub_confidences[j].item())\n",
        "                  full_test_predictions['novelty_score'].append(novelty_score[j].item())\n",
        "\n",
        "      full_predictions_df = pd.DataFrame(data=full_test_predictions)\n",
        "\n",
        "      simplified_test_predictions = {\n",
        "          'image': full_test_predictions['image'],\n",
        "          'superclass_index': full_test_predictions['superclass_index'],\n",
        "          'subclass_index': full_test_predictions['subclass_index']\n",
        "      }\n",
        "      simplified_predictions_df = pd.DataFrame(data=simplified_test_predictions)\n",
        "\n",
        "      # Summarize\n",
        "      novel_super_count = sum(1 for idx in full_test_predictions['superclass_index'] if idx == novel_superclass_idx)\n",
        "      novel_sub_count = sum(1 for idx in full_test_predictions['subclass_index'] if idx == novel_subclass_idx)\n",
        "\n",
        "      total_count = len(full_test_predictions['image'])\n",
        "      novel_super_perc = 100 * novel_super_count / total_count if total_count > 0 else 0\n",
        "      novel_sub_perc = 100 * novel_sub_count / total_count if total_count > 0 else 0\n",
        "\n",
        "      print(f'Test set predictions:')\n",
        "      print(f'Images predicted as novel superclass: {novel_super_count} ({novel_super_perc:.2f}%)')\n",
        "      print(f'Images predicted as novel subclass: {novel_sub_count} ({novel_sub_perc:.2f}%)')\n",
        "\n",
        "      # Print distribution of novelty scores to help with threshold tuning\n",
        "      print(f'Novelty score distribution:')\n",
        "      bins = [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
        "      for i in range(len(bins)-1):\n",
        "          count = sum(1 for score in full_test_predictions['novelty_score']\n",
        "                    if bins[i] <= score < bins[i+1])\n",
        "          print(f'  {bins[i]:.1f}-{bins[i+1]:.1f}: {count} ({100*count/total_count:.2f}%)')\n",
        "\n",
        "      if save_to_csv:\n",
        "          # Save in the same format as the first method\n",
        "          simplified_predictions_df.to_csv(output_file, index=False)\n",
        "          print(f\"Predictions saved to '{output_file}'\")\n",
        "\n",
        "      if return_predictions:\n",
        "          # Return the full predictions for internal use\n",
        "          return full_predictions_df\n",
        "\n",
        "\n",
        "# Helper function to run the cross-validation and find optimal threshold\n",
        "def train_with_novelty_detection(full_dataset, image_preprocessing, device='cuda', batch_size=64, epochs=5,\n",
        "                               min_known_acc=95, min_novel_acc=20):\n",
        "    # Initialize novelty detection trainer with requirements\n",
        "    novelty_trainer = NoveltyDetectionTrainer(\n",
        "        full_dataset=full_dataset,\n",
        "        image_preprocessing=image_preprocessing,\n",
        "        device=device,\n",
        "        batch_size=batch_size,\n",
        "        min_known_acc=min_known_acc,  # Minimum known accuracy requirement\n",
        "        min_novel_acc=min_novel_acc   # Minimum novel accuracy requirement\n",
        "    )\n",
        "\n",
        "    # Run cross-validation to evaluate novelty detection\n",
        "    print(\"Running cross-validation for novelty detection...\")\n",
        "    avg_results, fold_results = novelty_trainer.cross_validate_novelty_detection(epochs=epochs)\n",
        "\n",
        "    # Find optimal threshold\n",
        "    print(\"\\nFinding optimal energy threshold...\")\n",
        "    best_threshold, threshold_results = novelty_trainer.find_optimal_threshold()\n",
        "\n",
        "    return avg_results, best_threshold"
      ],
      "metadata": {
        "id": "8ZwTsSjAAJXf"
      },
      "id": "8ZwTsSjAAJXf",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "min_known_acc = 95  # Minimum known class accuracy (95%)\n",
        "min_novel_acc = 20  # Minimum novel class accuracy (20%)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "avg_results, best_threshold = train_with_novelty_detection(\n",
        "    full_dataset=full_dataset,\n",
        "    image_preprocessing=image_preprocessing,\n",
        "    device=device,\n",
        "    batch_size=64,\n",
        "    epochs=5,\n",
        "    min_known_acc=min_known_acc,\n",
        "    min_novel_acc=min_novel_acc\n",
        ")\n",
        "\n",
        "print(f\"Cross-validation complete. Best threshold: {best_threshold}\")\n",
        "\n",
        "\n",
        "model = CNN(input_size=64, num_superclasses=4, num_subclasses=88).to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00005)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    min_known_acc=min_known_acc,\n",
        "    min_novel_acc=min_novel_acc\n",
        ")\n",
        "\n",
        "\n",
        "num_epochs = 15\n",
        "print(f\"Training for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    trainer.train_epoch()\n",
        "    metrics = trainer.validate_epoch()\n",
        "\n",
        "\n",
        "    print(f\"Metrics after epoch {epoch+1}:\")\n",
        "    print(f\"  Loss: {metrics['loss']:.4f}\")\n",
        "    print(f\"  Known accuracy: {metrics['known_acc']:.2f}%\")\n",
        "    print(f\"  Novel accuracy: {metrics['novel_acc']:.2f}%\")\n",
        "    print(f\"  Balanced accuracy: {metrics['balanced_acc']:.2f}%\")\n",
        "\n",
        "print(\"\\nTesting final model...\")\n",
        "test_results = trainer.test(save_to_csv=True, output_file='fixed_threshold_predictions.csv')\n",
        "\n",
        "print(\"Training and testing complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMdbg8mTAiyR",
        "outputId": "38ec2d1a-a0f5-42ed-8269-ea75346c44bf"
      },
      "id": "vMdbg8mTAiyR",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Found superclasses with indices: [0, 1, 2]\n",
            "Running cross-validation for novelty detection...\n",
            "\n",
            "=== Fold 1/3: Treating superclass 0 as novel ===\n",
            "Epoch 1/5, Loss: 3.4655\n",
            "Epoch 2/5, Loss: 1.9756\n",
            "Epoch 3/5, Loss: 1.3872\n",
            "Epoch 4/5, Loss: 1.0365\n",
            "Epoch 5/5, Loss: 0.8144\n",
            "Calibrated energy statistics: mean=-8.3460, std=2.5859\n",
            "Calibrated energy statistics: mean=-8.2152, std=2.4908\n",
            "✓ Requirements met: known=96.62%, novel=44.81%\n",
            "Fold 1 results:\n",
            "  known_superclass_accuracy: 0.9662\n",
            "  novel_superclass_accuracy: 0.4481\n",
            "  balanced_superclass_accuracy: 0.7072\n",
            "  known_subclass_accuracy: 0.8243\n",
            "  novel_subclass_accuracy: 0.5524\n",
            "  balanced_subclass_accuracy: 0.6884\n",
            "\n",
            "=== Fold 2/3: Treating superclass 1 as novel ===\n",
            "Epoch 1/5, Loss: 3.5474\n",
            "Epoch 2/5, Loss: 2.0337\n",
            "Epoch 3/5, Loss: 1.5333\n",
            "Epoch 4/5, Loss: 1.1356\n",
            "Epoch 5/5, Loss: 0.9279\n",
            "Calibrated energy statistics: mean=-6.8824, std=2.5071\n",
            "Calibrated energy statistics: mean=-6.8908, std=2.5935\n",
            "✗ Requirements not met:\n",
            "  Novel accuracy 11.90% < 20% requirement\n",
            "Fold 2 results:\n",
            "  known_superclass_accuracy: 0.9905\n",
            "  novel_superclass_accuracy: 0.1190\n",
            "  balanced_superclass_accuracy: 0.5548\n",
            "  known_subclass_accuracy: 0.8076\n",
            "  novel_subclass_accuracy: 0.4942\n",
            "  balanced_subclass_accuracy: 0.6509\n",
            "\n",
            "=== Fold 3/3: Treating superclass 2 as novel ===\n",
            "Epoch 1/5, Loss: 3.5458\n",
            "Epoch 2/5, Loss: 2.0853\n",
            "Epoch 3/5, Loss: 1.4599\n",
            "Epoch 4/5, Loss: 1.0431\n",
            "Epoch 5/5, Loss: 0.8303\n",
            "Calibrated energy statistics: mean=-10.1823, std=2.5535\n",
            "Calibrated energy statistics: mean=-9.6907, std=2.7649\n",
            "✓ Requirements met: known=97.21%, novel=25.19%\n",
            "Fold 3 results:\n",
            "  known_superclass_accuracy: 0.9721\n",
            "  novel_superclass_accuracy: 0.2519\n",
            "  balanced_superclass_accuracy: 0.6120\n",
            "  known_subclass_accuracy: 0.7970\n",
            "  novel_subclass_accuracy: 0.5157\n",
            "  balanced_subclass_accuracy: 0.6563\n",
            "\n",
            "=== Cross-Validation Summary ===\n",
            "known_superclass_accuracy: 0.9763\n",
            "novel_superclass_accuracy: 0.2730\n",
            "balanced_superclass_accuracy: 0.6246\n",
            "known_subclass_accuracy: 0.8096\n",
            "novel_subclass_accuracy: 0.5208\n",
            "balanced_subclass_accuracy: 0.6652\n",
            "\n",
            "Finding optimal energy threshold...\n",
            "\n",
            "=== Finding optimal threshold for fold 1: Superclass 0 as novel ===\n",
            "Epoch 1/5, Loss: 3.6178\n",
            "Epoch 2/5, Loss: 2.2618\n",
            "Epoch 3/5, Loss: 1.6600\n",
            "Epoch 4/5, Loss: 1.2083\n",
            "Epoch 5/5, Loss: 0.9730\n",
            "Calibrated energy statistics: mean=-6.7593, std=2.6397\n",
            "Threshold -3.00: Known Acc=0.0090, Novel Acc=1.0000, Balanced Acc=0.5045\n",
            "Threshold -2.90: Known Acc=0.0113, Novel Acc=1.0000, Balanced Acc=0.5056\n",
            "Threshold -2.80: Known Acc=0.0135, Novel Acc=1.0000, Balanced Acc=0.5068\n",
            "Threshold -2.70: Known Acc=0.0135, Novel Acc=1.0000, Balanced Acc=0.5068\n",
            "Threshold -2.60: Known Acc=0.0180, Novel Acc=1.0000, Balanced Acc=0.5090\n",
            "Threshold -2.50: Known Acc=0.0203, Novel Acc=1.0000, Balanced Acc=0.5101\n",
            "Threshold -2.40: Known Acc=0.0225, Novel Acc=1.0000, Balanced Acc=0.5113\n",
            "Threshold -2.30: Known Acc=0.0248, Novel Acc=0.9995, Balanced Acc=0.5121\n",
            "Threshold -2.20: Known Acc=0.0248, Novel Acc=0.9995, Balanced Acc=0.5121\n",
            "Threshold -2.10: Known Acc=0.0270, Novel Acc=0.9989, Balanced Acc=0.5130\n",
            "Threshold -2.00: Known Acc=0.0315, Novel Acc=0.9978, Balanced Acc=0.5147\n",
            "Threshold -1.90: Known Acc=0.0338, Novel Acc=0.9962, Balanced Acc=0.5150\n",
            "Threshold -1.80: Known Acc=0.0360, Novel Acc=0.9930, Balanced Acc=0.5145\n",
            "Threshold -1.70: Known Acc=0.0473, Novel Acc=0.9886, Balanced Acc=0.5180\n",
            "Threshold -1.60: Known Acc=0.0518, Novel Acc=0.9849, Balanced Acc=0.5183\n",
            "Threshold -1.50: Known Acc=0.0608, Novel Acc=0.9795, Balanced Acc=0.5201\n",
            "Threshold -1.40: Known Acc=0.0766, Novel Acc=0.9751, Balanced Acc=0.5259\n",
            "Threshold -1.30: Known Acc=0.0901, Novel Acc=0.9665, Balanced Acc=0.5283\n",
            "Threshold -1.20: Known Acc=0.1014, Novel Acc=0.9595, Balanced Acc=0.5304\n",
            "Threshold -1.10: Known Acc=0.1239, Novel Acc=0.9524, Balanced Acc=0.5382\n",
            "Threshold -1.00: Known Acc=0.1329, Novel Acc=0.9465, Balanced Acc=0.5397\n",
            "Threshold -0.90: Known Acc=0.1486, Novel Acc=0.9335, Balanced Acc=0.5411\n",
            "Threshold -0.80: Known Acc=0.1779, Novel Acc=0.9211, Balanced Acc=0.5495\n",
            "Threshold -0.70: Known Acc=0.2072, Novel Acc=0.9043, Balanced Acc=0.5558\n",
            "Threshold -0.60: Known Acc=0.2275, Novel Acc=0.8859, Balanced Acc=0.5567\n",
            "Threshold -0.50: Known Acc=0.2770, Novel Acc=0.8643, Balanced Acc=0.5707\n",
            "Threshold -0.40: Known Acc=0.3153, Novel Acc=0.8432, Balanced Acc=0.5793\n",
            "Threshold -0.30: Known Acc=0.3649, Novel Acc=0.8184, Balanced Acc=0.5916\n",
            "Threshold -0.20: Known Acc=0.4054, Novel Acc=0.7919, Balanced Acc=0.5986\n",
            "Threshold -0.10: Known Acc=0.4527, Novel Acc=0.7676, Balanced Acc=0.6101\n",
            "Threshold 0.00: Known Acc=0.5000, Novel Acc=0.7416, Balanced Acc=0.6208\n",
            "Threshold 0.10: Known Acc=0.5315, Novel Acc=0.7124, Balanced Acc=0.6220\n",
            "Threshold 0.20: Known Acc=0.5518, Novel Acc=0.6805, Balanced Acc=0.6162\n",
            "Threshold 0.30: Known Acc=0.6059, Novel Acc=0.6454, Balanced Acc=0.6256\n",
            "Threshold 0.40: Known Acc=0.6464, Novel Acc=0.6076, Balanced Acc=0.6270\n",
            "Threshold 0.50: Known Acc=0.6847, Novel Acc=0.5751, Balanced Acc=0.6299\n",
            "Threshold 0.60: Known Acc=0.7297, Novel Acc=0.5286, Balanced Acc=0.6292\n",
            "Threshold 0.70: Known Acc=0.7500, Novel Acc=0.4838, Balanced Acc=0.6169\n",
            "Threshold 0.80: Known Acc=0.7883, Novel Acc=0.4443, Balanced Acc=0.6163\n",
            "Threshold 0.90: Known Acc=0.8198, Novel Acc=0.4054, Balanced Acc=0.6126\n",
            "Threshold 1.00: Known Acc=0.8514, Novel Acc=0.3638, Balanced Acc=0.6076\n",
            "Threshold 1.10: Known Acc=0.8649, Novel Acc=0.3232, Balanced Acc=0.5941\n",
            "Threshold 1.20: Known Acc=0.8964, Novel Acc=0.2816, Balanced Acc=0.5890\n",
            "Threshold 1.30: Known Acc=0.9189, Novel Acc=0.2443, Balanced Acc=0.5816\n",
            "Threshold 1.40: Known Acc=0.9347, Novel Acc=0.1957, Balanced Acc=0.5652\n",
            "Threshold 1.50: Known Acc=0.9459, Novel Acc=0.1600, Balanced Acc=0.5530\n",
            "Threshold 1.60: Known Acc=0.9595, Novel Acc=0.1243, Balanced Acc=0.5419\n",
            "Threshold 1.70: Known Acc=0.9640, Novel Acc=0.0951, Balanced Acc=0.5295\n",
            "Threshold 1.80: Known Acc=0.9730, Novel Acc=0.0751, Balanced Acc=0.5241\n",
            "Threshold 1.90: Known Acc=0.9775, Novel Acc=0.0600, Balanced Acc=0.5187\n",
            "Threshold 2.00: Known Acc=0.9842, Novel Acc=0.0378, Balanced Acc=0.5110\n",
            "Threshold 2.10: Known Acc=0.9910, Novel Acc=0.0270, Balanced Acc=0.5090\n",
            "Threshold 2.20: Known Acc=0.9977, Novel Acc=0.0178, Balanced Acc=0.5078\n",
            "Threshold 2.30: Known Acc=1.0000, Novel Acc=0.0081, Balanced Acc=0.5041\n",
            "Threshold 2.40: Known Acc=1.0000, Novel Acc=0.0043, Balanced Acc=0.5022\n",
            "Threshold 2.50: Known Acc=1.0000, Novel Acc=0.0016, Balanced Acc=0.5008\n",
            "Threshold 2.60: Known Acc=1.0000, Novel Acc=0.0011, Balanced Acc=0.5005\n",
            "Threshold 2.70: Known Acc=1.0000, Novel Acc=0.0011, Balanced Acc=0.5005\n",
            "Threshold 2.80: Known Acc=1.0000, Novel Acc=0.0000, Balanced Acc=0.5000\n",
            "Threshold 2.90: Known Acc=1.0000, Novel Acc=0.0000, Balanced Acc=0.5000\n",
            "\n",
            "No threshold meets both criteria (known ≥95%, novel ≥20%)\n",
            "Using pre-selected threshold of 1.9 from cross-validation...\n",
            "Best threshold: 1.90\n",
            "Known accuracy: 97.7477\n",
            "Novel accuracy: 6.0000\n",
            "Balanced accuracy: 51.8739\n",
            "Cross-validation complete. Best threshold: 1.900000000000004\n",
            "Training for 15 epochs...\n",
            "\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.930\n",
            "Calibrated energy statistics: mean=-3.3455, std=1.3192\n",
            "Validation loss: 3.110\n",
            "Validation superclass acc: 89.97%\n",
            "Validation subclass acc: 10.03%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 89.97%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1311\n",
            "Average subclass confidence: 0.3339\n",
            "Samples predicted as novel superclass: 21 (3.34%)\n",
            "Samples predicted as novel subclass: 515 (82.01%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 89.97% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 1:\n",
            "  Loss: 3.1102\n",
            "  Known accuracy: 89.97%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 2/15\n",
            "Training loss: 2.591\n",
            "Calibrated energy statistics: mean=-2.5247, std=1.0805\n",
            "Validation loss: 2.363\n",
            "Validation superclass acc: 92.83%\n",
            "Validation subclass acc: 28.82%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.83%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.0785\n",
            "Average subclass confidence: 0.4655\n",
            "Samples predicted as novel superclass: 19 (3.03%)\n",
            "Samples predicted as novel subclass: 387 (61.62%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 92.83% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 2:\n",
            "  Loss: 2.3634\n",
            "  Known accuracy: 92.83%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 3/15\n",
            "Training loss: 2.070\n",
            "Calibrated energy statistics: mean=-2.7706, std=1.0918\n",
            "Validation loss: 2.136\n",
            "Validation superclass acc: 92.36%\n",
            "Validation subclass acc: 39.97%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.36%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.0308\n",
            "Average subclass confidence: 0.5603\n",
            "Samples predicted as novel superclass: 19 (3.03%)\n",
            "Samples predicted as novel subclass: 283 (45.06%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 92.36% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 3:\n",
            "  Loss: 2.1356\n",
            "  Known accuracy: 92.36%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 4/15\n",
            "Training loss: 1.754\n",
            "Calibrated energy statistics: mean=-2.4131, std=0.9765\n",
            "Validation loss: 1.832\n",
            "Validation superclass acc: 94.11%\n",
            "Validation subclass acc: 52.87%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 94.11%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.0629\n",
            "Average subclass confidence: 0.6374\n",
            "Samples predicted as novel superclass: 24 (3.82%)\n",
            "Samples predicted as novel subclass: 208 (33.12%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 94.11% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 4:\n",
            "  Loss: 1.8319\n",
            "  Known accuracy: 94.11%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 5/15\n",
            "Training loss: 1.543\n",
            "Calibrated energy statistics: mean=-2.6465, std=1.0835\n",
            "Validation loss: 1.694\n",
            "Validation superclass acc: 96.34%\n",
            "Validation subclass acc: 58.12%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.34%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.0874\n",
            "Average subclass confidence: 0.6769\n",
            "Samples predicted as novel superclass: 10 (1.59%)\n",
            "Samples predicted as novel subclass: 187 (29.78%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 5:\n",
            "  Loss: 1.6939\n",
            "  Known accuracy: 96.34%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 6/15\n",
            "Training loss: 1.373\n",
            "Calibrated energy statistics: mean=-2.4887, std=0.9655\n",
            "Validation loss: 1.563\n",
            "Validation superclass acc: 95.86%\n",
            "Validation subclass acc: 60.99%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 95.86%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.0772\n",
            "Average subclass confidence: 0.7001\n",
            "Samples predicted as novel superclass: 17 (2.71%)\n",
            "Samples predicted as novel subclass: 158 (25.16%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 6:\n",
            "  Loss: 1.5630\n",
            "  Known accuracy: 95.86%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 7/15\n",
            "Training loss: 1.267\n",
            "Calibrated energy statistics: mean=-3.0284, std=1.1585\n",
            "Validation loss: 1.648\n",
            "Validation superclass acc: 96.50%\n",
            "Validation subclass acc: 64.33%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.50%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.0409\n",
            "Average subclass confidence: 0.7277\n",
            "Samples predicted as novel superclass: 12 (1.91%)\n",
            "Samples predicted as novel subclass: 133 (21.18%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 7:\n",
            "  Loss: 1.6475\n",
            "  Known accuracy: 96.50%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 8/15\n",
            "Training loss: 1.199\n",
            "Calibrated energy statistics: mean=-2.6808, std=0.9955\n",
            "Validation loss: 1.508\n",
            "Validation superclass acc: 93.95%\n",
            "Validation subclass acc: 67.83%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 93.95%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1207\n",
            "Average subclass confidence: 0.7393\n",
            "Samples predicted as novel superclass: 30 (4.78%)\n",
            "Samples predicted as novel subclass: 129 (20.54%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 93.95% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 8:\n",
            "  Loss: 1.5080\n",
            "  Known accuracy: 93.95%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 9/15\n",
            "Training loss: 1.114\n",
            "Calibrated energy statistics: mean=-2.6049, std=0.9492\n",
            "Validation loss: 1.509\n",
            "Validation superclass acc: 96.82%\n",
            "Validation subclass acc: 65.92%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.82%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1133\n",
            "Average subclass confidence: 0.7418\n",
            "Samples predicted as novel superclass: 16 (2.55%)\n",
            "Samples predicted as novel subclass: 131 (20.86%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 9:\n",
            "  Loss: 1.5087\n",
            "  Known accuracy: 96.82%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 10/15\n",
            "Training loss: 1.072\n",
            "Calibrated energy statistics: mean=-2.7413, std=0.9123\n",
            "Validation loss: 1.456\n",
            "Validation superclass acc: 93.15%\n",
            "Validation subclass acc: 67.83%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 93.15%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1611\n",
            "Average subclass confidence: 0.7482\n",
            "Samples predicted as novel superclass: 31 (4.94%)\n",
            "Samples predicted as novel subclass: 141 (22.45%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 93.15% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 10:\n",
            "  Loss: 1.4557\n",
            "  Known accuracy: 93.15%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 11/15\n",
            "Training loss: 1.047\n",
            "Calibrated energy statistics: mean=-2.7398, std=0.9525\n",
            "Validation loss: 1.449\n",
            "Validation superclass acc: 95.70%\n",
            "Validation subclass acc: 67.83%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 95.70%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1593\n",
            "Average subclass confidence: 0.7405\n",
            "Samples predicted as novel superclass: 19 (3.03%)\n",
            "Samples predicted as novel subclass: 146 (23.25%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 11:\n",
            "  Loss: 1.4489\n",
            "  Known accuracy: 95.70%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 12/15\n",
            "Training loss: 0.993\n",
            "Calibrated energy statistics: mean=-2.6906, std=0.9796\n",
            "Validation loss: 1.522\n",
            "Validation superclass acc: 94.59%\n",
            "Validation subclass acc: 68.31%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 94.59%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1861\n",
            "Average subclass confidence: 0.7604\n",
            "Samples predicted as novel superclass: 30 (4.78%)\n",
            "Samples predicted as novel subclass: 126 (20.06%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 94.59% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 12:\n",
            "  Loss: 1.5222\n",
            "  Known accuracy: 94.59%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 13/15\n",
            "Training loss: 0.958\n",
            "Calibrated energy statistics: mean=-2.4124, std=0.8187\n",
            "Validation loss: 1.366\n",
            "Validation superclass acc: 95.22%\n",
            "Validation subclass acc: 70.38%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 95.22%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1807\n",
            "Average subclass confidence: 0.7420\n",
            "Samples predicted as novel superclass: 28 (4.46%)\n",
            "Samples predicted as novel subclass: 143 (22.77%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 13:\n",
            "  Loss: 1.3656\n",
            "  Known accuracy: 95.22%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 14/15\n",
            "Training loss: 0.944\n",
            "Calibrated energy statistics: mean=-2.8552, std=0.8953\n",
            "Validation loss: 1.361\n",
            "Validation superclass acc: 95.06%\n",
            "Validation subclass acc: 71.66%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 95.06%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.2257\n",
            "Average subclass confidence: 0.7696\n",
            "Samples predicted as novel superclass: 28 (4.46%)\n",
            "Samples predicted as novel subclass: 122 (19.43%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 14:\n",
            "  Loss: 1.3614\n",
            "  Known accuracy: 95.06%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Epoch 15/15\n",
            "Training loss: 0.928\n",
            "Calibrated energy statistics: mean=-2.7735, std=0.8220\n",
            "Validation loss: 1.389\n",
            "Validation superclass acc: 91.88%\n",
            "Validation subclass acc: 69.59%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 91.88%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.2700\n",
            "Average subclass confidence: 0.7495\n",
            "Samples predicted as novel superclass: 49 (7.80%)\n",
            "Samples predicted as novel subclass: 133 (21.18%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 91.88% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Metrics after epoch 15:\n",
            "  Loss: 1.3887\n",
            "  Known accuracy: 91.88%\n",
            "  Novel accuracy: 0.00%\n",
            "  Balanced accuracy: 0.00%\n",
            "\n",
            "Testing final model...\n",
            "Test set predictions:\n",
            "Images predicted as novel superclass: 1319 (11.80%)\n",
            "Images predicted as novel subclass: 5709 (51.06%)\n",
            "Novelty score distribution:\n",
            "  0.0-0.2: 9030 (80.77%)\n",
            "  0.2-0.4: 0 (0.00%)\n",
            "  0.4-0.5: 831 (7.43%)\n",
            "  0.5-0.6: 0 (0.00%)\n",
            "  0.6-0.8: 542 (4.85%)\n",
            "  0.8-1.0: 0 (0.00%)\n",
            "Predictions saved to 'fixed_threshold_predictions.csv'\n",
            "Training and testing complete!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}