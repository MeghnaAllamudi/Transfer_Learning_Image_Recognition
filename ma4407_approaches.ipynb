{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3",
      "metadata": {
        "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c",
      "metadata": {
        "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c"
      },
      "outputs": [],
      "source": [
        "# Create Dataset class for multilabel classification\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.ann_df = ann_df\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.ann_df['image'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        super_idx = self.ann_df['superclass_index'][idx]\n",
        "        super_label = self.super_map_df['class'][super_idx]\n",
        "\n",
        "        sub_idx = self.ann_df['subclass_index'][idx]\n",
        "        sub_label = self.sub_map_df['class'][sub_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, super_idx, super_label, sub_idx, sub_label\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
        "        self.super_map_df = super_map_df\n",
        "        self.sub_map_df = sub_map_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): # Count files in img_dir\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "60tzy4N0CoKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89567b0-b739-4e4e-a861-1229c126e206"
      },
      "id": "60tzy4N0CoKA",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ann_df = pd.read_csv('/content/drive/My Drive/train_data.csv')\n",
        "super_map_df = pd.read_csv('/content/drive/My Drive/superclass_mapping.csv')\n",
        "sub_map_df = pd.read_csv('/content/drive/My Drive/subclass_mapping.csv')"
      ],
      "metadata": {
        "id": "a6LV0RMPE2HJ"
      },
      "id": "a6LV0RMPE2HJ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e7398553-8842-4ad8-b348-767921a22482",
      "metadata": {
        "id": "e7398553-8842-4ad8-b348-767921a22482",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1764f767-f5c0-48bd-8a2d-06cb8d8f7c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching images locally from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6288/6288 [02:44<00:00, 38.32it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached 6288 images to local storage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Test Dataset\n",
        "#test_ann_df = pd.read_csv('/content/drive/My Drive/test_data.csv')\n",
        "\n",
        "train_img_dir = '/content/drive/My Drive/train_images/train_images/'\n",
        "test_img_dir = '/content/drive/My Drive/test_images/test_images/'\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "# Create a local cache directory\n",
        "local_cache_dir = \"/content/local_train_image_cache\"\n",
        "os.makedirs(local_cache_dir, exist_ok=True)\n",
        "\n",
        "# Copy your dataset from Google Drive to local storage once\n",
        "if len(os.listdir(local_cache_dir)) == 0:  # Only copy if cache is empty\n",
        "    print(\"Caching images locally from Google Drive...\")\n",
        "    source_dir = train_img_dir\n",
        "\n",
        "    # Get list of image files\n",
        "    image_files = [f for f in os.listdir(source_dir)\n",
        "                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # Copy files with progress bar\n",
        "    for img in tqdm(image_files):\n",
        "        shutil.copy(os.path.join(source_dir, img),\n",
        "                   os.path.join(local_cache_dir, img))\n",
        "\n",
        "    print(f\"Cached {len(image_files)} images to local storage\")\n",
        "\n",
        "\n",
        "image_preprocessing = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0), std=(1)),\n",
        "])\n",
        "\n",
        "# Update this in your original code where you define image_preprocessing\n",
        "# image_preprocessing = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),  # Resize to 224x224 for ViT\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
        "# ])\n",
        "\n",
        "# Create train and val split\n",
        "full_dataset = MultiClassImageDataset(train_ann_df, super_map_df, sub_map_df, local_cache_dir, transform=image_preprocessing)\n",
        "train_dataset, val_dataset = random_split(full_dataset, [0.9, 0.1])\n",
        "\n",
        "#Create test dataset\n",
        "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=1,\n",
        "                         shuffle=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "#BEST MODEL\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size=64, num_superclasses=4, num_subclasses=88):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_size = input_size // (2**3)\n",
        "\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(self.feature_size * self.feature_size * 128, 256)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "\n",
        "        self.fc3a = nn.Linear(128, num_superclasses)\n",
        "        self.fc3b = nn.Linear(128, num_subclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "\n",
        "        return super_out, sub_out\n",
        "\n",
        "    def get_features(self, x):\n",
        "        \"\"\"Extract features before the final classification layer\"\"\"\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class NoveltyDetectionTrainer:\n",
        "    def __init__(self, full_dataset, image_preprocessing, device='cuda', batch_size=64):\n",
        "        self.full_dataset = full_dataset\n",
        "        self.image_preprocessing = image_preprocessing\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "        self.energy_mean = 0\n",
        "        self.energy_std = 1\n",
        "\n",
        "\n",
        "        self.superclass_indices = set()\n",
        "        for i in range(len(full_dataset)):\n",
        "            _, super_idx, _, _, _ = full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "            self.superclass_indices.add(super_idx)\n",
        "\n",
        "        self.superclass_indices = sorted(list(self.superclass_indices))\n",
        "        print(f\"Found superclasses with indices: {self.superclass_indices}\")\n",
        "\n",
        "    def cross_validate_novelty_detection(self, epochs=5, confidence_threshold=0.0):\n",
        "        results = []\n",
        "\n",
        "        for fold, novel_idx in enumerate(self.superclass_indices):\n",
        "            print(f\"\\n=== Fold {fold+1}/{len(self.superclass_indices)}: Treating superclass {novel_idx} as novel ===\")\n",
        "\n",
        "            known_indices, novel_indices = self._split_by_superclass(novel_idx)\n",
        "\n",
        "            np.random.shuffle(known_indices)\n",
        "            train_size = int(0.9 * len(known_indices))\n",
        "            train_indices = known_indices[:train_size]\n",
        "            val_known_indices = known_indices[train_size:]\n",
        "\n",
        "            train_dataset = Subset(self.full_dataset, train_indices)\n",
        "            val_known_dataset = Subset(self.full_dataset, val_known_indices)\n",
        "            val_novel_dataset = Subset(self.full_dataset, novel_indices)\n",
        "\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "            val_known_loader = DataLoader(val_known_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "            val_novel_loader = DataLoader(val_novel_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "            model = CNN(input_size=64, num_superclasses=len(self.superclass_indices)+1).to(self.device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "            self._train_model(model, criterion, optimizer, train_loader, epochs)\n",
        "\n",
        "\n",
        "            self._calibrate_energy_stats(model, train_loader)\n",
        "\n",
        "\n",
        "            metrics = self._evaluate_novelty_detection(model, val_known_loader, val_novel_loader, confidence_threshold)\n",
        "            results.append(metrics)\n",
        "\n",
        "            print(f\"Fold {fold+1} results:\")\n",
        "            for key, value in metrics.items():\n",
        "                print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "\n",
        "        avg_results = {}\n",
        "        for key in results[0].keys():\n",
        "            avg_results[key] = sum(r[key] for r in results) / len(results)\n",
        "\n",
        "\n",
        "        for key, value in avg_results.items():\n",
        "            print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "        return avg_results, results\n",
        "\n",
        "    def find_optimal_threshold(self, fold_index=0, threshold_range=np.arange(-3.0, 3.0, 0.1)):\n",
        "        novel_idx = self.superclass_indices[fold_index]\n",
        "        print(f\"\\n=== Finding optimal threshold for fold {fold_index+1}: Superclass {novel_idx} as novel ===\")\n",
        "\n",
        "\n",
        "        known_indices, novel_indices = self._split_by_superclass(novel_idx)\n",
        "\n",
        "\n",
        "        np.random.shuffle(known_indices)\n",
        "        train_size = int(0.9 * len(known_indices))\n",
        "        train_indices = known_indices[:train_size]\n",
        "        val_known_indices = known_indices[train_size:]\n",
        "\n",
        "\n",
        "        train_dataset = Subset(self.full_dataset, train_indices)\n",
        "        val_known_dataset = Subset(self.full_dataset, val_known_indices)\n",
        "        val_novel_dataset = Subset(self.full_dataset, novel_indices)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_known_loader = DataLoader(val_known_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        val_novel_loader = DataLoader(val_novel_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "        model = CNN(input_size=64, num_superclasses=len(self.superclass_indices)+1).to(self.device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "        self._train_model(model, criterion, optimizer, train_loader, epochs=5)\n",
        "\n",
        "\n",
        "        self._calibrate_energy_stats(model, train_loader)\n",
        "\n",
        "\n",
        "        known_energies, novel_energies = self._collect_energies(model, val_known_loader, val_novel_loader)\n",
        "\n",
        "\n",
        "        results = []\n",
        "        for threshold in threshold_range:\n",
        "\n",
        "            known_correct = sum(1 for e in known_energies if e <= threshold)\n",
        "            known_accuracy = known_correct / len(known_energies) if known_energies else 0\n",
        "\n",
        "\n",
        "            novel_correct = sum(1 for e in novel_energies if e > threshold)\n",
        "            novel_accuracy = novel_correct / len(novel_energies) if novel_energies else 0\n",
        "\n",
        "            balanced_accuracy = (known_accuracy + novel_accuracy) / 2\n",
        "\n",
        "            results.append({\n",
        "                'threshold': threshold,\n",
        "                'known_accuracy': known_accuracy,\n",
        "                'novel_accuracy': novel_accuracy,\n",
        "                'balanced_accuracy': balanced_accuracy\n",
        "            })\n",
        "\n",
        "            print(f\"Threshold {threshold:.2f}: Known Acc={known_accuracy:.4f}, Novel Acc={novel_accuracy:.4f}, Balanced Acc={balanced_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "        best_result = max(results, key=lambda x: x['balanced_accuracy'])\n",
        "\n",
        "        print(f\"\\nBest threshold: {best_result['threshold']:.2f}\")\n",
        "        print(f\"Known accuracy: {best_result['known_accuracy']:.4f}\")\n",
        "        print(f\"Novel accuracy: {best_result['novel_accuracy']:.4f}\")\n",
        "        print(f\"Balanced accuracy: {best_result['balanced_accuracy']:.4f}\")\n",
        "\n",
        "        return best_result['threshold'], results\n",
        "\n",
        "    def _calibrate_energy_stats(self, model, loader):\n",
        "      \"\"\"Calculate energy statistics on a dataset for normalization\"\"\"\n",
        "      model.eval()\n",
        "      all_energies = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for data in loader:\n",
        "              inputs = data[0].to(self.device)\n",
        "\n",
        "\n",
        "              super_outputs, _ = model(inputs)\n",
        "\n",
        "\n",
        "              energies = -torch.logsumexp(super_outputs, dim=1)\n",
        "              all_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "      all_energies = np.array(all_energies)\n",
        "      self.energy_mean = float(np.mean(all_energies))\n",
        "      self.energy_std = float(np.std(all_energies) + 1e-6)  # Add epsilon to avoid division by zero\n",
        "\n",
        "      print(f\"Calibrated energy statistics: mean={self.energy_mean:.4f}, std={self.energy_std:.4f}\")\n",
        "\n",
        "    def _compute_normalized_energy(self, logits):\n",
        "\n",
        "\n",
        "      raw_energy = -torch.logsumexp(logits, dim=1)\n",
        "\n",
        "\n",
        "      normalized_energy = (raw_energy - self.energy_mean) / self.energy_std\n",
        "\n",
        "      return normalized_energy\n",
        "\n",
        "    def _split_by_superclass(self, novel_superclass_idx):\n",
        "        \"\"\"Split dataset indices into known and novel based on superclass\"\"\"\n",
        "        known_indices = []\n",
        "        novel_indices = []\n",
        "\n",
        "        for i in range(len(self.full_dataset)):\n",
        "            _, super_idx, _, _, _ = self.full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "\n",
        "            if super_idx == novel_superclass_idx:\n",
        "                novel_indices.append(i)\n",
        "            else:\n",
        "                known_indices.append(i)\n",
        "\n",
        "        return known_indices, novel_indices\n",
        "\n",
        "    def _train_model(self, model, criterion, optimizer, train_loader, epochs):\n",
        "        \"\"\"Train the model on known classes\"\"\"\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(train_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                super_labels = super_labels.to(self.device)\n",
        "                sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                super_outputs, sub_outputs = model(inputs)\n",
        "                loss = criterion(super_outputs, super_labels) + criterion(sub_outputs, sub_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "    def _evaluate_novelty_detection(self, model, known_loader, novel_loader, threshold):\n",
        "      \"\"\"Evaluate novelty detection performance using balanced ensemble approach.\"\"\"\n",
        "      model.eval()\n",
        "\n",
        "\n",
        "      self._calibrate_energy_stats(model, known_loader)\n",
        "\n",
        "      def eval_loader(loader, is_novel):\n",
        "          super_correct, sub_correct = 0, 0\n",
        "          super_total, sub_total = 0, 0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for data in loader:\n",
        "                  inputs, _, _, _, _ = data\n",
        "                  inputs = inputs.to(self.device)\n",
        "\n",
        "                  super_outputs, sub_outputs = model(inputs)\n",
        "\n",
        "\n",
        "                  super_energies = self._compute_normalized_energy(super_outputs)\n",
        "                  energy_novel = super_energies > threshold\n",
        "\n",
        "\n",
        "                  super_probs = F.softmax(super_outputs, dim=1)\n",
        "                  super_confidences, _ = torch.max(super_probs, dim=1)\n",
        "                  confidence_novel = super_confidences < 0.7\n",
        "\n",
        "\n",
        "                  energy_weight = 0.6\n",
        "                  confidence_weight = 0.4\n",
        "\n",
        "                  novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "\n",
        "                  is_novel_super = novelty_score > 0.5\n",
        "\n",
        "                  sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "                  sub_confidences, _ = torch.max(sub_probs, dim=1)\n",
        "                  is_novel_sub = sub_confidences < 0.5\n",
        "\n",
        "                  if is_novel:\n",
        "                      super_correct += is_novel_super.sum().item()\n",
        "                      sub_correct += is_novel_sub.sum().item()\n",
        "                  else:\n",
        "                      super_correct += (~is_novel_super).sum().item()\n",
        "                      sub_correct += (~is_novel_sub).sum().item()\n",
        "\n",
        "                  super_total += inputs.size(0)\n",
        "                  sub_total += inputs.size(0)\n",
        "\n",
        "          return (\n",
        "              super_correct / super_total if super_total else 0,\n",
        "              sub_correct / sub_total if sub_total else 0\n",
        "          )\n",
        "\n",
        "\n",
        "      known_super_acc, known_sub_acc = eval_loader(known_loader, is_novel=False)\n",
        "      novel_super_acc, novel_sub_acc = eval_loader(novel_loader, is_novel=True)\n",
        "\n",
        "      balanced_super_acc = (known_super_acc + novel_super_acc) / 2\n",
        "      balanced_sub_acc = (known_sub_acc + novel_sub_acc) / 2\n",
        "\n",
        "      return {\n",
        "          'known_superclass_accuracy': known_super_acc,\n",
        "          'novel_superclass_accuracy': novel_super_acc,\n",
        "          'balanced_superclass_accuracy': balanced_super_acc,\n",
        "          'known_subclass_accuracy': known_sub_acc,\n",
        "          'novel_subclass_accuracy': novel_sub_acc,\n",
        "          'balanced_subclass_accuracy': balanced_sub_acc\n",
        "      }\n",
        "\n",
        "\n",
        "    def _collect_energies(self, model, known_loader, novel_loader):\n",
        "        \"\"\"Collect normalized energy scores for known and novel classes\"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        known_energies = []\n",
        "        novel_energies = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for data in known_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                super_outputs, _ = model(inputs)\n",
        "                energies = self._compute_normalized_energy(super_outputs)\n",
        "                known_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "\n",
        "            for data in novel_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                super_outputs, _ = model(inputs)\n",
        "                energies = self._compute_normalized_energy(super_outputs)\n",
        "                novel_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "        return known_energies, novel_energies\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        self.energy_mean = 0\n",
        "        self.energy_std = 1\n",
        "        self.energy_calibrated = False\n",
        "\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "        )\n",
        "\n",
        "\n",
        "        self.temperature = 1.5\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/(i+1):.3f}')\n",
        "        avg_loss = running_loss/(i+1)\n",
        "        self.scheduler.step(avg_loss)\n",
        "        return avg_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self._calibrate_energy_stats()\n",
        "\n",
        "    def _calibrate_energy_stats(self):\n",
        "        \"\"\"Calculate energy statistics on training data for normalization\"\"\"\n",
        "        self.model.eval()\n",
        "        all_energies = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in self.train_loader:\n",
        "                inputs = data[0].to(self.device)\n",
        "\n",
        "\n",
        "                super_outputs, _ = self.model(inputs)\n",
        "\n",
        "\n",
        "                energies = -torch.logsumexp(super_outputs, dim=1)\n",
        "                all_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "\n",
        "        all_energies = np.array(all_energies)\n",
        "        self.energy_mean = float(np.mean(all_energies))\n",
        "        self.energy_std = float(np.std(all_energies) + 1e-6)\n",
        "        self.energy_calibrated = True\n",
        "\n",
        "        print(f\"Calibrated energy statistics: mean={self.energy_mean:.4f}, std={self.energy_std:.4f}\")\n",
        "\n",
        "    def compute_normalized_energy(self, logits):\n",
        "\n",
        "\n",
        "        raw_energy = -torch.logsumexp(logits, dim=1)\n",
        "\n",
        "\n",
        "        if not self.energy_calibrated:\n",
        "\n",
        "            print(\"Warning: Energy statistics not calibrated, using raw energy\")\n",
        "            return raw_energy\n",
        "\n",
        "        normalized_energy = (raw_energy - self.energy_mean) / self.energy_std\n",
        "\n",
        "        return normalized_energy\n",
        "\n",
        "    def validate_epoch(self, novel_superclass_idx=3, novel_subclass_idx=87, confidence_threshold=0.0, temperature=1.0):\n",
        "\n",
        "      # Make sure energy statistics are calibrated\n",
        "      if not self.energy_calibrated:\n",
        "          self._calibrate_energy_stats()\n",
        "\n",
        "      self.model.eval()\n",
        "\n",
        "      # Metrics to track\n",
        "      correct_with_novelty = 0\n",
        "      super_correct_standard = 0\n",
        "      sub_correct = 0\n",
        "\n",
        "      novel_total = 0\n",
        "      known_total = 0\n",
        "      novel_correct = 0\n",
        "      known_correct = 0\n",
        "\n",
        "      total = 0\n",
        "\n",
        "      novel_super_predictions = 0\n",
        "      novel_sub_predictions = 0\n",
        "\n",
        "      all_super_energies = []\n",
        "      all_sub_confidences = []\n",
        "\n",
        "      running_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for i, data in enumerate(self.val_loader):\n",
        "              inputs, super_labels, _, sub_labels, _ = data\n",
        "              inputs = inputs.to(self.device)\n",
        "              super_labels = super_labels.to(self.device)\n",
        "              sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "              super_outputs, sub_outputs = self.model(inputs)\n",
        "\n",
        "\n",
        "              super_energies = self.compute_normalized_energy(super_outputs)\n",
        "              energy_novel = super_energies > confidence_threshold\n",
        "\n",
        "\n",
        "              super_probs = F.softmax(super_outputs, dim=1)\n",
        "              super_confidences, super_predicted = torch.max(super_probs, dim=1)\n",
        "\n",
        "              conf_threshold = 0.7\n",
        "              confidence_novel = super_confidences < conf_threshold\n",
        "\n",
        "              energy_weight = 0.6\n",
        "              confidence_weight = 0.4\n",
        "\n",
        "\n",
        "              novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "\n",
        "\n",
        "              decision_threshold = 0.5\n",
        "              novel_super_mask = novelty_score > decision_threshold\n",
        "\n",
        "\n",
        "              final_super_preds = torch.where(\n",
        "                  novel_super_mask,\n",
        "                  torch.full_like(super_predicted, novel_superclass_idx),\n",
        "                  super_predicted\n",
        "              )\n",
        "\n",
        "\n",
        "              sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "              sub_confidences, sub_predicted = torch.max(sub_probs, dim=1)\n",
        "              sub_threshold = 0.5\n",
        "              novel_sub_mask = sub_confidences < sub_threshold\n",
        "\n",
        "              final_sub_preds = torch.where(\n",
        "                  novel_sub_mask,\n",
        "                  torch.full_like(sub_predicted, novel_subclass_idx),\n",
        "                  sub_predicted\n",
        "              )\n",
        "\n",
        "\n",
        "              total += super_labels.size(0)\n",
        "\n",
        "\n",
        "              correct_with_novelty += (final_super_preds == super_labels).sum().item()\n",
        "              super_correct_standard += (super_predicted == super_labels).sum().item()\n",
        "              sub_correct += (final_sub_preds == sub_labels).sum().item()\n",
        "\n",
        "\n",
        "              is_novel_label = super_labels == novel_superclass_idx\n",
        "              novel_total += is_novel_label.sum().item()\n",
        "              known_total += (~is_novel_label).sum().item()\n",
        "\n",
        "              novel_correct += ((final_super_preds == super_labels) & is_novel_label).sum().item()\n",
        "              known_correct += ((final_super_preds == super_labels) & ~is_novel_label).sum().item()\n",
        "\n",
        "\n",
        "              novel_super_predictions += novel_super_mask.sum().item()\n",
        "              novel_sub_predictions += novel_sub_mask.sum().item()\n",
        "\n",
        "\n",
        "              all_super_energies.extend(super_energies.cpu().numpy())\n",
        "              all_sub_confidences.extend(sub_confidences.cpu().numpy())\n",
        "\n",
        "\n",
        "              loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "              running_loss += loss.item()\n",
        "\n",
        "\n",
        "      super_acc = 100 * correct_with_novelty / total if total > 0 else 0\n",
        "      sub_acc = 100 * sub_correct / total if total > 0 else 0\n",
        "\n",
        "      novel_acc = 100 * novel_correct / novel_total if novel_total > 0 else 0\n",
        "      known_acc = 100 * known_correct / known_total if known_total > 0 else 0\n",
        "      balanced_acc = (novel_acc + known_acc) / 2 if novel_total > 0 and known_total > 0 else 0\n",
        "\n",
        "      avg_super_energy = sum(all_super_energies) / len(all_super_energies) if all_super_energies else 0\n",
        "      avg_sub_conf = sum(all_sub_confidences) / len(all_sub_confidences) if all_sub_confidences else 0\n",
        "\n",
        "      novel_super_perc = 100 * novel_super_predictions / total if total > 0 else 0\n",
        "      novel_sub_perc = 100 * novel_sub_predictions / total if total > 0 else 0\n",
        "\n",
        "      # Display metrics\n",
        "      print(f'Validation loss: {running_loss/(i+1):.3f}')\n",
        "      print(f'Validation superclass acc: {super_acc:.2f}%')\n",
        "      print(f'Validation subclass acc: {sub_acc:.2f}%')\n",
        "      print(f'Novel superclass acc: {novel_acc:.2f}%, Known superclass acc: {known_acc:.2f}%')\n",
        "      print(f'Balanced superclass acc: {balanced_acc:.2f}%')\n",
        "      print(f'Average normalized superclass energy: {avg_super_energy:.4f}')\n",
        "      print(f'Average subclass confidence: {avg_sub_conf:.4f}')\n",
        "      print(f'Samples predicted as novel superclass: {novel_super_predictions} ({novel_super_perc:.2f}%)')\n",
        "      print(f'Samples predicted as novel subclass: {novel_sub_predictions} ({novel_sub_perc:.2f}%)')\n",
        "\n",
        "      return {\n",
        "          'loss': running_loss/(i+1),\n",
        "          'accuracy': super_acc,\n",
        "          'novel_acc': novel_acc,\n",
        "          'known_acc': known_acc,\n",
        "          'balanced_acc': balanced_acc\n",
        "      }\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False, confidence_threshold=0.0):\n",
        "      if not self.test_loader:\n",
        "          raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "      # Make sure energy statistics are calibrated\n",
        "      if not self.energy_calibrated:\n",
        "          self._calibrate_energy_stats()\n",
        "\n",
        "      self.model.eval()\n",
        "      novel_superclass_idx = 3  # Index for novel superclass\n",
        "      novel_subclass_idx = 87   # Index for novel subclass\n",
        "\n",
        "      # Create full data structure for internal use\n",
        "      full_test_predictions = {\n",
        "          'image': [],\n",
        "          'superclass_index': [],\n",
        "          'subclass_index': [],\n",
        "          'superclass_energy': [],\n",
        "          'subclass_confidence': [],\n",
        "          'novelty_score': []\n",
        "      }\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for i, data in enumerate(self.test_loader):\n",
        "              inputs, img_name = data[0].to(self.device), data[1]\n",
        "\n",
        "              super_outputs, sub_outputs = self.model(inputs)\n",
        "\n",
        "\n",
        "              super_energies = self.compute_normalized_energy(super_outputs)\n",
        "              energy_novel = super_energies > confidence_threshold\n",
        "\n",
        "\n",
        "              super_probs = F.softmax(super_outputs, dim=1)\n",
        "              super_confidences, super_predicted = torch.max(super_probs, dim=1)\n",
        "\n",
        "\n",
        "              conf_threshold = 0.7\n",
        "              confidence_novel = super_confidences < conf_threshold\n",
        "\n",
        "\n",
        "              energy_weight = 0.6\n",
        "              confidence_weight = 0.4\n",
        "\n",
        "              novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "              decision_threshold = 0.5\n",
        "              novel_super_mask = novelty_score > decision_threshold\n",
        "\n",
        "\n",
        "              sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "              sub_confidences, sub_predicted = torch.max(sub_probs, dim=1)\n",
        "              sub_threshold = 0.5\n",
        "              novel_sub_mask = sub_confidences < sub_threshold\n",
        "\n",
        "              for j in range(inputs.size(0)):\n",
        "                  img = img_name[j] if isinstance(img_name, list) else img_name[0]\n",
        "\n",
        "\n",
        "                  super_pred = novel_superclass_idx if novel_super_mask[j] else super_predicted[j].item()\n",
        "                  sub_pred = novel_subclass_idx if novel_sub_mask[j] else sub_predicted[j].item()\n",
        "\n",
        "                  full_test_predictions['image'].append(img)\n",
        "                  full_test_predictions['superclass_index'].append(super_pred)\n",
        "                  full_test_predictions['subclass_index'].append(sub_pred)\n",
        "                  full_test_predictions['superclass_energy'].append(super_energies[j].item())\n",
        "                  full_test_predictions['subclass_confidence'].append(sub_confidences[j].item())\n",
        "                  full_test_predictions['novelty_score'].append(novelty_score[j].item())\n",
        "\n",
        "      full_predictions_df = pd.DataFrame(data=full_test_predictions)\n",
        "\n",
        "\n",
        "      simplified_test_predictions = {\n",
        "          'image': full_test_predictions['image'],\n",
        "          'superclass_index': full_test_predictions['superclass_index'],\n",
        "          'subclass_index': full_test_predictions['subclass_index']\n",
        "      }\n",
        "      simplified_predictions_df = pd.DataFrame(data=simplified_test_predictions)\n",
        "\n",
        "\n",
        "      novel_super_count = sum(1 for idx in full_test_predictions['superclass_index'] if idx == novel_superclass_idx)\n",
        "      novel_sub_count = sum(1 for idx in full_test_predictions['subclass_index'] if idx == novel_subclass_idx)\n",
        "\n",
        "      total_count = len(full_test_predictions['image'])\n",
        "      novel_super_perc = 100 * novel_super_count / total_count if total_count > 0 else 0\n",
        "      novel_sub_perc = 100 * novel_sub_count / total_count if total_count > 0 else 0\n",
        "\n",
        "      print(f'Test set predictions:')\n",
        "      print(f'Images predicted as novel superclass: {novel_super_count} ({novel_super_perc:.2f}%)')\n",
        "      print(f'Images predicted as novel subclass: {novel_sub_count} ({novel_sub_perc:.2f}%)')\n",
        "\n",
        "\n",
        "      print(f'Novelty score distribution:')\n",
        "      bins = [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
        "      for i in range(len(bins)-1):\n",
        "          count = sum(1 for score in full_test_predictions['novelty_score']\n",
        "                    if bins[i] <= score < bins[i+1])\n",
        "          print(f'  {bins[i]:.1f}-{bins[i+1]:.1f}: {count} ({100*count/total_count:.2f}%)')\n",
        "\n",
        "      if save_to_csv:\n",
        "\n",
        "          simplified_predictions_df.to_csv('example_test_predictions.csv', index=False)\n",
        "          print(\"Predictions saved to 'example_test_predictions.csv'\")\n",
        "\n",
        "      if return_predictions:\n",
        "\n",
        "          return full_predictions_df\n",
        "\n",
        "\n",
        "def train_with_novelty_detection(full_dataset, image_preprocessing, device='cuda', batch_size=64, epochs=5):\n",
        "\n",
        "    novelty_trainer = NoveltyDetectionTrainer(\n",
        "        full_dataset=full_dataset,\n",
        "        image_preprocessing=image_preprocessing,\n",
        "        device=device,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"Running cross-validation for novelty detection...\")\n",
        "    avg_results, fold_results = novelty_trainer.cross_validate_novelty_detection(epochs=epochs)\n",
        "\n",
        "\n",
        "    print(\"\\nFinding optimal energy threshold...\")\n",
        "    best_threshold, threshold_results = novelty_trainer.find_optimal_threshold()\n",
        "\n",
        "    return avg_results, best_threshold\n"
      ],
      "metadata": {
        "id": "20bsRDmhGSLM"
      },
      "id": "20bsRDmhGSLM",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. First run cross-validation to find the optimal threshold\n",
        "results, threshold = train_with_novelty_detection(full_dataset, image_preprocessing)\n",
        "device = 'cuda'\n",
        "\n",
        "# 2. Then train your final model on all data and use the threshold for inference\n",
        "model = CNN(input_size=64, num_superclasses=3, num_subclasses=87).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader, device=device)\n",
        "\n",
        "\n",
        "trainer.energy_mean = 0\n",
        "trainer.energy_std = 1\n",
        "\n",
        "for epoch in range(20):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    trainer.train_epoch()\n",
        "    trainer.validate_epoch(confidence_threshold=threshold)  # Use optimized threshold\n",
        "    print('')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n",
        "predictions = trainer.test(save_to_csv=True, confidence_threshold=threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmR10iTb3RjY",
        "outputId": "3669def1-cf56-44b0-c58d-ba4f66ee8a72"
      },
      "id": "cmR10iTb3RjY",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found superclasses with indices: [0, 1, 2]\n",
            "Running cross-validation for novelty detection...\n",
            "\n",
            "=== Fold 1/3: Treating superclass 0 as novel ===\n",
            "Epoch 1/5, Loss: 3.5685\n",
            "Epoch 2/5, Loss: 2.1151\n",
            "Epoch 3/5, Loss: 1.4693\n",
            "Epoch 4/5, Loss: 1.1098\n",
            "Epoch 5/5, Loss: 0.8552\n",
            "Calibrated energy statistics: mean=-7.2986, std=2.6351\n",
            "Calibrated energy statistics: mean=-7.1025, std=2.7703\n",
            "Fold 1 results:\n",
            "  known_superclass_accuracy: 0.4595\n",
            "  novel_superclass_accuracy: 0.9330\n",
            "  balanced_superclass_accuracy: 0.6962\n",
            "  known_subclass_accuracy: 0.7432\n",
            "  novel_subclass_accuracy: 0.6173\n",
            "  balanced_subclass_accuracy: 0.6803\n",
            "\n",
            "=== Fold 2/3: Treating superclass 1 as novel ===\n",
            "Epoch 1/5, Loss: 3.7484\n",
            "Epoch 2/5, Loss: 2.2629\n",
            "Epoch 3/5, Loss: 1.5926\n",
            "Epoch 4/5, Loss: 1.2059\n",
            "Epoch 5/5, Loss: 1.0068\n",
            "Calibrated energy statistics: mean=-7.6700, std=2.9918\n",
            "Calibrated energy statistics: mean=-7.4149, std=2.9566\n",
            "Fold 2 results:\n",
            "  known_superclass_accuracy: 0.4537\n",
            "  novel_superclass_accuracy: 0.9621\n",
            "  balanced_superclass_accuracy: 0.7079\n",
            "  known_subclass_accuracy: 0.7696\n",
            "  novel_subclass_accuracy: 0.5005\n",
            "  balanced_subclass_accuracy: 0.6350\n",
            "\n",
            "=== Fold 3/3: Treating superclass 2 as novel ===\n",
            "Epoch 1/5, Loss: 3.5379\n",
            "Epoch 2/5, Loss: 1.9899\n",
            "Epoch 3/5, Loss: 1.3234\n",
            "Epoch 4/5, Loss: 0.9992\n",
            "Epoch 5/5, Loss: 0.7455\n",
            "Calibrated energy statistics: mean=-7.6432, std=2.2067\n",
            "Calibrated energy statistics: mean=-7.4178, std=2.2847\n",
            "Fold 3 results:\n",
            "  known_superclass_accuracy: 0.4924\n",
            "  novel_superclass_accuracy: 0.7145\n",
            "  balanced_superclass_accuracy: 0.6035\n",
            "  known_subclass_accuracy: 0.8046\n",
            "  novel_subclass_accuracy: 0.4728\n",
            "  balanced_subclass_accuracy: 0.6387\n",
            "\n",
            "=== Cross-Validation Summary ===\n",
            "known_superclass_accuracy: 0.4685\n",
            "novel_superclass_accuracy: 0.8699\n",
            "balanced_superclass_accuracy: 0.6692\n",
            "known_subclass_accuracy: 0.7725\n",
            "novel_subclass_accuracy: 0.5302\n",
            "balanced_subclass_accuracy: 0.6513\n",
            "\n",
            "Finding optimal energy threshold...\n",
            "\n",
            "=== Finding optimal threshold for fold 1: Superclass 0 as novel ===\n",
            "Epoch 1/5, Loss: 3.4100\n",
            "Epoch 2/5, Loss: 1.8834\n",
            "Epoch 3/5, Loss: 1.3717\n",
            "Epoch 4/5, Loss: 1.0254\n",
            "Epoch 5/5, Loss: 0.8080\n",
            "Calibrated energy statistics: mean=-8.1469, std=2.2365\n",
            "Threshold -3.00: Known Acc=0.0023, Novel Acc=0.9995, Balanced Acc=0.5009\n",
            "Threshold -2.90: Known Acc=0.0023, Novel Acc=0.9995, Balanced Acc=0.5009\n",
            "Threshold -2.80: Known Acc=0.0023, Novel Acc=0.9995, Balanced Acc=0.5009\n",
            "Threshold -2.70: Known Acc=0.0045, Novel Acc=0.9995, Balanced Acc=0.5020\n",
            "Threshold -2.60: Known Acc=0.0045, Novel Acc=0.9989, Balanced Acc=0.5017\n",
            "Threshold -2.50: Known Acc=0.0045, Novel Acc=0.9978, Balanced Acc=0.5012\n",
            "Threshold -2.40: Known Acc=0.0045, Novel Acc=0.9973, Balanced Acc=0.5009\n",
            "Threshold -2.30: Known Acc=0.0090, Novel Acc=0.9968, Balanced Acc=0.5029\n",
            "Threshold -2.20: Known Acc=0.0158, Novel Acc=0.9968, Balanced Acc=0.5063\n",
            "Threshold -2.10: Known Acc=0.0270, Novel Acc=0.9962, Balanced Acc=0.5116\n",
            "Threshold -2.00: Known Acc=0.0315, Novel Acc=0.9957, Balanced Acc=0.5136\n",
            "Threshold -1.90: Known Acc=0.0405, Novel Acc=0.9951, Balanced Acc=0.5178\n",
            "Threshold -1.80: Known Acc=0.0450, Novel Acc=0.9930, Balanced Acc=0.5190\n",
            "Threshold -1.70: Known Acc=0.0495, Novel Acc=0.9930, Balanced Acc=0.5213\n",
            "Threshold -1.60: Known Acc=0.0563, Novel Acc=0.9919, Balanced Acc=0.5241\n",
            "Threshold -1.50: Known Acc=0.0811, Novel Acc=0.9881, Balanced Acc=0.5346\n",
            "Threshold -1.40: Known Acc=0.1036, Novel Acc=0.9870, Balanced Acc=0.5453\n",
            "Threshold -1.30: Known Acc=0.1171, Novel Acc=0.9843, Balanced Acc=0.5507\n",
            "Threshold -1.20: Known Acc=0.1351, Novel Acc=0.9784, Balanced Acc=0.5568\n",
            "Threshold -1.10: Known Acc=0.1577, Novel Acc=0.9773, Balanced Acc=0.5675\n",
            "Threshold -1.00: Known Acc=0.1757, Novel Acc=0.9735, Balanced Acc=0.5746\n",
            "Threshold -0.90: Known Acc=0.2005, Novel Acc=0.9708, Balanced Acc=0.5856\n",
            "Threshold -0.80: Known Acc=0.2095, Novel Acc=0.9665, Balanced Acc=0.5880\n",
            "Threshold -0.70: Known Acc=0.2500, Novel Acc=0.9638, Balanced Acc=0.6069\n",
            "Threshold -0.60: Known Acc=0.2793, Novel Acc=0.9573, Balanced Acc=0.6183\n",
            "Threshold -0.50: Known Acc=0.3131, Novel Acc=0.9481, Balanced Acc=0.6306\n",
            "Threshold -0.40: Known Acc=0.3446, Novel Acc=0.9405, Balanced Acc=0.6426\n",
            "Threshold -0.30: Known Acc=0.3739, Novel Acc=0.9330, Balanced Acc=0.6534\n",
            "Threshold -0.20: Known Acc=0.4189, Novel Acc=0.9238, Balanced Acc=0.6714\n",
            "Threshold -0.10: Known Acc=0.4527, Novel Acc=0.9124, Balanced Acc=0.6826\n",
            "Threshold 0.00: Known Acc=0.4820, Novel Acc=0.8978, Balanced Acc=0.6899\n",
            "Threshold 0.10: Known Acc=0.5248, Novel Acc=0.8784, Balanced Acc=0.7016\n",
            "Threshold 0.20: Known Acc=0.5676, Novel Acc=0.8643, Balanced Acc=0.7159\n",
            "Threshold 0.30: Known Acc=0.6104, Novel Acc=0.8459, Balanced Acc=0.7282\n",
            "Threshold 0.40: Known Acc=0.6486, Novel Acc=0.8265, Balanced Acc=0.7376\n",
            "Threshold 0.50: Known Acc=0.6892, Novel Acc=0.8086, Balanced Acc=0.7489\n",
            "Threshold 0.60: Known Acc=0.7185, Novel Acc=0.7827, Balanced Acc=0.7506\n",
            "Threshold 0.70: Known Acc=0.7432, Novel Acc=0.7541, Balanced Acc=0.7486\n",
            "Threshold 0.80: Known Acc=0.7725, Novel Acc=0.7270, Balanced Acc=0.7498\n",
            "Threshold 0.90: Known Acc=0.7973, Novel Acc=0.6978, Balanced Acc=0.7476\n",
            "Threshold 1.00: Known Acc=0.8153, Novel Acc=0.6638, Balanced Acc=0.7395\n",
            "Threshold 1.10: Known Acc=0.8356, Novel Acc=0.6324, Balanced Acc=0.7340\n",
            "Threshold 1.20: Known Acc=0.8604, Novel Acc=0.6070, Balanced Acc=0.7337\n",
            "Threshold 1.30: Known Acc=0.8829, Novel Acc=0.5649, Balanced Acc=0.7239\n",
            "Threshold 1.40: Known Acc=0.9009, Novel Acc=0.5362, Balanced Acc=0.7186\n",
            "Threshold 1.50: Known Acc=0.9189, Novel Acc=0.4968, Balanced Acc=0.7078\n",
            "Threshold 1.60: Known Acc=0.9324, Novel Acc=0.4573, Balanced Acc=0.6949\n",
            "Threshold 1.70: Known Acc=0.9437, Novel Acc=0.4227, Balanced Acc=0.6832\n",
            "Threshold 1.80: Known Acc=0.9505, Novel Acc=0.3822, Balanced Acc=0.6663\n",
            "Threshold 1.90: Known Acc=0.9572, Novel Acc=0.3438, Balanced Acc=0.6505\n",
            "Threshold 2.00: Known Acc=0.9617, Novel Acc=0.3038, Balanced Acc=0.6327\n",
            "Threshold 2.10: Known Acc=0.9640, Novel Acc=0.2654, Balanced Acc=0.6147\n",
            "Threshold 2.20: Known Acc=0.9662, Novel Acc=0.2324, Balanced Acc=0.5993\n",
            "Threshold 2.30: Known Acc=0.9820, Novel Acc=0.1962, Balanced Acc=0.5891\n",
            "Threshold 2.40: Known Acc=0.9820, Novel Acc=0.1611, Balanced Acc=0.5715\n",
            "Threshold 2.50: Known Acc=0.9910, Novel Acc=0.1254, Balanced Acc=0.5582\n",
            "Threshold 2.60: Known Acc=0.9955, Novel Acc=0.0914, Balanced Acc=0.5434\n",
            "Threshold 2.70: Known Acc=0.9977, Novel Acc=0.0595, Balanced Acc=0.5286\n",
            "Threshold 2.80: Known Acc=0.9977, Novel Acc=0.0351, Balanced Acc=0.5164\n",
            "Threshold 2.90: Known Acc=0.9977, Novel Acc=0.0195, Balanced Acc=0.5086\n",
            "\n",
            "Best threshold: 0.60\n",
            "Known accuracy: 0.7185\n",
            "Novel accuracy: 0.7827\n",
            "Balanced accuracy: 0.7506\n",
            "Epoch 1\n",
            "Training loss: 3.662\n",
            "Calibrated energy statistics: mean=-5.0952, std=2.5940\n",
            "Validation loss: 2.930\n",
            "Validation superclass acc: 66.56%\n",
            "Validation subclass acc: 8.28%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 66.56%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -0.0023\n",
            "Average subclass confidence: 0.2987\n",
            "Samples predicted as novel superclass: 204 (32.48%)\n",
            "Samples predicted as novel subclass: 526 (83.76%)\n",
            "\n",
            "Epoch 2\n",
            "Training loss: 2.220\n",
            "Validation loss: 1.777\n",
            "Validation superclass acc: 78.82%\n",
            "Validation subclass acc: 31.21%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 78.82%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -0.2712\n",
            "Average subclass confidence: 0.5104\n",
            "Samples predicted as novel superclass: 128 (20.38%)\n",
            "Samples predicted as novel subclass: 349 (55.57%)\n",
            "\n",
            "Epoch 3\n",
            "Training loss: 1.527\n",
            "Validation loss: 1.331\n",
            "Validation superclass acc: 83.28%\n",
            "Validation subclass acc: 45.70%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 83.28%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -0.5277\n",
            "Average subclass confidence: 0.6137\n",
            "Samples predicted as novel superclass: 102 (16.24%)\n",
            "Samples predicted as novel subclass: 238 (37.90%)\n",
            "\n",
            "Epoch 4\n",
            "Training loss: 1.179\n",
            "Validation loss: 1.129\n",
            "Validation superclass acc: 88.85%\n",
            "Validation subclass acc: 55.25%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 88.85%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -0.8929\n",
            "Average subclass confidence: 0.6776\n",
            "Samples predicted as novel superclass: 67 (10.67%)\n",
            "Samples predicted as novel subclass: 170 (27.07%)\n",
            "\n",
            "Epoch 5\n",
            "Training loss: 0.911\n",
            "Validation loss: 1.064\n",
            "Validation superclass acc: 89.65%\n",
            "Validation subclass acc: 60.51%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 89.65%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -0.8798\n",
            "Average subclass confidence: 0.7237\n",
            "Samples predicted as novel superclass: 65 (10.35%)\n",
            "Samples predicted as novel subclass: 129 (20.54%)\n",
            "\n",
            "Epoch 6\n",
            "Training loss: 0.740\n",
            "Validation loss: 1.113\n",
            "Validation superclass acc: 92.99%\n",
            "Validation subclass acc: 64.65%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.99%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.3068\n",
            "Average subclass confidence: 0.7690\n",
            "Samples predicted as novel superclass: 40 (6.37%)\n",
            "Samples predicted as novel subclass: 105 (16.72%)\n",
            "\n",
            "Epoch 7\n",
            "Training loss: 0.602\n",
            "Validation loss: 1.045\n",
            "Validation superclass acc: 90.45%\n",
            "Validation subclass acc: 62.26%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 90.45%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.1015\n",
            "Average subclass confidence: 0.7622\n",
            "Samples predicted as novel superclass: 59 (9.39%)\n",
            "Samples predicted as novel subclass: 113 (17.99%)\n",
            "\n",
            "Epoch 8\n",
            "Training loss: 0.548\n",
            "Validation loss: 1.046\n",
            "Validation superclass acc: 92.68%\n",
            "Validation subclass acc: 68.31%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.68%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.6235\n",
            "Average subclass confidence: 0.8054\n",
            "Samples predicted as novel superclass: 38 (6.05%)\n",
            "Samples predicted as novel subclass: 77 (12.26%)\n",
            "\n",
            "Epoch 9\n",
            "Training loss: 0.491\n",
            "Validation loss: 1.051\n",
            "Validation superclass acc: 92.20%\n",
            "Validation subclass acc: 69.27%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.20%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.6240\n",
            "Average subclass confidence: 0.8292\n",
            "Samples predicted as novel superclass: 45 (7.17%)\n",
            "Samples predicted as novel subclass: 53 (8.44%)\n",
            "\n",
            "Epoch 10\n",
            "Training loss: 0.404\n",
            "Validation loss: 1.055\n",
            "Validation superclass acc: 92.99%\n",
            "Validation subclass acc: 66.24%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.99%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.5472\n",
            "Average subclass confidence: 0.8098\n",
            "Samples predicted as novel superclass: 39 (6.21%)\n",
            "Samples predicted as novel subclass: 82 (13.06%)\n",
            "\n",
            "Epoch 11\n",
            "Training loss: 0.360\n",
            "Validation loss: 1.035\n",
            "Validation superclass acc: 91.56%\n",
            "Validation subclass acc: 72.13%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 91.56%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.7068\n",
            "Average subclass confidence: 0.8561\n",
            "Samples predicted as novel superclass: 50 (7.96%)\n",
            "Samples predicted as novel subclass: 49 (7.80%)\n",
            "\n",
            "Epoch 12\n",
            "Training loss: 0.310\n",
            "Validation loss: 0.878\n",
            "Validation superclass acc: 94.27%\n",
            "Validation subclass acc: 72.61%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 94.27%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -1.7833\n",
            "Average subclass confidence: 0.8380\n",
            "Samples predicted as novel superclass: 29 (4.62%)\n",
            "Samples predicted as novel subclass: 58 (9.24%)\n",
            "\n",
            "Epoch 13\n",
            "Training loss: 0.286\n",
            "Validation loss: 1.089\n",
            "Validation superclass acc: 94.90%\n",
            "Validation subclass acc: 73.25%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 94.90%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.3983\n",
            "Average subclass confidence: 0.8679\n",
            "Samples predicted as novel superclass: 25 (3.98%)\n",
            "Samples predicted as novel subclass: 45 (7.17%)\n",
            "\n",
            "Epoch 14\n",
            "Training loss: 0.239\n",
            "Validation loss: 1.295\n",
            "Validation superclass acc: 94.27%\n",
            "Validation subclass acc: 70.38%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 94.27%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.3237\n",
            "Average subclass confidence: 0.8695\n",
            "Samples predicted as novel superclass: 28 (4.46%)\n",
            "Samples predicted as novel subclass: 44 (7.01%)\n",
            "\n",
            "Epoch 15\n",
            "Training loss: 0.238\n",
            "Validation loss: 0.984\n",
            "Validation superclass acc: 96.02%\n",
            "Validation subclass acc: 74.36%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.02%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.5640\n",
            "Average subclass confidence: 0.8717\n",
            "Samples predicted as novel superclass: 18 (2.87%)\n",
            "Samples predicted as novel subclass: 50 (7.96%)\n",
            "\n",
            "Epoch 16\n",
            "Training loss: 0.235\n",
            "Validation loss: 1.151\n",
            "Validation superclass acc: 94.43%\n",
            "Validation subclass acc: 72.13%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 94.43%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.7083\n",
            "Average subclass confidence: 0.8695\n",
            "Samples predicted as novel superclass: 28 (4.46%)\n",
            "Samples predicted as novel subclass: 40 (6.37%)\n",
            "\n",
            "Epoch 17\n",
            "Training loss: 0.226\n",
            "Validation loss: 1.119\n",
            "Validation superclass acc: 93.79%\n",
            "Validation subclass acc: 71.50%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 93.79%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.6569\n",
            "Average subclass confidence: 0.8605\n",
            "Samples predicted as novel superclass: 31 (4.94%)\n",
            "Samples predicted as novel subclass: 57 (9.08%)\n",
            "\n",
            "Epoch 18\n",
            "Training loss: 0.202\n",
            "Validation loss: 1.130\n",
            "Validation superclass acc: 96.97%\n",
            "Validation subclass acc: 74.68%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.97%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -3.3178\n",
            "Average subclass confidence: 0.9015\n",
            "Samples predicted as novel superclass: 14 (2.23%)\n",
            "Samples predicted as novel subclass: 28 (4.46%)\n",
            "\n",
            "Epoch 19\n",
            "Training loss: 0.198\n",
            "Validation loss: 0.966\n",
            "Validation superclass acc: 96.34%\n",
            "Validation subclass acc: 73.89%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 96.34%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -2.8060\n",
            "Average subclass confidence: 0.8913\n",
            "Samples predicted as novel superclass: 19 (3.03%)\n",
            "Samples predicted as novel subclass: 33 (5.25%)\n",
            "\n",
            "Epoch 20\n",
            "Training loss: 0.184\n",
            "Validation loss: 1.186\n",
            "Validation superclass acc: 94.90%\n",
            "Validation subclass acc: 74.52%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 94.90%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: -3.0130\n",
            "Average subclass confidence: 0.8961\n",
            "Samples predicted as novel superclass: 25 (3.98%)\n",
            "Samples predicted as novel subclass: 38 (6.05%)\n",
            "\n",
            "Finished Training\n",
            "Test set predictions:\n",
            "Images predicted as novel superclass: 1194 (10.68%)\n",
            "Images predicted as novel subclass: 1323 (11.83%)\n",
            "Novelty score distribution:\n",
            "  0.0-0.2: 9905 (88.60%)\n",
            "  0.2-0.4: 0 (0.00%)\n",
            "  0.4-0.5: 81 (0.72%)\n",
            "  0.5-0.6: 0 (0.00%)\n",
            "  0.6-0.8: 962 (8.60%)\n",
            "  0.8-1.0: 0 (0.00%)\n",
            "Predictions saved to 'example_test_predictions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import weibull_min\n",
        "from sklearn.preprocessing import normalize\n",
        "from collections import defaultdict\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size=64, num_superclasses=4, num_subclasses=88):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_size = input_size // (2**3)\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(self.feature_size * self.feature_size * 128, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3a = nn.Linear(128, num_superclasses)  # 4 superclasses: bird, dog, reptile, novel\n",
        "        self.fc3b = nn.Linear(128, num_subclasses)    # All subclasses + novel\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "        return super_out, sub_out\n",
        "\n",
        "    def get_features(self, x):\n",
        "        \"\"\"Extract features before the final classification layer\"\"\"\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def get_logits(self, features):\n",
        "        \"\"\"Get class logits from features\"\"\"\n",
        "        super_out = self.fc3a(features)\n",
        "        sub_out = self.fc3b(features)\n",
        "        return super_out, sub_out\n",
        "\n",
        "\n",
        "class OpenMaxModel:\n",
        "    def __init__(self, model, num_superclasses=3, num_subclasses=87, tailsize=20, alpha=10):\n",
        "        \"\"\"\n",
        "        Initialize OpenMax model with a pre-trained CNN\n",
        "\n",
        "        Args:\n",
        "            model: Pre-trained CNN model\n",
        "            num_superclasses: Number of known superclasses\n",
        "            num_subclasses: Number of known subclasses\n",
        "            tailsize: Number of extremal samples to use for Weibull fitting\n",
        "            alpha: Number of top activations to consider for recalibration\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.num_superclasses = num_superclasses\n",
        "        self.num_subclasses = num_subclasses\n",
        "        self.tailsize = tailsize\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Storage for class means and Weibull models\n",
        "        self.super_means = None\n",
        "        self.sub_means = None\n",
        "        self.super_weibull_models = None\n",
        "        self.sub_weibull_models = None\n",
        "\n",
        "    def fit(self, train_loader, device):\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Collect activations for each class\n",
        "        super_activations = defaultdict(list)\n",
        "        sub_activations = defaultdict(list)\n",
        "\n",
        "        # Collect activations for each class\n",
        "        with torch.no_grad():\n",
        "            for data in train_loader:\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                # Get features\n",
        "                features = self.model.get_features(inputs)\n",
        "                super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "                # Store activations for each class\n",
        "                for i in range(inputs.size(0)):\n",
        "                    super_class = super_labels[i].item()\n",
        "                    sub_class = sub_labels[i].item()\n",
        "\n",
        "                    super_activations[super_class].append(super_logits[i].cpu().numpy())\n",
        "                    sub_activations[sub_class].append(sub_logits[i].cpu().numpy())\n",
        "\n",
        "        # Compute means for each class\n",
        "        self.super_means = {}\n",
        "        self.sub_means = {}\n",
        "\n",
        "        for c in range(self.num_superclasses):\n",
        "            if c in super_activations and len(super_activations[c]) > 0:\n",
        "                self.super_means[c] = np.mean(super_activations[c], axis=0)\n",
        "\n",
        "        for c in range(self.num_subclasses):\n",
        "            if c in sub_activations and len(sub_activations[c]) > 0:\n",
        "                self.sub_means[c] = np.mean(sub_activations[c], axis=0)\n",
        "\n",
        "        # Compute distances to mean for Weibull fitting\n",
        "        super_dists = defaultdict(list)\n",
        "        sub_dists = defaultdict(list)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in train_loader:\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                # Get features\n",
        "                features = self.model.get_features(inputs)\n",
        "                super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "                # Compute distances\n",
        "                for i in range(inputs.size(0)):\n",
        "                    super_class = super_labels[i].item()\n",
        "                    sub_class = sub_labels[i].item()\n",
        "\n",
        "                    if super_class in self.super_means:\n",
        "                        super_mean = self.super_means[super_class]\n",
        "                        super_logit = super_logits[i].cpu().numpy()\n",
        "                        super_dist = np.linalg.norm(super_logit - super_mean)\n",
        "                        super_dists[super_class].append(super_dist)\n",
        "\n",
        "                    if sub_class in self.sub_means:\n",
        "                        sub_mean = self.sub_means[sub_class]\n",
        "                        sub_logit = sub_logits[i].cpu().numpy()\n",
        "                        sub_dist = np.linalg.norm(sub_logit - sub_mean)\n",
        "                        sub_dists[sub_class].append(sub_dist)\n",
        "\n",
        "        # Fit Weibull models\n",
        "        self.super_weibull_models = {}\n",
        "        self.sub_weibull_models = {}\n",
        "\n",
        "        for c in range(self.num_superclasses):\n",
        "            if c in super_dists and len(super_dists[c]) > self.tailsize:\n",
        "                # Sort distances and take tailsize largest\n",
        "                sorted_dists = sorted(super_dists[c])\n",
        "                tail_dists = sorted_dists[-self.tailsize:]\n",
        "\n",
        "                # Fit Weibull distribution\n",
        "                try:\n",
        "                    shape, loc, scale = weibull_min.fit(tail_dists, floc=0)\n",
        "                    self.super_weibull_models[c] = (shape, loc, scale)\n",
        "                except:\n",
        "                    print(f\"Warning: Failed to fit Weibull for superclass {c}\")\n",
        "\n",
        "        for c in range(self.num_subclasses):\n",
        "            if c in sub_dists and len(sub_dists[c]) > self.tailsize:\n",
        "                # Sort distances and take tailsize largest\n",
        "                sorted_dists = sorted(sub_dists[c])\n",
        "                tail_dists = sorted_dists[-self.tailsize:]\n",
        "\n",
        "                # Fit Weibull distribution\n",
        "                try:\n",
        "                    shape, loc, scale = weibull_min.fit(tail_dists, floc=0)\n",
        "                    self.sub_weibull_models[c] = (shape, loc, scale)\n",
        "                except:\n",
        "                    print(f\"Warning: Failed to fit Weibull for subclass {c}\")\n",
        "\n",
        "    def predict(self, inputs, device):\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get features and logits\n",
        "            features = self.model.get_features(inputs)\n",
        "            super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "            # Convert to numpy for processing\n",
        "            super_logits_np = super_logits.cpu().numpy()\n",
        "            sub_logits_np = sub_logits.cpu().numpy()\n",
        "\n",
        "            # Process each sample\n",
        "            super_preds = []\n",
        "            sub_preds = []\n",
        "\n",
        "            for i in range(inputs.size(0)):\n",
        "                # Recalibrate superclass logits\n",
        "                super_logit = super_logits_np[i]\n",
        "                super_pred = self._recalibrate_sample(super_logit, self.super_means, self.super_weibull_models, self.num_superclasses)\n",
        "                super_preds.append(super_pred)\n",
        "\n",
        "                # Recalibrate subclass logits\n",
        "                sub_logit = sub_logits_np[i]\n",
        "                sub_pred = self._recalibrate_sample(sub_logit, self.sub_means, self.sub_weibull_models, self.num_subclasses)\n",
        "                sub_preds.append(sub_pred)\n",
        "\n",
        "            # Convert back to tensors\n",
        "            super_preds = torch.tensor(super_preds, device=device)\n",
        "            sub_preds = torch.tensor(sub_preds, device=device)\n",
        "\n",
        "        return super_preds, sub_preds\n",
        "\n",
        "    def _recalibrate_sample(self, logits, means, weibull_models, num_classes):\n",
        "\n",
        "        # Get top alpha class indices\n",
        "        top_alpha_idx = np.argsort(logits)[-self.alpha:]\n",
        "\n",
        "        # Compute distances to class means\n",
        "        distances = {}\n",
        "        for c in range(num_classes):\n",
        "            if c in means:\n",
        "                distances[c] = np.linalg.norm(logits - means[c])\n",
        "\n",
        "        # Recalibrate activations\n",
        "        recalibrated = np.copy(logits)\n",
        "        for c in top_alpha_idx:\n",
        "            if c < num_classes and c in weibull_models:\n",
        "                # Get Weibull parameters\n",
        "                shape, loc, scale = weibull_models[c]\n",
        "\n",
        "                # Compute probability of being an outlier\n",
        "                dist = distances.get(c, 0)\n",
        "                weibull_score = 1 - weibull_min.cdf(dist, shape, loc, scale)\n",
        "\n",
        "                # Adjust activation\n",
        "                recalibrated[c] = logits[c] * (1 - weibull_score)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        recalibrated_probs = np.exp(recalibrated) / np.sum(np.exp(recalibrated))\n",
        "\n",
        "        # Compute probability of being unknown\n",
        "        unknown_prob = 1.0 - np.sum(recalibrated_probs)\n",
        "\n",
        "        # Make final prediction (including novel class)\n",
        "        if unknown_prob > 0.5:  # Threshold for novel class detection\n",
        "            return num_classes  # Return novel class index\n",
        "        else:\n",
        "            return np.argmax(recalibrated_probs)\n",
        "\n",
        "\n",
        "class OpenMaxTrainer:\n",
        "    def __init__(self, model, openmax_model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n",
        "        self.model = model\n",
        "        self.openmax_model = openmax_model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, _, sub_labels, _ = data\n",
        "            inputs = inputs.to(self.device)\n",
        "            super_labels = super_labels.to(self.device)\n",
        "            sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/(i+1):.3f}')\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                super_labels = super_labels.to(self.device)\n",
        "                sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "                # Standard forward pass for loss calculation\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "                loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, sub_preds = self.openmax_model.predict(inputs, self.device)\n",
        "\n",
        "                total += super_labels.size(0)\n",
        "                super_correct += (super_preds == super_labels).sum().item()\n",
        "                sub_correct += (sub_preds == sub_labels).sum().item()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        print(f'Validation loss: {running_loss/(i+1):.3f}')\n",
        "        print(f'Validation superclass acc: {100 * super_correct / total:.2f}%')\n",
        "        print(f'Validation subclass acc: {100 * sub_correct / total:.2f}%')\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False):\n",
        "        if not self.test_loader:\n",
        "            raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Evaluate on test set with OpenMax\n",
        "        test_predictions = {\n",
        "            'image': [],\n",
        "            'superclass_index': [],\n",
        "            'subclass_index': []\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(self.device), data[1]\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, sub_preds = self.openmax_model.predict(inputs, self.device)\n",
        "\n",
        "                for j in range(inputs.size(0)):\n",
        "                    img = img_name[j] if isinstance(img_name, list) else img_name[0]\n",
        "\n",
        "                    test_predictions['image'].append(img)\n",
        "                    test_predictions['superclass_index'].append(super_preds[j].item())\n",
        "                    test_predictions['subclass_index'].append(sub_preds[j].item())\n",
        "\n",
        "        test_predictions = pd.DataFrame(data=test_predictions)\n",
        "\n",
        "        # Print summary of novel predictions\n",
        "        novel_super_count = sum(1 for idx in test_predictions['superclass_index'] if idx == self.openmax_model.num_superclasses)\n",
        "        novel_sub_count = sum(1 for idx in test_predictions['subclass_index'] if idx == self.openmax_model.num_subclasses)\n",
        "\n",
        "        print(f'Test set predictions:')\n",
        "        print(f'Images predicted as novel superclass: {novel_super_count} ({100*novel_super_count/len(test_predictions):.2f}%)')\n",
        "        print(f'Images predicted as novel subclass: {novel_sub_count} ({100*novel_sub_count/len(test_predictions):.2f}%)')\n",
        "\n",
        "        if save_to_csv:\n",
        "            test_predictions.to_csv('openmax_test_predictions.csv', index=False)\n",
        "\n",
        "        if return_predictions:\n",
        "            return test_predictions\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "def train_with_openmax(full_dataset, device='cuda', batch_size=64, num_epochs=20):\n",
        "    # Create cross-validation split\n",
        "    from torch.utils.data import random_split\n",
        "\n",
        "    # Split into train and validation\n",
        "    train_size = int(0.9 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = CNN(input_size=64, num_superclasses=4, num_subclasses=88).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Train standard model first\n",
        "    trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, device=device)\n",
        "\n",
        "    print(\"Training standard model...\")\n",
        "    for epoch in range(10):  # Pre-train for 10 epochs\n",
        "        print(f'Epoch {epoch+1}/10')\n",
        "        trainer.train_epoch()\n",
        "\n",
        "    # Initialize OpenMax model\n",
        "    openmax_model = OpenMaxModel(model, num_superclasses=3, num_subclasses=87)\n",
        "\n",
        "    # Fit OpenMax parameters\n",
        "    print(\"\\nFitting OpenMax parameters...\")\n",
        "    openmax_model.fit(train_loader, device)\n",
        "\n",
        "    # Continue training with OpenMax\n",
        "    openmax_trainer = OpenMaxTrainer(model, openmax_model, criterion, optimizer, train_loader, val_loader, device=device)\n",
        "\n",
        "    print(\"\\nTraining with OpenMax...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        openmax_trainer.train_epoch()\n",
        "        openmax_trainer.validate_epoch()\n",
        "        print()\n",
        "\n",
        "    print(\"Finished Training\")\n",
        "\n",
        "    return model, openmax_model\n",
        "\n",
        "\n",
        "# Cross-validation for OpenMax\n",
        "def openmax_cross_validation(full_dataset, image_preprocessing, device='cuda', batch_size=64):\n",
        "    \"\"\"Run cross-validation for OpenMax novel detection\"\"\"\n",
        "    # Get all unique superclass indices\n",
        "    superclass_indices = set()\n",
        "    for i in range(len(full_dataset)):\n",
        "        _, super_idx, _, _, _ = full_dataset[i]\n",
        "        if hasattr(super_idx, 'item'):\n",
        "            super_idx = super_idx.item()\n",
        "        superclass_indices.add(super_idx)\n",
        "\n",
        "    superclass_indices = sorted(list(superclass_indices))\n",
        "    print(f\"Found superclasses with indices: {superclass_indices}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # For each superclass, treat it as novel and others as known\n",
        "    for fold, novel_idx in enumerate(superclass_indices):\n",
        "        print(f\"\\n=== Fold {fold+1}/{len(superclass_indices)}: Treating superclass {novel_idx} as novel ===\")\n",
        "\n",
        "        # Create data splits\n",
        "        known_indices = []\n",
        "        novel_indices = []\n",
        "\n",
        "        for i in range(len(full_dataset)):\n",
        "            _, super_idx, _, _, _ = full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "\n",
        "            if super_idx == novel_idx:\n",
        "                novel_indices.append(i)\n",
        "            else:\n",
        "                known_indices.append(i)\n",
        "\n",
        "        # Further split known indices into train/validation\n",
        "        np.random.shuffle(known_indices)\n",
        "        train_size = int(0.9 * len(known_indices))\n",
        "        train_indices = known_indices[:train_size]\n",
        "        val_known_indices = known_indices[train_size:]\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "        val_known_dataset = torch.utils.data.Subset(full_dataset, val_known_indices)\n",
        "        val_novel_dataset = torch.utils.data.Subset(full_dataset, novel_indices)\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_known_loader = torch.utils.data.DataLoader(val_known_dataset, batch_size=batch_size, shuffle=False)\n",
        "        val_novel_loader = torch.utils.data.DataLoader(val_novel_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Initialize model\n",
        "        model = CNN(input_size=64, num_superclasses=len(superclass_indices)+1).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "        # Train standard model first\n",
        "        print(\"Training standard model...\")\n",
        "        for epoch in range(5):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(train_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                super_outputs, sub_outputs = model(inputs)\n",
        "                loss = criterion(super_outputs, super_labels) + criterion(sub_outputs, sub_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/5, Loss: {running_loss/(i+1):.4f}')\n",
        "\n",
        "        # Initialize OpenMax model\n",
        "        num_known_classes = len(superclass_indices) - 1  # Excluding the novel class\n",
        "        openmax_model = OpenMaxModel(model, num_superclasses=num_known_classes)\n",
        "\n",
        "        # Fit OpenMax parameters\n",
        "        print(\"\\nFitting OpenMax parameters...\")\n",
        "        openmax_model.fit(train_loader, device)\n",
        "\n",
        "        # Evaluate OpenMax on known and novel samples\n",
        "        model.eval()\n",
        "\n",
        "        # Test on known classes\n",
        "        known_correct = 0\n",
        "        known_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in val_known_loader:\n",
        "                inputs, super_labels, _, _, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, _ = openmax_model.predict(inputs, device)\n",
        "\n",
        "                known_total += super_labels.size(0)\n",
        "                known_correct += (super_preds == super_labels).sum().item()\n",
        "\n",
        "        # Test on novel classes\n",
        "        novel_correct = 0\n",
        "        novel_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in val_novel_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, _ = openmax_model.predict(inputs, device)\n",
        "\n",
        "                # For novel classes, prediction should be the novel class index\n",
        "                novel_total += inputs.size(0)\n",
        "                novel_correct += (super_preds == num_known_classes).sum().item()\n",
        "\n",
        "        known_acc = known_correct / known_total if known_total > 0 else 0\n",
        "        novel_acc = novel_correct / novel_total if novel_total > 0 else 0\n",
        "        balanced_acc = (known_acc + novel_acc) / 2\n",
        "\n",
        "        results.append({\n",
        "            'fold': fold,\n",
        "            'novel_class': novel_idx,\n",
        "            'known_accuracy': known_acc,\n",
        "            'novel_accuracy': novel_acc,\n",
        "            'balanced_accuracy': balanced_acc\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold+1} results:\")\n",
        "        print(f\"  Known class accuracy: {known_acc:.4f}\")\n",
        "        print(f\"  Novel class accuracy: {novel_acc:.4f}\")\n",
        "        print(f\"  Balanced accuracy: {balanced_acc:.4f}\")\n",
        "\n",
        "    # Calculate average results\n",
        "    avg_known_acc = sum(r['known_accuracy'] for r in results) / len(results)\n",
        "    avg_novel_acc = sum(r['novel_accuracy'] for r in results) / len(results)\n",
        "    avg_balanced_acc = sum(r['balanced_accuracy'] for r in results) / len(results)\n",
        "\n",
        "    print(\"\\n=== OpenMax Cross-Validation Summary ===\")\n",
        "    print(f\"Average known accuracy: {avg_known_acc:.4f}\")\n",
        "    print(f\"Average novel accuracy: {avg_novel_acc:.4f}\")\n",
        "    print(f\"Average balanced accuracy: {avg_balanced_acc:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "LqTJlERxTGkF"
      },
      "id": "LqTJlERxTGkF",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. First, cross-validate to evaluate OpenMax performance\n",
        "results = openmax_cross_validation(full_dataset, image_preprocessing)\n",
        "\n",
        "# 2. Then train your final model with OpenMax\n",
        "model, openmax_model = train_with_openmax(full_dataset)\n",
        "\n",
        "# 3. Test with OpenMax on the test set\n",
        "openmax_trainer = OpenMaxTrainer(model, openmax_model, criterion, optimizer, train_loader, val_loader, test_loader, device)\n",
        "predictions = openmax_trainer.test(save_to_csv=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw01ohW8JCH5",
        "outputId": "c03f9dc8-f1e6-45c7-c40d-447fecf420b9"
      },
      "id": "Aw01ohW8JCH5",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found superclasses with indices: [0, 1, 2]\n",
            "\n",
            "=== Fold 1/3: Treating superclass 0 as novel ===\n",
            "Training standard model...\n",
            "Epoch 1/5, Loss: 2.9957\n",
            "Epoch 2/5, Loss: 1.3963\n",
            "Epoch 3/5, Loss: 0.8281\n",
            "Epoch 4/5, Loss: 0.5496\n",
            "Epoch 5/5, Loss: 0.4145\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "Fold 1 results:\n",
            "  Known class accuracy: 0.7568\n",
            "  Novel class accuracy: 0.9946\n",
            "  Balanced accuracy: 0.8757\n",
            "\n",
            "=== Fold 2/3: Treating superclass 1 as novel ===\n",
            "Training standard model...\n",
            "Epoch 1/5, Loss: 3.0070\n",
            "Epoch 2/5, Loss: 1.3814\n",
            "Epoch 3/5, Loss: 0.9175\n",
            "Epoch 4/5, Loss: 0.5548\n",
            "Epoch 5/5, Loss: 0.3574\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "Fold 2 results:\n",
            "  Known class accuracy: 0.6556\n",
            "  Novel class accuracy: 0.9827\n",
            "  Balanced accuracy: 0.8192\n",
            "\n",
            "=== Fold 3/3: Treating superclass 2 as novel ===\n",
            "Training standard model...\n",
            "Epoch 1/5, Loss: 3.2028\n",
            "Epoch 2/5, Loss: 1.4771\n",
            "Epoch 3/5, Loss: 0.8684\n",
            "Epoch 4/5, Loss: 0.5534\n",
            "Epoch 5/5, Loss: 0.3015\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "Fold 3 results:\n",
            "  Known class accuracy: 0.5888\n",
            "  Novel class accuracy: 0.0000\n",
            "  Balanced accuracy: 0.2944\n",
            "\n",
            "=== OpenMax Cross-Validation Summary ===\n",
            "Average known accuracy: 0.6671\n",
            "Average novel accuracy: 0.6591\n",
            "Average balanced accuracy: 0.6631\n",
            "Training standard model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.162\n",
            "Epoch 2/10\n",
            "Training loss: 1.460\n",
            "Epoch 3/10\n",
            "Training loss: 0.947\n",
            "Epoch 4/10\n",
            "Training loss: 0.635\n",
            "Epoch 5/10\n",
            "Training loss: 0.428\n",
            "Epoch 6/10\n",
            "Training loss: 0.327\n",
            "Epoch 7/10\n",
            "Training loss: 0.288\n",
            "Epoch 8/10\n",
            "Training loss: 0.231\n",
            "Epoch 9/10\n",
            "Training loss: 0.189\n",
            "Epoch 10/10\n",
            "Training loss: 0.171\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "\n",
            "Training with OpenMax...\n",
            "Epoch 1/20\n",
            "Training loss: 1.105\n",
            "Validation loss: 1.608\n",
            "Validation superclass acc: 74.09%\n",
            "Validation subclass acc: 48.49%\n",
            "\n",
            "Epoch 2/20\n",
            "Training loss: 0.357\n",
            "Validation loss: 1.076\n",
            "Validation superclass acc: 78.06%\n",
            "Validation subclass acc: 58.51%\n",
            "\n",
            "Epoch 3/20\n",
            "Training loss: 0.119\n",
            "Validation loss: 0.981\n",
            "Validation superclass acc: 78.38%\n",
            "Validation subclass acc: 60.41%\n",
            "\n",
            "Epoch 4/20\n",
            "Training loss: 0.092\n",
            "Validation loss: 1.003\n",
            "Validation superclass acc: 81.88%\n",
            "Validation subclass acc: 65.98%\n",
            "\n",
            "Epoch 5/20\n",
            "Training loss: 0.082\n",
            "Validation loss: 1.092\n",
            "Validation superclass acc: 83.31%\n",
            "Validation subclass acc: 66.30%\n",
            "\n",
            "Epoch 6/20\n",
            "Training loss: 0.063\n",
            "Validation loss: 1.345\n",
            "Validation superclass acc: 81.88%\n",
            "Validation subclass acc: 66.45%\n",
            "\n",
            "Epoch 7/20\n",
            "Training loss: 0.030\n",
            "Validation loss: 1.176\n",
            "Validation superclass acc: 82.35%\n",
            "Validation subclass acc: 64.71%\n",
            "\n",
            "Epoch 8/20\n",
            "Training loss: 0.022\n",
            "Validation loss: 1.041\n",
            "Validation superclass acc: 85.85%\n",
            "Validation subclass acc: 72.18%\n",
            "\n",
            "Epoch 9/20\n",
            "Training loss: 0.013\n",
            "Validation loss: 1.026\n",
            "Validation superclass acc: 84.26%\n",
            "Validation subclass acc: 72.50%\n",
            "\n",
            "Epoch 10/20\n",
            "Training loss: 0.009\n",
            "Validation loss: 0.953\n",
            "Validation superclass acc: 86.01%\n",
            "Validation subclass acc: 75.99%\n",
            "\n",
            "Epoch 11/20\n",
            "Training loss: 0.010\n",
            "Validation loss: 1.042\n",
            "Validation superclass acc: 88.71%\n",
            "Validation subclass acc: 77.27%\n",
            "\n",
            "Epoch 12/20\n",
            "Training loss: 0.010\n",
            "Validation loss: 1.023\n",
            "Validation superclass acc: 88.87%\n",
            "Validation subclass acc: 74.88%\n",
            "\n",
            "Epoch 13/20\n",
            "Training loss: 0.011\n",
            "Validation loss: 1.096\n",
            "Validation superclass acc: 85.21%\n",
            "Validation subclass acc: 76.63%\n",
            "\n",
            "Epoch 14/20\n",
            "Training loss: 0.023\n",
            "Validation loss: 1.425\n",
            "Validation superclass acc: 80.60%\n",
            "Validation subclass acc: 72.02%\n",
            "\n",
            "Epoch 15/20\n",
            "Training loss: 0.045\n",
            "Validation loss: 1.392\n",
            "Validation superclass acc: 88.87%\n",
            "Validation subclass acc: 72.18%\n",
            "\n",
            "Epoch 16/20\n",
            "Training loss: 0.159\n",
            "Validation loss: 2.337\n",
            "Validation superclass acc: 82.83%\n",
            "Validation subclass acc: 64.39%\n",
            "\n",
            "Epoch 17/20\n",
            "Training loss: 0.158\n",
            "Validation loss: 1.340\n",
            "Validation superclass acc: 86.96%\n",
            "Validation subclass acc: 66.61%\n",
            "\n",
            "Epoch 18/20\n",
            "Training loss: 0.106\n",
            "Validation loss: 1.538\n",
            "Validation superclass acc: 86.96%\n",
            "Validation subclass acc: 69.63%\n",
            "\n",
            "Epoch 19/20\n",
            "Training loss: 0.057\n",
            "Validation loss: 1.352\n",
            "Validation superclass acc: 88.87%\n",
            "Validation subclass acc: 71.54%\n",
            "\n",
            "Epoch 20/20\n",
            "Training loss: 0.037\n",
            "Validation loss: 1.243\n",
            "Validation superclass acc: 89.35%\n",
            "Validation subclass acc: 75.20%\n",
            "\n",
            "Finished Training\n",
            "Test set predictions:\n",
            "Images predicted as novel superclass: 0 (0.00%)\n",
            "Images predicted as novel subclass: 0 (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d77cbe-3ba1-46e9-9be1-208b9cabab0b",
      "metadata": {
        "id": "61d77cbe-3ba1-46e9-9be1-208b9cabab0b"
      },
      "outputs": [],
      "source": [
        "test_predictions = trainer.test(save_to_csv=True, return_predictions=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "# Function for OpenMax cross-validation\n",
        "def openmax_cross_validation(full_dataset, device='cuda', batch_size=64, epochs=10):\n",
        "    \"\"\"\n",
        "    Run enhanced cross-validation for OpenMax novel detection\n",
        "\n",
        "    Args:\n",
        "        full_dataset: The complete dataset\n",
        "        device: Device to use (cuda or cpu)\n",
        "        batch_size: Batch size for training\n",
        "        epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        Results dictionary with cross-validation metrics\n",
        "    \"\"\"\n",
        "    # Get all unique superclass indices\n",
        "    superclass_indices = set()\n",
        "    for i in range(len(full_dataset)):\n",
        "        _, super_idx, _, _, _ = full_dataset[i]\n",
        "        if hasattr(super_idx, 'item'):\n",
        "            super_idx = super_idx.item()\n",
        "        superclass_indices.add(super_idx)\n",
        "\n",
        "    superclass_indices = sorted(list(superclass_indices))\n",
        "    print(f\"Found superclasses with indices: {superclass_indices}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # For each superclass, treat it as novel and others as known\n",
        "    for fold, novel_idx in enumerate(superclass_indices):\n",
        "        print(f\"\\n=== Fold {fold+1}/{len(superclass_indices)}: Treating superclass {novel_idx} as novel ===\")\n",
        "\n",
        "        # Create data splits\n",
        "        known_indices = []\n",
        "        novel_indices = []\n",
        "\n",
        "        for i in range(len(full_dataset)):\n",
        "            _, super_idx, _, _, _ = full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "\n",
        "            if super_idx == novel_idx:\n",
        "                novel_indices.append(i)\n",
        "            else:\n",
        "                known_indices.append(i)\n",
        "\n",
        "        # Further split known indices into train/validation with stratification\n",
        "        np.random.shuffle(known_indices)\n",
        "        train_size = int(0.9 * len(known_indices))\n",
        "        train_indices = known_indices[:train_size]\n",
        "        val_known_indices = known_indices[train_size:]\n",
        "\n",
        "        # Create datasets\n",
        "        from torch.utils.data import Subset\n",
        "        train_dataset = Subset(full_dataset, train_indices)\n",
        "        val_known_dataset = Subset(full_dataset, val_known_indices)\n",
        "        val_novel_dataset = Subset(full_dataset, novel_indices)\n",
        "\n",
        "        # Combine known and novel validation sets\n",
        "        val_indices = val_known_indices + novel_indices\n",
        "        val_dataset = Subset(full_dataset, val_indices)\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Separate loaders for evaluation\n",
        "        val_known_loader = torch.utils.data.DataLoader(\n",
        "            val_known_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_novel_loader = torch.utils.data.DataLoader(\n",
        "            val_novel_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Initialize enhanced model\n",
        "        model = CNN(\n",
        "            input_size=64,\n",
        "            num_superclasses=len(superclass_indices)+1\n",
        "        ).to(device)\n",
        "\n",
        "        # Use label smoothing for regularization\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # Use AdamW optimizer with weight decay\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=0.001,\n",
        "            weight_decay=0.0001\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Training model...\")\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            model.train()\n",
        "\n",
        "            for i, data in enumerate(train_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                super_outputs, sub_outputs = model(inputs)\n",
        "                loss = criterion(super_outputs, super_labels) + criterion(sub_outputs, sub_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Print progress every epoch\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/(i+1):.4f}')\n",
        "\n",
        "        # Initialize OpenMax model with adjusted parameters for better novel detection\n",
        "        num_known_classes = len(superclass_indices) - 1  # Excluding the novel class\n",
        "        openmax_model = EnhancedOpenMaxModel(\n",
        "            model,\n",
        "            num_superclasses=num_known_classes,\n",
        "            tailsize=30,      # Increased tailsize for better statistics\n",
        "            alpha=5,          # Reduced alpha to consider fewer top classes\n",
        "            threshold=0.7     # Adjusted threshold for better balance\n",
        "        )\n",
        "\n",
        "        # Fit OpenMax parameters\n",
        "        print(\"\\nFitting OpenMax parameters...\")\n",
        "        openmax_model.fit(train_loader, device)\n",
        "\n",
        "        # Evaluate OpenMax on known and novel samples\n",
        "        model.eval()\n",
        "\n",
        "        # Test on known classes\n",
        "        known_correct = 0\n",
        "        known_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in val_known_loader:\n",
        "                inputs, super_labels, _, _, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, _, _, _ = openmax_model.predict(inputs, device)\n",
        "\n",
        "                known_total += super_labels.size(0)\n",
        "                known_correct += (super_preds == super_labels).sum().item()\n",
        "\n",
        "        # Test on novel classes\n",
        "        novel_correct = 0\n",
        "        novel_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in val_novel_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, _, unknown_probs, _ = openmax_model.predict(inputs, device)\n",
        "\n",
        "                # For novel classes, prediction should be the novel class index\n",
        "                novel_total += inputs.size(0)\n",
        "                novel_correct += (super_preds == num_known_classes).sum().item()\n",
        "\n",
        "                # Print statistics of unknown probabilities for novel samples\n",
        "                if novel_total > 0:\n",
        "                    print(f\"Novel samples unknown probability: mean={np.mean(unknown_probs):.4f}, \"\n",
        "                          f\"min={np.min(unknown_probs):.4f}, max={np.max(unknown_probs):.4f}\")\n",
        "\n",
        "        known_acc = known_correct / known_total if known_total > 0 else 0\n",
        "        novel_acc = novel_correct / novel_total if novel_total > 0 else 0\n",
        "        balanced_acc = (known_acc + novel_acc) / 2\n",
        "\n",
        "        results.append({\n",
        "            'fold': fold,\n",
        "            'novel_class': novel_idx,\n",
        "            'known_accuracy': known_acc,\n",
        "            'novel_accuracy': novel_acc,\n",
        "            'balanced_accuracy': balanced_acc\n",
        "        })\n",
        "\n",
        "        print(f\"Fold {fold+1} results:\")\n",
        "        print(f\"  Known class accuracy: {known_acc:.4f}\")\n",
        "        print(f\"  Novel class accuracy: {novel_acc:.4f}\")\n",
        "        print(f\"  Balanced accuracy: {balanced_acc:.4f}\")\n",
        "\n",
        "    # Calculate average results\n",
        "    avg_known_acc = sum(r['known_accuracy'] for r in results) / len(results)\n",
        "    avg_novel_acc = sum(r['novel_accuracy'] for r in results) / len(results)\n",
        "    avg_balanced_acc = sum(r['balanced_accuracy'] for r in results) / len(results)\n",
        "\n",
        "    print(\"\\n=== Enhanced OpenMax Cross-Validation Summary ===\")\n",
        "    print(f\"Average known accuracy: {avg_known_acc:.4f}\")\n",
        "    print(f\"Average novel accuracy: {avg_novel_acc:.4f}\")\n",
        "    print(f\"Average balanced accuracy: {avg_balanced_acc:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Function to set up and train a complete OpenMax model\n",
        "def setup_and_train_openmax(train_loader, val_loader, test_loader=None, device='cuda', epochs=15):\n",
        "    \"\"\"\n",
        "    Set up and train a complete OpenMax model\n",
        "\n",
        "    Args:\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        test_loader: Test data loader (optional)\n",
        "        device: Device to use (cuda or cpu)\n",
        "        epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (model, openmax_model, trainer)\n",
        "    \"\"\"\n",
        "    # Initialize enhanced model\n",
        "    model = CNN(\n",
        "        input_size=64,\n",
        "        num_superclasses=4,\n",
        "        num_subclasses=88\n",
        "    ).to(device)\n",
        "\n",
        "    # Use label smoothing for regularization\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # Use AdamW optimizer with weight decay\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=0.001,\n",
        "        weight_decay=0.0001\n",
        "    )\n",
        "\n",
        "    # Train the base model first\n",
        "    print(\"Training base model...\")\n",
        "    for epoch in range(5):  # Pre-train for 5 epochs\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(train_loader):\n",
        "            inputs, super_labels, _, sub_labels, _ = data\n",
        "            inputs = inputs.to(device)\n",
        "            super_labels = super_labels.to(device)\n",
        "            sub_labels = sub_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = model(inputs)\n",
        "            loss = criterion(super_outputs, super_labels) + criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/5, Loss: {running_loss/(i+1):.4f}')\n",
        "\n",
        "    # Initialize OpenMax model (assuming 3 known superclasses, excluding novel)\n",
        "    openmax_model = EnhancedOpenMaxModel(\n",
        "        model,\n",
        "        num_superclasses=3,\n",
        "        num_subclasses=87,\n",
        "        tailsize=30,\n",
        "        alpha=5,\n",
        "        threshold=0.7\n",
        "    )\n",
        "\n",
        "    # Fit OpenMax parameters\n",
        "    print(\"\\nFitting OpenMax parameters...\")\n",
        "    openmax_model.fit(train_loader, device)\n",
        "\n",
        "    # Create OpenMax trainer\n",
        "    trainer = EnhancedOpenMaxTrainer(\n",
        "        model=model,\n",
        "        openmax_model=openmax_model,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        test_loader=test_loader,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Continue training with OpenMax\n",
        "    print(\"\\nTraining with OpenMax...\")\n",
        "    best_balanced_acc = 0\n",
        "    best_state_dict = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch+1}/{epochs}')\n",
        "        trainer.train_epoch()\n",
        "        metrics = trainer.validate_epoch()\n",
        "\n",
        "        # Save best model\n",
        "        if metrics['balanced_acc'] > best_balanced_acc:\n",
        "            best_balanced_acc = metrics['balanced_acc']\n",
        "            best_state_dict = model.state_dict().copy()\n",
        "            print(f\"New best model! Balanced accuracy: {best_balanced_acc:.4f}\")\n",
        "\n",
        "    # Load best model if found\n",
        "    if best_state_dict is not None:\n",
        "        model.load_state_dict(best_state_dict)\n",
        "\n",
        "        # Re-fit OpenMax with best model\n",
        "        openmax_model.fit(train_loader, device)\n",
        "\n",
        "    return model, openmax_model, trainer\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# model, openmax_model, trainer = setup_and_train_openmax(train_loader, val_loader, test_loader)\n",
        "# test_predictions = trainer.test(save_to_csv=True)import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import weibull_min\n",
        "from sklearn.preprocessing import normalize\n",
        "from collections import defaultdict\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size=64, num_superclasses=4, num_subclasses=88):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_size = input_size // (2**3)\n",
        "\n",
        "        # First convolutional block with increased capacity\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding='same'),  # Increased from 32 to 64\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Second convolutional block with increased capacity\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding='same'),  # Increased from 64 to 128\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Third convolutional block with increased capacity\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding='same'),  # Increased from 128 to 256\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        self.fc1 = nn.Linear(self.feature_size * self.feature_size * 256, 512)  # Increased capacity\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "\n",
        "        # Classification heads\n",
        "        self.fc3a = nn.Linear(256, num_superclasses)  # Superclass prediction\n",
        "        self.fc3b = nn.Linear(256, num_subclasses)    # Subclass prediction\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "\n",
        "        return super_out, sub_out\n",
        "\n",
        "    def get_features(self, x):\n",
        "        \"\"\"Extract features before the final classification layer\"\"\"\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_logits(self, features):\n",
        "        \"\"\"Get class logits from features\"\"\"\n",
        "        super_out = self.fc3a(features)\n",
        "        sub_out = self.fc3b(features)\n",
        "        return super_out, sub_out\n",
        "\n",
        "\n",
        "class EnhancedOpenMaxModel:\n",
        "    def __init__(self, model, num_superclasses=3, num_subclasses=87, tailsize=20, alpha=6, threshold=0.8):\n",
        "        \"\"\"\n",
        "        Initialize Enhanced OpenMax model with a pre-trained CNN\n",
        "\n",
        "        Args:\n",
        "            model: Pre-trained CNN model\n",
        "            num_superclasses: Number of known superclasses\n",
        "            num_subclasses: Number of known subclasses\n",
        "            tailsize: Number of extremal samples to use for Weibull fitting\n",
        "            alpha: Number of top activations to consider for recalibration\n",
        "            threshold: Threshold for unknown class probability (higher is more conservative)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.num_superclasses = num_superclasses  # Number of known superclasses\n",
        "        self.num_subclasses = num_subclasses  # Number of known subclasses\n",
        "        self.tailsize = tailsize\n",
        "        self.alpha = alpha\n",
        "        self.threshold = threshold  # Increased threshold for better novel detection\n",
        "\n",
        "        # Storage for class means and Weibull models\n",
        "        self.super_means = None\n",
        "        self.sub_means = None\n",
        "        self.super_weibull_models = None\n",
        "        self.sub_weibull_models = None\n",
        "\n",
        "        # Statistics to better normalize the data\n",
        "        self.super_logits_mean = None\n",
        "        self.super_logits_std = None\n",
        "        self.sub_logits_mean = None\n",
        "        self.sub_logits_std = None\n",
        "\n",
        "        # For MAV (Mean Activation Vector) calibration\n",
        "        self.super_mavs = None\n",
        "        self.sub_mavs = None\n",
        "\n",
        "        # Class distance statistics\n",
        "        self.super_dist_mean = None\n",
        "        self.super_dist_std = None\n",
        "        self.sub_dist_mean = None\n",
        "        self.sub_dist_std = None\n",
        "\n",
        "    def fit(self, train_loader, device):\n",
        "        \"\"\"\n",
        "        Fit Weibull distributions to activation vectors for known classes\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Collect activations for each class\n",
        "        super_activations = defaultdict(list)\n",
        "        sub_activations = defaultdict(list)\n",
        "        all_super_logits = []\n",
        "        all_sub_logits = []\n",
        "\n",
        "        # Collect activations for each class\n",
        "        with torch.no_grad():\n",
        "            for data in train_loader:\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                # Get features\n",
        "                features = self.model.get_features(inputs)\n",
        "                super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "                # Store activations for each class\n",
        "                for i in range(inputs.size(0)):\n",
        "                    super_class = super_labels[i].item()\n",
        "                    sub_class = sub_labels[i].item()\n",
        "\n",
        "                    # Only store for known classes\n",
        "                    if super_class < self.num_superclasses:\n",
        "                        super_activations[super_class].append(super_logits[i].cpu().numpy())\n",
        "                        all_super_logits.append(super_logits[i].cpu().numpy())\n",
        "\n",
        "                    if sub_class < self.num_subclasses:\n",
        "                        sub_activations[sub_class].append(sub_logits[i].cpu().numpy())\n",
        "                        all_sub_logits.append(sub_logits[i].cpu().numpy())\n",
        "\n",
        "        # Compute logits statistics for better normalization\n",
        "        all_super_logits = np.array(all_super_logits)\n",
        "        all_sub_logits = np.array(all_sub_logits)\n",
        "\n",
        "        self.super_logits_mean = np.mean(all_super_logits, axis=0)\n",
        "        self.super_logits_std = np.std(all_super_logits, axis=0) + 1e-6\n",
        "\n",
        "        self.sub_logits_mean = np.mean(all_sub_logits, axis=0)\n",
        "        self.sub_logits_std = np.std(all_sub_logits, axis=0) + 1e-6\n",
        "\n",
        "        # Compute means for each class (MAVs)\n",
        "        self.super_mavs = {}\n",
        "        self.sub_mavs = {}\n",
        "\n",
        "        for c in range(self.num_superclasses):\n",
        "            if c in super_activations and len(super_activations[c]) > 0:\n",
        "                self.super_mavs[c] = np.mean(super_activations[c], axis=0)\n",
        "\n",
        "        for c in range(self.num_subclasses):\n",
        "            if c in sub_activations and len(sub_activations[c]) > 0:\n",
        "                self.sub_mavs[c] = np.mean(sub_activations[c], axis=0)\n",
        "\n",
        "        # Compute distances to mean for Weibull fitting\n",
        "        super_dists = defaultdict(list)\n",
        "        sub_dists = defaultdict(list)\n",
        "        all_super_dists = []\n",
        "        all_sub_dists = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in train_loader:\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                # Get features\n",
        "                features = self.model.get_features(inputs)\n",
        "                super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "                # Compute distances\n",
        "                for i in range(inputs.size(0)):\n",
        "                    super_class = super_labels[i].item()\n",
        "                    sub_class = sub_labels[i].item()\n",
        "\n",
        "                    # Only process known classes\n",
        "                    if super_class < self.num_superclasses and super_class in self.super_mavs:\n",
        "                        super_mav = self.super_mavs[super_class]\n",
        "                        super_logit = super_logits[i].cpu().numpy()\n",
        "\n",
        "                        # Normalize logits for better distance calculation\n",
        "                        norm_super_logit = (super_logit - self.super_logits_mean) / self.super_logits_std\n",
        "                        norm_super_mav = (super_mav - self.super_logits_mean) / self.super_logits_std\n",
        "\n",
        "                        # Use cosine distance instead of euclidean for better results\n",
        "                        super_dist = 1 - np.dot(norm_super_logit, norm_super_mav) / (\n",
        "                            np.linalg.norm(norm_super_logit) * np.linalg.norm(norm_super_mav) + 1e-10)\n",
        "\n",
        "                        super_dists[super_class].append(super_dist)\n",
        "                        all_super_dists.append(super_dist)\n",
        "\n",
        "                    if sub_class < self.num_subclasses and sub_class in self.sub_mavs:\n",
        "                        sub_mav = self.sub_mavs[sub_class]\n",
        "                        sub_logit = sub_logits[i].cpu().numpy()\n",
        "\n",
        "                        # Normalize logits\n",
        "                        norm_sub_logit = (sub_logit - self.sub_logits_mean) / self.sub_logits_std\n",
        "                        norm_sub_mav = (sub_mav - self.sub_logits_mean) / self.sub_logits_std\n",
        "\n",
        "                        # Use cosine distance\n",
        "                        sub_dist = 1 - np.dot(norm_sub_logit, norm_sub_mav) / (\n",
        "                            np.linalg.norm(norm_sub_logit) * np.linalg.norm(norm_sub_mav) + 1e-10)\n",
        "\n",
        "                        sub_dists[sub_class].append(sub_dist)\n",
        "                        all_sub_dists.append(sub_dist)\n",
        "\n",
        "        # Compute global distance statistics\n",
        "        all_super_dists = np.array(all_super_dists)\n",
        "        all_sub_dists = np.array(all_sub_dists)\n",
        "\n",
        "        self.super_dist_mean = np.mean(all_super_dists)\n",
        "        self.super_dist_std = np.std(all_super_dists) + 1e-6\n",
        "\n",
        "        self.sub_dist_mean = np.mean(all_sub_dists)\n",
        "        self.sub_dist_std = np.std(all_sub_dists) + 1e-6\n",
        "\n",
        "        print(f\"Super distance stats: mean={self.super_dist_mean:.4f}, std={self.super_dist_std:.4f}\")\n",
        "        print(f\"Sub distance stats: mean={self.sub_dist_mean:.4f}, std={self.sub_dist_std:.4f}\")\n",
        "\n",
        "        # Fit Weibull models\n",
        "        self.super_weibull_models = {}\n",
        "        self.sub_weibull_models = {}\n",
        "\n",
        "        for c in range(self.num_superclasses):\n",
        "            if c in super_dists and len(super_dists[c]) >= self.tailsize:\n",
        "                # Sort distances and take tailsize largest\n",
        "                sorted_dists = sorted(super_dists[c])\n",
        "                tail_dists = sorted_dists[-self.tailsize:]\n",
        "\n",
        "                # Fit Weibull distribution\n",
        "                try:\n",
        "                    shape, loc, scale = weibull_min.fit(tail_dists, floc=0)\n",
        "                    self.super_weibull_models[c] = (shape, loc, scale)\n",
        "                    print(f\"Fitted Weibull for superclass {c}: shape={shape:.4f}, scale={scale:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Failed to fit Weibull for superclass {c}: {e}\")\n",
        "\n",
        "        for c in range(self.num_subclasses):\n",
        "            if c in sub_dists and len(sub_dists[c]) >= self.tailsize:\n",
        "                # Sort distances and take tailsize largest\n",
        "                sorted_dists = sorted(sub_dists[c])\n",
        "                tail_dists = sorted_dists[-self.tailsize:]\n",
        "\n",
        "                # Fit Weibull distribution\n",
        "                try:\n",
        "                    shape, loc, scale = weibull_min.fit(tail_dists, floc=0)\n",
        "                    self.sub_weibull_models[c] = (shape, loc, scale)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Failed to fit Weibull for subclass {c}: {e}\")\n",
        "\n",
        "    def predict(self, inputs, device):\n",
        "        \"\"\"\n",
        "        Predict with Enhanced OpenMax recalibration\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get features and logits\n",
        "            features = self.model.get_features(inputs)\n",
        "            super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "            # Convert to numpy for processing\n",
        "            super_logits_np = super_logits.cpu().numpy()\n",
        "            sub_logits_np = sub_logits.cpu().numpy()\n",
        "\n",
        "            # Process each sample\n",
        "            super_preds = []\n",
        "            sub_preds = []\n",
        "            super_unknown_probs = []\n",
        "            sub_unknown_probs = []\n",
        "\n",
        "            for i in range(inputs.size(0)):\n",
        "                # Recalibrate superclass logits\n",
        "                super_logit = super_logits_np[i]\n",
        "                super_pred, super_unknown_prob = self._recalibrate_sample(\n",
        "                    super_logit,\n",
        "                    self.super_mavs,\n",
        "                    self.super_weibull_models,\n",
        "                    self.num_superclasses,\n",
        "                    self.super_logits_mean,\n",
        "                    self.super_logits_std,\n",
        "                    self.super_dist_mean,\n",
        "                    self.super_dist_std\n",
        "                )\n",
        "                super_preds.append(super_pred)\n",
        "                super_unknown_probs.append(super_unknown_prob)\n",
        "\n",
        "                # Recalibrate subclass logits\n",
        "                sub_logit = sub_logits_np[i]\n",
        "                sub_pred, sub_unknown_prob = self._recalibrate_sample(\n",
        "                    sub_logit,\n",
        "                    self.sub_mavs,\n",
        "                    self.sub_weibull_models,\n",
        "                    self.num_subclasses,\n",
        "                    self.sub_logits_mean,\n",
        "                    self.sub_logits_std,\n",
        "                    self.sub_dist_mean,\n",
        "                    self.sub_dist_std\n",
        "                )\n",
        "                sub_preds.append(sub_pred)\n",
        "                sub_unknown_probs.append(sub_unknown_prob)\n",
        "\n",
        "            # Convert back to tensors\n",
        "            super_preds = torch.tensor(super_preds, device=device)\n",
        "            sub_preds = torch.tensor(sub_preds, device=device)\n",
        "\n",
        "        return super_preds, sub_preds, super_unknown_probs, sub_unknown_probs\n",
        "\n",
        "    def _recalibrate_sample(self, logits, mavs, weibull_models, num_classes,\n",
        "                           logits_mean, logits_std, dist_mean, dist_std):\n",
        "        \"\"\"\n",
        "        Recalibrate logits for a single sample with enhanced techniques\n",
        "\n",
        "        Args:\n",
        "            logits: Logits from the model for a single sample\n",
        "            mavs: Mean Activation Vectors for each class\n",
        "            weibull_models: Fitted Weibull models\n",
        "            num_classes: Number of known classes\n",
        "            logits_mean: Mean of all logits for normalization\n",
        "            logits_std: Standard deviation of all logits for normalization\n",
        "            dist_mean: Mean of all distances for normalization\n",
        "            dist_std: Standard deviation of all distances for normalization\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (predicted class including novel class, unknown probability)\n",
        "        \"\"\"\n",
        "        # Get top alpha class indices\n",
        "        top_alpha_idx = np.argsort(logits)[-self.alpha:]\n",
        "\n",
        "        # Normalize logits\n",
        "        norm_logits = (logits - logits_mean) / logits_std\n",
        "\n",
        "        # Compute distances to class means and weibull scores\n",
        "        distances = {}\n",
        "        weibull_scores = {}\n",
        "\n",
        "        # Calculate distance to each known class\n",
        "        for c in range(num_classes):\n",
        "            if c in mavs:\n",
        "                mav = mavs[c]\n",
        "                norm_mav = (mav - logits_mean) / logits_std\n",
        "\n",
        "                # Use cosine distance\n",
        "                dist = 1 - np.dot(norm_logits, norm_mav) / (\n",
        "                    np.linalg.norm(norm_logits) * np.linalg.norm(norm_mav) + 1e-10)\n",
        "\n",
        "                # Normalize distance\n",
        "                norm_dist = (dist - dist_mean) / dist_std\n",
        "                distances[c] = norm_dist\n",
        "\n",
        "                # Calculate weibull score (probability of being an outlier)\n",
        "                if c in weibull_models:\n",
        "                    shape, loc, scale = weibull_models[c]\n",
        "                    try:\n",
        "                        # Higher score means more likely to be an outlier\n",
        "                        weibull_scores[c] = 1 - weibull_min.cdf(dist, shape, loc, scale)\n",
        "                    except:\n",
        "                        weibull_scores[c] = 0.5  # Default if calculation fails\n",
        "\n",
        "        # Recalibrate activations using Weibull scores\n",
        "        recalibrated = np.copy(logits)\n",
        "\n",
        "        # Weight for outlier evidence\n",
        "        evidence_weights = []\n",
        "\n",
        "        for c in top_alpha_idx:\n",
        "            if c < num_classes and c in weibull_models:\n",
        "                # Get weibull score\n",
        "                w_score = weibull_scores.get(c, 0.5)\n",
        "\n",
        "                # Adjust activation - more reduction for higher weibull scores\n",
        "                recalibrated[c] = logits[c] * (1 - w_score)\n",
        "\n",
        "                # Collect evidence for being an outlier\n",
        "                evidence_weights.append(w_score)\n",
        "\n",
        "        # Apply softmax to get probabilities (excluding unknown)\n",
        "        # Add small epsilon to prevent underflow\n",
        "        recalibrated_exp = np.exp(recalibrated - np.max(recalibrated))\n",
        "        recalibrated_probs = recalibrated_exp / (np.sum(recalibrated_exp) + 1e-10)\n",
        "\n",
        "        # Compute probability of being unknown using average of evidence weights\n",
        "        # Higher values of evidence_weights indicate stronger evidence of being novel\n",
        "        if evidence_weights:\n",
        "            avg_evidence = np.mean(evidence_weights)\n",
        "\n",
        "            # Apply sigmoid scaling to convert evidence to probability\n",
        "            unknown_prob = 1.0 / (1.0 + np.exp(-10 * (avg_evidence - 0.5)))\n",
        "        else:\n",
        "            unknown_prob = 0.5  # Default if no evidence is available\n",
        "\n",
        "        # Make final prediction (including novel class)\n",
        "        if unknown_prob > self.threshold:\n",
        "            return num_classes, unknown_prob  # Return novel class index\n",
        "        else:\n",
        "            return np.argmax(recalibrated_probs), unknown_prob\n",
        "\n",
        "\n",
        "class EnhancedOpenMaxTrainer:\n",
        "    def __init__(self, model, openmax_model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda'):\n",
        "        self.model = model\n",
        "        self.openmax_model = openmax_model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "        # Add scheduler for better training\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=10, eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        # For center loss\n",
        "        self.centers = None\n",
        "        self.center_loss_weight = 0.05\n",
        "\n",
        "    def _init_centers(self, num_classes, feature_dim):\n",
        "        \"\"\"Initialize class centers for center loss\"\"\"\n",
        "        self.centers = torch.zeros(num_classes, feature_dim, device=self.device)\n",
        "\n",
        "    def _update_centers(self, features, labels, alpha=0.1):\n",
        "        \"\"\"Update class centers based on current batch\"\"\"\n",
        "        if self.centers is None:\n",
        "            # Initialize centers if not done already\n",
        "            self._init_centers(4, features.size(1))  # Assuming 4 superclasses\n",
        "\n",
        "        # Create a mask for each class in the batch\n",
        "        batch_size = features.size(0)\n",
        "        unique_labels = torch.unique(labels)\n",
        "\n",
        "        for label in unique_labels:\n",
        "            if label >= self.centers.size(0):\n",
        "                continue  # Skip if label is out of range\n",
        "\n",
        "            mask = (labels == label).float()\n",
        "            mask_sum = mask.sum()\n",
        "\n",
        "            if mask_sum > 0:\n",
        "                # Compute mean features for this class in the batch\n",
        "                class_features = torch.sum(features * mask.unsqueeze(1), dim=0) / mask_sum\n",
        "\n",
        "                # Update center with moving average\n",
        "                self.centers[label] = alpha * class_features + (1 - alpha) * self.centers[label]\n",
        "\n",
        "    def _center_loss(self, features, labels):\n",
        "        \"\"\"Calculate center loss\"\"\"\n",
        "        if self.centers is None:\n",
        "            return 0.0\n",
        "\n",
        "        batch_size = features.size(0)\n",
        "\n",
        "        # Gather features by labels\n",
        "        centers_batch = self.centers[labels]\n",
        "\n",
        "        # Calculate center loss\n",
        "        return 0.5 * torch.sum(torch.pow(features - centers_batch, 2)) / batch_size\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, _, sub_labels, _ = data\n",
        "            inputs = inputs.to(self.device)\n",
        "            super_labels = super_labels.to(self.device)\n",
        "            sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            features = self.model.get_features(inputs)\n",
        "            super_outputs, sub_outputs = self.model.get_logits(features)\n",
        "\n",
        "            # Standard cross-entropy loss\n",
        "            ce_loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "\n",
        "            # Center loss for better feature separation\n",
        "            c_loss = self._center_loss(features, super_labels)\n",
        "\n",
        "            # Combined loss\n",
        "            loss = ce_loss + self.center_loss_weight * c_loss\n",
        "\n",
        "            # Backward and optimize\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Update center representations\n",
        "            with torch.no_grad():\n",
        "                self._update_centers(features.detach(), super_labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / (i+1)\n",
        "        print(f'Training loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Step the scheduler\n",
        "        self.scheduler.step()\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # For separate known/novel evaluation\n",
        "        novel_total = 0\n",
        "        known_total = 0\n",
        "        novel_correct = 0\n",
        "        known_correct = 0\n",
        "\n",
        "        all_super_unknown_probs = []\n",
        "        all_is_novel = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                super_labels = super_labels.to(self.device)\n",
        "                sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "                # Get features for loss calculation\n",
        "                features = self.model.get_features(inputs)\n",
        "                super_outputs, sub_outputs = self.model.get_logits(features)\n",
        "\n",
        "                # Standard cross-entropy loss\n",
        "                ce_loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "\n",
        "                # Center loss\n",
        "                c_loss = self._center_loss(features, super_labels)\n",
        "\n",
        "                # Combined loss\n",
        "                loss = ce_loss + self.center_loss_weight * c_loss\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, sub_preds, super_unknown_probs, _ = self.openmax_model.predict(inputs, self.device)\n",
        "\n",
        "                # Track overall accuracy\n",
        "                total += super_labels.size(0)\n",
        "                super_correct += (super_preds == super_labels).sum().item()\n",
        "                sub_correct += (sub_preds == sub_labels).sum().item()\n",
        "\n",
        "                # Track novel vs known separately\n",
        "                is_novel = super_labels >= self.openmax_model.num_superclasses\n",
        "                novel_total += is_novel.sum().item()\n",
        "                known_total += (~is_novel).sum().item()\n",
        "\n",
        "                # A novel sample is correctly classified if detected as novel\n",
        "                novel_correct += (is_novel & (super_preds == self.openmax_model.num_superclasses)).sum().item()\n",
        "\n",
        "                # A known sample is correctly classified if prediction matches label\n",
        "                known_correct += ((~is_novel) & (super_preds == super_labels)).sum().item()\n",
        "\n",
        "                # Store probabilities and labels for threshold analysis\n",
        "                all_super_unknown_probs.extend(super_unknown_probs)\n",
        "                all_is_novel.extend(is_novel.cpu().numpy())\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_loss = running_loss / (i+1)\n",
        "        super_acc = 100 * super_correct / total if total > 0 else 0\n",
        "        sub_acc = 100 * sub_correct / total if total > 0 else 0\n",
        "\n",
        "        novel_acc = 100 * novel_correct / novel_total if novel_total > 0 else 0\n",
        "        known_acc = 100 * known_correct / known_total if known_total > 0 else 0\n",
        "        balanced_acc = (novel_acc + known_acc) / 2 if novel_total > 0 and known_total > 0 else 0\n",
        "\n",
        "        print(f'Validation loss: {avg_loss:.4f}')\n",
        "        print(f'Validation superclass acc: {super_acc:.2f}%, subclass acc: {sub_acc:.2f}%')\n",
        "\n",
        "        if novel_total > 0:\n",
        "            print(f'Novel acc: {novel_acc:.2f}%, Known acc: {known_acc:.2f}%, Balanced acc: {balanced_acc:.2f}%')\n",
        "\n",
        "        # Return metrics dictionary\n",
        "        return {\n",
        "            'loss': avg_loss,\n",
        "            'super_acc': super_acc,\n",
        "            'sub_acc': sub_acc,\n",
        "            'novel_acc': novel_acc,\n",
        "            'known_acc': known_acc,\n",
        "            'balanced_acc': balanced_acc\n",
        "        }\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False, output_file='openmax_test_predictions.csv'):\n",
        "        if not self.test_loader:\n",
        "            raise ValueError('test_loader not specified')\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Evaluate on test set with OpenMax\n",
        "        test_predictions = {\n",
        "            'image': [],\n",
        "            'superclass_index': [],\n",
        "            'subclass_index': [],\n",
        "            'unknown_probability': []\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(self.device), data[1]\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, sub_preds, super_unknown_probs, sub_unknown_probs = self.openmax_model.predict(inputs, self.device)\n",
        "\n",
        "                for j in range(inputs.size(0)):\n",
        "                    img = img_name[j] if isinstance(img_name, list) else img_name[0]\n",
        "\n",
        "                    test_predictions['image'].append(img)\n",
        "                    test_predictions['superclass_index'].append(super_preds[j].item())\n",
        "                    test_predictions['subclass_index'].append(sub_preds[j].item())\n",
        "                    test_predictions['unknown_probability'].append(super_unknown_probs[j])\n",
        "\n",
        "        # Create full DataFrame with all information\n",
        "        full_predictions_df = pd.DataFrame(data=test_predictions)\n",
        "\n",
        "        # Create simplified DataFrame for output (compatible with original code)\n",
        "        simplified_predictions = {\n",
        "            'image': test_predictions['image'],\n",
        "            'superclass_index': test_predictions['superclass_index'],\n",
        "            'subclass_index': test_predictions['subclass_index']\n",
        "        }\n",
        "        simplified_predictions_df = pd.DataFrame(data=simplified_predictions)\n",
        "\n",
        "        # Print summary of novel predictions\n",
        "        novel_super_count = sum(1 for idx in test_predictions['superclass_index'] if idx == self.openmax_model.num_superclasses)\n",
        "        novel_sub_count = sum(1 for idx in test_predictions['subclass_index'] if idx == self.openmax_model.num_subclasses)\n",
        "\n",
        "        total_count = len(test_predictions['image'])\n",
        "        novel_super_perc = 100 * novel_super_count / total_count if total_count > 0 else 0\n",
        "        novel_sub_perc = 100 * novel_sub_count / total_count if total_count > 0 else 0\n",
        "\n",
        "        print(f'Test set predictions:')\n",
        "        print(f'Images predicted as novel superclass: {novel_super_count} ({novel_super_perc:.2f}%)')\n",
        "        print(f'Images predicted as novel subclass: {novel_sub_count} ({novel_sub_perc:.2f}%)')\n",
        "\n",
        "        # Also print unknown probability statistics\n",
        "        unknown_probs = np.array(test_predictions['unknown_probability'])\n",
        "        print(f'Unknown probability statistics:')\n",
        "        print(f'  Mean: {np.mean(unknown_probs):.4f}')\n",
        "        print(f'  Std: {np.std(unknown_probs):.4f}')\n",
        "        print(f'  Min: {np.min(unknown_probs):.4f}')\n",
        "        print(f'  Max: {np.max(unknown_probs):.4f}')\n",
        "\n",
        "        if save_to_csv:\n",
        "            # Save in the same format as the original code\n",
        "            simplified_predictions_df.to_csv(output_file, index=False)\n",
        "            print(f\"Predictions saved to '{output_file}'\")\n",
        "\n",
        "            # # Save detailed version with probabilities\n",
        "            # full_predictions_df.to_csv(f'detailed_{output_file}', index=False)\n",
        "            # print(f\"Detailed predictions saved to 'detailed_{output_file}'\")\n",
        "\n",
        "        if return_predictions:\n",
        "            # Return the predictions for further analysis\n",
        "            return full_predictions_df"
      ],
      "metadata": {
        "id": "KWXivc4VulUy",
        "cellView": "form"
      },
      "id": "KWXivc4VulUy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = openmax_cross_validation(full_dataset, device='cuda', batch_size=64, epochs=10)\n",
        "model, openmax_model, trainer = setup_and_train_openmax(train_loader, val_loader, test_loader)\n",
        "test_predictions = trainer.test(save_to_csv=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcevTYzYuqF_",
        "outputId": "3b440c10-89a9-445b-c90d-af8d0d0fb347"
      },
      "id": "rcevTYzYuqF_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found superclasses with indices: [0, 1, 2]\n",
            "\n",
            "=== Fold 1/3: Treating superclass 0 as novel ===\n",
            "Training model...\n",
            "Epoch 1/10, Loss: 4.4512\n",
            "Epoch 2/10, Loss: 3.2227\n",
            "Epoch 3/10, Loss: 2.7819\n",
            "Epoch 4/10, Loss: 2.4416\n",
            "Epoch 5/10, Loss: 2.2537\n",
            "Epoch 6/10, Loss: 2.0857\n",
            "Epoch 7/10, Loss: 1.9910\n",
            "Epoch 8/10, Loss: 1.8896\n",
            "Epoch 9/10, Loss: 1.8026\n",
            "Epoch 10/10, Loss: 1.7324\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "Super distance stats: mean=1.0000, std=0.0000\n",
            "Sub distance stats: mean=0.1782, std=0.1414\n",
            "Fitted Weibull for superclass 1: shape=2886523556.1939, scale=1.0000\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Fold 1 results:\n",
            "  Known class accuracy: 0.9932\n",
            "  Novel class accuracy: 0.6022\n",
            "  Balanced accuracy: 0.7977\n",
            "\n",
            "=== Fold 2/3: Treating superclass 1 as novel ===\n",
            "Training model...\n",
            "Epoch 1/10, Loss: 4.1779\n",
            "Epoch 2/10, Loss: 3.0904\n",
            "Epoch 3/10, Loss: 2.6608\n",
            "Epoch 4/10, Loss: 2.4365\n",
            "Epoch 5/10, Loss: 2.1742\n",
            "Epoch 6/10, Loss: 2.0424\n",
            "Epoch 7/10, Loss: 1.9509\n",
            "Epoch 8/10, Loss: 1.8168\n",
            "Epoch 9/10, Loss: 1.7761\n",
            "Epoch 10/10, Loss: 1.7047\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "Super distance stats: mean=1.0000, std=0.0000\n",
            "Sub distance stats: mean=0.1946, std=0.1536\n",
            "Fitted Weibull for superclass 0: shape=2886523556.1939, scale=1.0000\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Novel samples unknown probability: mean=0.2574, min=0.2574, max=0.2574\n",
            "Fold 2 results:\n",
            "  Known class accuracy: 0.9810\n",
            "  Novel class accuracy: 0.1900\n",
            "  Balanced accuracy: 0.5855\n",
            "\n",
            "=== Fold 3/3: Treating superclass 2 as novel ===\n",
            "Training model...\n",
            "Epoch 1/10, Loss: 4.0722\n",
            "Epoch 2/10, Loss: 2.8967\n",
            "Epoch 3/10, Loss: 2.5428\n",
            "Epoch 4/10, Loss: 2.2273\n",
            "Epoch 5/10, Loss: 2.0717\n",
            "Epoch 6/10, Loss: 1.8723\n",
            "Epoch 7/10, Loss: 1.7873\n",
            "Epoch 8/10, Loss: 1.6975\n",
            "Epoch 9/10, Loss: 1.6199\n",
            "Epoch 10/10, Loss: 1.5659\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "Super distance stats: mean=0.2491, std=0.2053\n",
            "Sub distance stats: mean=0.1728, std=0.1385\n",
            "Fitted Weibull for superclass 0: shape=6.6153, scale=0.9118\n",
            "Fitted Weibull for superclass 1: shape=4.0489, scale=0.8700\n",
            "Novel samples unknown probability: mean=0.3140, min=0.0320, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3025, min=0.0320, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3365, min=0.0324, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3245, min=0.0320, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3067, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3556, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3324, min=0.0322, max=0.5000\n",
            "Novel samples unknown probability: mean=0.2879, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.2785, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3256, min=0.0322, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3145, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3367, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.2795, min=0.0321, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3289, min=0.0321, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3654, min=0.0329, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3444, min=0.0321, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3241, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.2933, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3378, min=0.0328, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3310, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.2558, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3124, min=0.0330, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3531, min=0.0320, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3380, min=0.0327, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3155, min=0.0324, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3303, min=0.0320, max=0.5000\n",
            "Novel samples unknown probability: mean=0.2865, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3346, min=0.0321, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3026, min=0.0322, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3112, min=0.0328, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3412, min=0.0322, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3133, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3102, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.2774, min=0.0320, max=0.5000\n",
            "Novel samples unknown probability: mean=0.2883, min=0.0319, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3439, min=0.0324, max=0.5000\n",
            "Novel samples unknown probability: mean=0.3431, min=0.0384, max=0.5000\n",
            "Fold 3 results:\n",
            "  Known class accuracy: 0.4848\n",
            "  Novel class accuracy: 0.0127\n",
            "  Balanced accuracy: 0.2488\n",
            "\n",
            "=== Enhanced OpenMax Cross-Validation Summary ===\n",
            "Average known accuracy: 0.8197\n",
            "Average novel accuracy: 0.2683\n",
            "Average balanced accuracy: 0.5440\n",
            "Training base model...\n",
            "Epoch 1/5, Loss: 4.4593\n",
            "Epoch 2/5, Loss: 3.2910\n",
            "Epoch 3/5, Loss: 2.7985\n",
            "Epoch 4/5, Loss: 2.4420\n",
            "Epoch 5/5, Loss: 2.2343\n",
            "\n",
            "Fitting OpenMax parameters...\n",
            "Super distance stats: mean=0.1793, std=0.1621\n",
            "Sub distance stats: mean=0.1596, std=0.1200\n",
            "Fitted Weibull for superclass 0: shape=8.6508, scale=0.7305\n",
            "Fitted Weibull for superclass 1: shape=5.3464, scale=0.7367\n",
            "Fitted Weibull for superclass 2: shape=5.8170, scale=0.8994\n",
            "\n",
            "Training with OpenMax...\n",
            "Epoch 1/15\n",
            "Training loss: 9.8794\n",
            "Validation loss: 4.9378\n",
            "Validation superclass acc: 95.70%, subclass acc: 14.97%\n",
            "Epoch 2/15\n",
            "Training loss: 4.8689\n",
            "Validation loss: 4.3166\n",
            "Validation superclass acc: 96.82%, subclass acc: 17.99%\n",
            "Epoch 3/15\n",
            "Training loss: 4.4943\n",
            "Validation loss: 4.1208\n",
            "Validation superclass acc: 97.93%, subclass acc: 21.97%\n",
            "Epoch 4/15\n",
            "Training loss: 4.2856\n",
            "Validation loss: 4.0312\n",
            "Validation superclass acc: 96.34%, subclass acc: 27.55%\n",
            "Epoch 5/15\n",
            "Training loss: 4.1390\n",
            "Validation loss: 3.8500\n",
            "Validation superclass acc: 98.89%, subclass acc: 31.85%\n",
            "Epoch 6/15\n",
            "Training loss: 3.9911\n",
            "Validation loss: 3.7221\n",
            "Validation superclass acc: 99.04%, subclass acc: 35.67%\n",
            "Epoch 7/15\n",
            "Training loss: 3.8735\n",
            "Validation loss: 3.6086\n",
            "Validation superclass acc: 98.89%, subclass acc: 39.33%\n",
            "Epoch 8/15\n",
            "Training loss: 3.7821\n",
            "Validation loss: 3.5111\n",
            "Validation superclass acc: 99.36%, subclass acc: 44.43%\n",
            "Epoch 9/15\n",
            "Training loss: 3.7165\n",
            "Validation loss: 3.4954\n",
            "Validation superclass acc: 99.36%, subclass acc: 47.29%\n",
            "Epoch 10/15\n",
            "Training loss: 3.6896\n",
            "Validation loss: 3.4727\n",
            "Validation superclass acc: 99.36%, subclass acc: 48.25%\n",
            "Epoch 11/15\n",
            "Training loss: 3.6779\n",
            "Validation loss: 3.4722\n",
            "Validation superclass acc: 99.36%, subclass acc: 48.41%\n",
            "Epoch 12/15\n",
            "Training loss: 3.6727\n",
            "Validation loss: 3.4563\n",
            "Validation superclass acc: 99.36%, subclass acc: 49.20%\n",
            "Epoch 13/15\n",
            "Training loss: 3.6582\n",
            "Validation loss: 3.4363\n",
            "Validation superclass acc: 99.36%, subclass acc: 48.73%\n",
            "Epoch 14/15\n",
            "Training loss: 3.6499\n",
            "Validation loss: 3.3743\n",
            "Validation superclass acc: 99.04%, subclass acc: 49.84%\n",
            "Epoch 15/15\n",
            "Training loss: 3.5932\n",
            "Validation loss: 3.3191\n",
            "Validation superclass acc: 98.73%, subclass acc: 50.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "#BEST OPEN MAX ENHANCED MODEL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import weibull_min\n",
        "from sklearn.preprocessing import normalize\n",
        "from collections import defaultdict\n",
        "\n",
        "class OptimizedCNN(nn.Module):\n",
        "    def __init__(self, input_size=64, num_superclasses=4, num_subclasses=88):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_size = input_size // (2**3)\n",
        "\n",
        "        # First convolutional block with increased capacity\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Second convolutional block with increased capacity\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Third convolutional block with increased capacity\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        self.fc1 = nn.Linear(self.feature_size * self.feature_size * 256, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)  # Added BatchNorm after FC\n",
        "        self.dropout1 = nn.Dropout(0.3)  # Increased dropout rate\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        # Classification heads\n",
        "        self.fc3a = nn.Linear(256, num_superclasses)\n",
        "        self.fc3b = nn.Linear(256, num_subclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "\n",
        "        return super_out, sub_out\n",
        "\n",
        "    def get_features(self, x):\n",
        "        \"\"\"Extract features before the final classification layer\"\"\"\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_logits(self, features):\n",
        "        \"\"\"Get class logits from features\"\"\"\n",
        "        super_out = self.fc3a(features)\n",
        "        sub_out = self.fc3b(features)\n",
        "        return super_out, sub_out\n",
        "\n",
        "\n",
        "class OptimizedOpenMaxModel:\n",
        "    def __init__(self, model, num_superclasses=3, num_subclasses=87, tailsize=40, alpha=4,\n",
        "                 threshold=0.6, distance_multiplier=1.5):\n",
        "\n",
        "        self.model = model\n",
        "        self.num_superclasses = num_superclasses\n",
        "        self.num_subclasses = num_subclasses\n",
        "        self.tailsize = tailsize\n",
        "        self.alpha = alpha\n",
        "        self.threshold = threshold\n",
        "        self.distance_multiplier = distance_multiplier\n",
        "\n",
        "        # Storage for class means and Weibull models\n",
        "        self.super_mavs = None  # Mean Activation Vectors\n",
        "        self.sub_mavs = None\n",
        "        self.super_weibull_models = None\n",
        "        self.sub_weibull_models = None\n",
        "\n",
        "        # Statistics for normalization\n",
        "        self.super_logits_mean = None\n",
        "        self.super_logits_std = None\n",
        "        self.sub_logits_mean = None\n",
        "        self.sub_logits_std = None\n",
        "        self.super_dist_mean = None\n",
        "        self.super_dist_std = None\n",
        "        self.sub_dist_mean = None\n",
        "        self.sub_dist_std = None\n",
        "\n",
        "    def fit(self, train_loader, device):\n",
        "        \"\"\"\n",
        "        Fit Weibull distributions to activation vectors for known classes\n",
        "        with improved normalization and calibration\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        print(\"\\nFitting OpenMax parameters with distance_multiplier =\", self.distance_multiplier)\n",
        "\n",
        "        # Collect activations for each class\n",
        "        super_activations = defaultdict(list)\n",
        "        sub_activations = defaultdict(list)\n",
        "        all_super_logits = []\n",
        "        all_sub_logits = []\n",
        "\n",
        "        # Collect activations for each class\n",
        "        print(\"Collecting class activations...\")\n",
        "        with torch.no_grad():\n",
        "            for data in train_loader:\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                # Get features\n",
        "                features = self.model.get_features(inputs)\n",
        "                super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "                # Store activations for each class\n",
        "                for i in range(inputs.size(0)):\n",
        "                    super_class = super_labels[i].item()\n",
        "                    sub_class = sub_labels[i].item()\n",
        "\n",
        "                    # Only store for known classes\n",
        "                    if super_class < self.num_superclasses:\n",
        "                        super_activations[super_class].append(super_logits[i].cpu().numpy())\n",
        "                        all_super_logits.append(super_logits[i].cpu().numpy())\n",
        "\n",
        "                    if sub_class < self.num_subclasses:\n",
        "                        sub_activations[sub_class].append(sub_logits[i].cpu().numpy())\n",
        "                        all_sub_logits.append(sub_logits[i].cpu().numpy())\n",
        "\n",
        "        # Compute logits statistics for normalization\n",
        "        all_super_logits = np.array(all_super_logits)\n",
        "        all_sub_logits = np.array(all_sub_logits)\n",
        "\n",
        "        self.super_logits_mean = np.mean(all_super_logits, axis=0)\n",
        "        self.super_logits_std = np.std(all_super_logits, axis=0) + 1e-6\n",
        "\n",
        "        self.sub_logits_mean = np.mean(all_sub_logits, axis=0)\n",
        "        self.sub_logits_std = np.std(all_sub_logits, axis=0) + 1e-6\n",
        "\n",
        "        # Compute Mean Activation Vectors (MAVs)\n",
        "        self.super_mavs = {}\n",
        "        self.sub_mavs = {}\n",
        "\n",
        "        for c in range(self.num_superclasses):\n",
        "            if c in super_activations and len(super_activations[c]) > 0:\n",
        "                self.super_mavs[c] = np.mean(super_activations[c], axis=0)\n",
        "                print(f\"Class {c} MAV stats: mean={np.mean(self.super_mavs[c]):.4f}, std={np.std(self.super_mavs[c]):.4f}\")\n",
        "\n",
        "        for c in range(self.num_subclasses):\n",
        "            if c in sub_activations and len(sub_activations[c]) > 0:\n",
        "                self.sub_mavs[c] = np.mean(sub_activations[c], axis=0)\n",
        "\n",
        "        # Compute distances to mean for Weibull fitting\n",
        "        print(\"Computing distances for Weibull fitting...\")\n",
        "        super_dists = defaultdict(list)\n",
        "        sub_dists = defaultdict(list)\n",
        "        all_super_dists = []\n",
        "        all_sub_dists = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in train_loader:\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                # Get features\n",
        "                features = self.model.get_features(inputs)\n",
        "                super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "                # Compute distances\n",
        "                for i in range(inputs.size(0)):\n",
        "                    super_class = super_labels[i].item()\n",
        "                    sub_class = sub_labels[i].item()\n",
        "\n",
        "                    # Only process known classes\n",
        "                    if super_class < self.num_superclasses and super_class in self.super_mavs:\n",
        "                        super_mav = self.super_mavs[super_class]\n",
        "                        super_logit = super_logits[i].cpu().numpy()\n",
        "\n",
        "                        # Normalize logits for better distance calculation\n",
        "                        norm_super_logit = (super_logit - self.super_logits_mean) / self.super_logits_std\n",
        "                        norm_super_mav = (super_mav - self.super_logits_mean) / self.super_logits_std\n",
        "\n",
        "                        # Use cosine distance for better performance\n",
        "                        super_dist = 1 - np.dot(norm_super_logit, norm_super_mav) / (\n",
        "                            np.linalg.norm(norm_super_logit) * np.linalg.norm(norm_super_mav) + 1e-10)\n",
        "\n",
        "                        # Apply distance multiplier\n",
        "                        super_dist = super_dist * self.distance_multiplier\n",
        "\n",
        "                        super_dists[super_class].append(super_dist)\n",
        "                        all_super_dists.append(super_dist)\n",
        "\n",
        "                    if sub_class < self.num_subclasses and sub_class in self.sub_mavs:\n",
        "                        sub_mav = self.sub_mavs[sub_class]\n",
        "                        sub_logit = sub_logits[i].cpu().numpy()\n",
        "\n",
        "                        # Normalize logits\n",
        "                        norm_sub_logit = (sub_logit - self.sub_logits_mean) / self.sub_logits_std\n",
        "                        norm_sub_mav = (sub_mav - self.sub_logits_mean) / self.sub_logits_std\n",
        "\n",
        "                        # Use cosine distance\n",
        "                        sub_dist = 1 - np.dot(norm_sub_logit, norm_sub_mav) / (\n",
        "                            np.linalg.norm(norm_sub_logit) * np.linalg.norm(norm_sub_mav) + 1e-10)\n",
        "\n",
        "                        sub_dists[sub_class].append(sub_dist)\n",
        "                        all_sub_dists.append(sub_dist)\n",
        "\n",
        "        # Compute global distance statistics\n",
        "        all_super_dists = np.array(all_super_dists)\n",
        "        all_sub_dists = np.array(all_sub_dists)\n",
        "\n",
        "        self.super_dist_mean = np.mean(all_super_dists)\n",
        "        self.super_dist_std = np.std(all_super_dists) + 1e-6\n",
        "\n",
        "        self.sub_dist_mean = np.mean(all_sub_dists)\n",
        "        self.sub_dist_std = np.std(all_sub_dists) + 1e-6\n",
        "\n",
        "        print(f\"Super distance stats: mean={self.super_dist_mean:.4f}, std={self.super_dist_std:.4f}\")\n",
        "        print(f\"Sub distance stats: mean={self.sub_dist_mean:.4f}, std={self.sub_dist_std:.4f}\")\n",
        "\n",
        "        # Fit Weibull models with improved robustness\n",
        "        self.super_weibull_models = {}\n",
        "        self.sub_weibull_models = {}\n",
        "\n",
        "        print(\"Fitting Weibull distributions...\")\n",
        "        for c in range(self.num_superclasses):\n",
        "            if c in super_dists and len(super_dists[c]) >= self.tailsize:\n",
        "                # Sort distances and take tailsize largest\n",
        "                sorted_dists = sorted(super_dists[c])\n",
        "                tail_dists = sorted_dists[-self.tailsize:]\n",
        "\n",
        "                # Fit Weibull with multiple methods for robustness\n",
        "                self.super_weibull_models[c] = self._robust_weibull_fit(tail_dists)\n",
        "                shape, loc, scale = self.super_weibull_models[c]\n",
        "                print(f\"Fitted Weibull for superclass {c}: shape={shape:.4f}, scale={scale:.4f}\")\n",
        "\n",
        "        for c in range(self.num_subclasses):\n",
        "            if c in sub_dists and len(sub_dists[c]) >= self.tailsize:\n",
        "                # Sort distances and take tailsize largest\n",
        "                sorted_dists = sorted(sub_dists[c])\n",
        "                tail_dists = sorted_dists[-self.tailsize:]\n",
        "\n",
        "                # Fit Weibull distribution\n",
        "                self.sub_weibull_models[c] = self._robust_weibull_fit(tail_dists)\n",
        "\n",
        "    def _robust_weibull_fit(self, distances):\n",
        "        \"\"\"\n",
        "        Fit Weibull distribution with multiple methods and fallbacks for robustness\n",
        "        \"\"\"\n",
        "        # Try multiple fitting methods\n",
        "        try:\n",
        "            # First try MLE with fixed location parameter\n",
        "            shape, loc, scale = weibull_min.fit(distances, floc=0)\n",
        "\n",
        "            # Check if shape is reasonable (not too extreme)\n",
        "            if 0.1 <= shape <= 20:\n",
        "                return shape, loc, scale\n",
        "\n",
        "            # If shape is extreme, try moment matching\n",
        "            shape, loc, scale = weibull_min.fit(distances, floc=0, method='mm')\n",
        "            return shape, loc, scale\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback to reasonable defaults\n",
        "            print(f\"Warning: Weibull fitting failed, using defaults: {e}\")\n",
        "            # Use empirical mean and standard deviation for defaults\n",
        "            mean_dist = np.mean(distances)\n",
        "            std_dist = np.std(distances)\n",
        "            # Approximate Weibull parameters\n",
        "            shape = 2.0  # Reasonable default shape\n",
        "            scale = mean_dist # Scale based on mean\n",
        "            return shape, 0, scale\n",
        "\n",
        "    def calibrate_threshold(self, val_loader, device):\n",
        "        \"\"\"\n",
        "        Calibrate the threshold for better novelty detection using validation data\n",
        "        \"\"\"\n",
        "        print(\"\\nCalibrating novelty detection threshold...\")\n",
        "        self.model.eval()\n",
        "\n",
        "        # Collect validation data\n",
        "        all_probs = []\n",
        "        is_novel = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                inputs, super_labels, _, _, _ = data\n",
        "                inputs = inputs.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "\n",
        "                # Get predictions\n",
        "                _, _, unknown_probs, _ = self.predict(inputs, device)\n",
        "\n",
        "                # Store results\n",
        "                novel_labels = super_labels >= self.num_superclasses\n",
        "                all_probs.extend(unknown_probs)\n",
        "                is_novel.extend(novel_labels.cpu().numpy())\n",
        "\n",
        "        # Find optimal threshold\n",
        "        thresholds = np.linspace(0.1, 0.9, 17)  # 0.1, 0.15, 0.2, ..., 0.9\n",
        "        best_threshold = 0.5\n",
        "        best_balanced_acc = 0\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            # Make predictions at this threshold\n",
        "            pred_novel = np.array(all_probs) > threshold\n",
        "            true_novel = np.array(is_novel)\n",
        "\n",
        "            # Calculate metrics\n",
        "            novel_correct = np.sum(pred_novel & true_novel)\n",
        "            novel_total = np.sum(true_novel)\n",
        "            novel_acc = novel_correct / novel_total if novel_total > 0 else 0\n",
        "\n",
        "            known_correct = np.sum((~pred_novel) & (~true_novel))\n",
        "            known_total = np.sum(~true_novel)\n",
        "            known_acc = known_correct / known_total if known_total > 0 else 0\n",
        "\n",
        "            balanced_acc = (novel_acc + known_acc) / 2\n",
        "\n",
        "            print(f\"Threshold {threshold:.2f}: Known={known_acc:.4f}, Novel={novel_acc:.4f}, Balanced={balanced_acc:.4f}\")\n",
        "\n",
        "            if balanced_acc > best_balanced_acc:\n",
        "                best_balanced_acc = balanced_acc\n",
        "                best_threshold = threshold\n",
        "\n",
        "        # Set the calibrated threshold\n",
        "        old_threshold = self.threshold\n",
        "        self.threshold = best_threshold\n",
        "        print(f\"Updated threshold from {old_threshold:.2f} to {best_threshold:.2f}, Balanced accuracy: {best_balanced_acc:.4f}\")\n",
        "\n",
        "        return best_threshold, best_balanced_acc\n",
        "\n",
        "    def predict(self, inputs, device):\n",
        "        \"\"\"\n",
        "        Predict with Enhanced OpenMax recalibration\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get features and logits\n",
        "            features = self.model.get_features(inputs)\n",
        "            super_logits, sub_logits = self.model.get_logits(features)\n",
        "\n",
        "            # Convert to numpy for processing\n",
        "            super_logits_np = super_logits.cpu().numpy()\n",
        "            sub_logits_np = sub_logits.cpu().numpy()\n",
        "\n",
        "            # Process each sample\n",
        "            super_preds = []\n",
        "            sub_preds = []\n",
        "            super_unknown_probs = []\n",
        "            sub_unknown_probs = []\n",
        "\n",
        "            for i in range(inputs.size(0)):\n",
        "                # Recalibrate superclass logits\n",
        "                super_logit = super_logits_np[i]\n",
        "                super_pred, super_unknown_prob = self._recalibrate_sample(\n",
        "                    super_logit,\n",
        "                    self.super_mavs,\n",
        "                    self.super_weibull_models,\n",
        "                    self.num_superclasses,\n",
        "                    self.super_logits_mean,\n",
        "                    self.super_logits_std,\n",
        "                    self.super_dist_mean,\n",
        "                    self.super_dist_std\n",
        "                )\n",
        "                super_preds.append(super_pred)\n",
        "                super_unknown_probs.append(super_unknown_prob)\n",
        "\n",
        "                # Recalibrate subclass logits\n",
        "                sub_logit = sub_logits_np[i]\n",
        "                sub_pred, sub_unknown_prob = self._recalibrate_sample(\n",
        "                    sub_logit,\n",
        "                    self.sub_mavs,\n",
        "                    self.sub_weibull_models,\n",
        "                    self.num_subclasses,\n",
        "                    self.sub_logits_mean,\n",
        "                    self.sub_logits_std,\n",
        "                    self.sub_dist_mean,\n",
        "                    self.sub_dist_std\n",
        "                )\n",
        "                sub_preds.append(sub_pred)\n",
        "                sub_unknown_probs.append(sub_unknown_prob)\n",
        "\n",
        "            # Convert back to tensors\n",
        "            super_preds = torch.tensor(super_preds, device=device)\n",
        "            sub_preds = torch.tensor(sub_preds, device=device)\n",
        "\n",
        "        return super_preds, sub_preds, super_unknown_probs, sub_unknown_probs\n",
        "\n",
        "    def _recalibrate_sample(self, logits, mavs, weibull_models, num_classes,\n",
        "                           logits_mean, logits_std, dist_mean, dist_std):\n",
        "        \"\"\"\n",
        "        Recalibrate logits for a single sample with improved techniques\n",
        "        \"\"\"\n",
        "        # Get top alpha class indices\n",
        "        top_alpha_idx = np.argsort(logits)[-self.alpha:]\n",
        "\n",
        "        # Normalize logits for better distance calculation\n",
        "        norm_logits = (logits - logits_mean) / logits_std\n",
        "        norm_logits_unit = norm_logits / (np.linalg.norm(norm_logits) + 1e-10)  # Unit normalize\n",
        "\n",
        "        # Compute distances to class means and weibull scores\n",
        "        distances = {}\n",
        "        weibull_scores = {}\n",
        "\n",
        "        # Calculate distance to each known class\n",
        "        for c in range(num_classes):\n",
        "            if c in mavs:\n",
        "                mav = mavs[c]\n",
        "                # Normalize MAV the same way as logits\n",
        "                norm_mav = (mav - logits_mean) / logits_std\n",
        "                norm_mav_unit = norm_mav / (np.linalg.norm(norm_mav) + 1e-10)\n",
        "\n",
        "                # Use cosine distance\n",
        "                dist = 1 - np.dot(norm_logits_unit, norm_mav_unit)\n",
        "\n",
        "                # Apply distance multiplier\n",
        "                dist = dist * self.distance_multiplier\n",
        "\n",
        "                # Store distance\n",
        "                distances[c] = dist\n",
        "\n",
        "                # Calculate weibull score if model exists\n",
        "                if c in weibull_models:\n",
        "                    shape, loc, scale = weibull_models[c]\n",
        "                    try:\n",
        "                        # Higher score means more likely to be an outlier\n",
        "                        weibull_scores[c] = 1 - weibull_min.cdf(dist, shape, loc, scale)\n",
        "                    except:\n",
        "                        weibull_scores[c] = 0.5  # Default if calculation fails\n",
        "\n",
        "        # Recalibrate activations using Weibull scores\n",
        "        recalibrated = np.copy(logits)\n",
        "\n",
        "        # Weight for outlier evidence\n",
        "        evidence_weights = []\n",
        "\n",
        "        for c in top_alpha_idx:\n",
        "            if c < num_classes and c in weibull_models:\n",
        "                # Get weibull score\n",
        "                w_score = weibull_scores.get(c, 0.5)\n",
        "\n",
        "                # Adjust activation - more reduction for higher weibull scores\n",
        "                recalibrated[c] = logits[c] * (1 - w_score)\n",
        "\n",
        "                # Collect evidence for being an outlier\n",
        "                evidence_weights.append(w_score)\n",
        "\n",
        "        # Apply softmax to get probabilities (excluding unknown)\n",
        "        recalibrated_exp = np.exp(recalibrated - np.max(recalibrated))\n",
        "        recalibrated_probs = recalibrated_exp / (np.sum(recalibrated_exp) + 1e-10)\n",
        "\n",
        "        # Compute probability of being unknown using average of evidence weights\n",
        "        if evidence_weights:\n",
        "            avg_evidence = np.mean(evidence_weights)\n",
        "\n",
        "            # Apply sigmoid scaling to convert evidence to probability\n",
        "            unknown_prob = 1.0 / (1.0 + np.exp(-10 * (avg_evidence - 0.5)))\n",
        "        else:\n",
        "            unknown_prob = 0.5  # Default if no evidence is available\n",
        "\n",
        "        # Make final prediction (including novel class)\n",
        "        if unknown_prob > self.threshold:\n",
        "            return num_classes, unknown_prob  # Return novel class index\n",
        "        else:\n",
        "            return np.argmax(recalibrated_probs), unknown_prob\n",
        "\n",
        "\n",
        "class OptimizedOpenMaxTrainer:\n",
        "    def __init__(self, model, openmax_model, criterion, optimizer, train_loader, val_loader,\n",
        "                 test_loader=None, device='cuda', center_loss_weight=0.0005):\n",
        "        self.model = model\n",
        "        self.openmax_model = openmax_model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "        # Critical fix: Reduce center loss weight dramatically (from 0.05 to 0.0005)\n",
        "        self.center_loss_weight = center_loss_weight\n",
        "\n",
        "        # Add scheduler for better convergence\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=10, eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        # For center loss\n",
        "        self.centers = None\n",
        "\n",
        "        # For tracking history\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'super_acc': [],\n",
        "            'sub_acc': [],\n",
        "            'novel_acc': [],\n",
        "            'known_acc': [],\n",
        "            'balanced_acc': []\n",
        "        }\n",
        "\n",
        "    def _init_centers(self, num_classes, feature_dim):\n",
        "        \"\"\"Initialize class centers for center loss\"\"\"\n",
        "        self.centers = torch.zeros(num_classes, feature_dim, device=self.device)\n",
        "\n",
        "    def _update_centers(self, features, labels, alpha=0.1):\n",
        "        \"\"\"Update class centers based on current batch\"\"\"\n",
        "        if self.centers is None:\n",
        "            # Initialize centers if not done already\n",
        "            self._init_centers(4, features.size(1))  # Assuming 4 superclasses\n",
        "\n",
        "        # Create a mask for each class in the batch\n",
        "        batch_size = features.size(0)\n",
        "        unique_labels = torch.unique(labels)\n",
        "\n",
        "        for label in unique_labels:\n",
        "            if label >= self.centers.size(0):\n",
        "                continue  # Skip if label is out of range\n",
        "\n",
        "            mask = (labels == label).float()\n",
        "            mask_sum = mask.sum()\n",
        "\n",
        "            if mask_sum > 0:\n",
        "                # Compute mean features for this class in the batch\n",
        "                class_features = torch.sum(features * mask.unsqueeze(1), dim=0) / mask_sum\n",
        "\n",
        "                # Update center with moving average\n",
        "                self.centers[label] = alpha * class_features + (1 - alpha) * self.centers[label]\n",
        "\n",
        "    def _center_loss(self, features, labels):\n",
        "        \"\"\"Calculate center loss\"\"\"\n",
        "        if self.centers is None:\n",
        "            # Return a tensor with 0 value, not a Python float\n",
        "            return torch.tensor(0.0, device=self.device)\n",
        "\n",
        "        batch_size = features.size(0)\n",
        "\n",
        "        # Gather features by labels\n",
        "        centers_batch = self.centers[labels]\n",
        "\n",
        "        # Calculate center loss\n",
        "        return 0.5 * torch.sum(torch.pow(features - centers_batch, 2)) / batch_size\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Train the model for one epoch with improved techniques\"\"\"\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        running_ce_loss = 0.0\n",
        "        running_center_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, _, sub_labels, _ = data\n",
        "            inputs = inputs.to(self.device)\n",
        "            super_labels = super_labels.to(self.device)\n",
        "            sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "            # Forward pass with feature extraction\n",
        "            features = self.model.get_features(inputs)\n",
        "            super_outputs, sub_outputs = self.model.get_logits(features)\n",
        "\n",
        "            # Calculate standard cross-entropy loss\n",
        "            ce_loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "\n",
        "            # Calculate center loss\n",
        "            c_loss = self._center_loss(features, super_labels)\n",
        "\n",
        "            # Combined loss\n",
        "            loss = ce_loss + self.center_loss_weight * c_loss\n",
        "\n",
        "            # Backward and optimize\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Update center representations\n",
        "            with torch.no_grad():\n",
        "                self._update_centers(features.detach(), super_labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_ce_loss += ce_loss.item()\n",
        "            running_center_loss += c_loss.item()\n",
        "\n",
        "        avg_loss = running_loss / (i+1)\n",
        "        avg_ce_loss = running_ce_loss / (i+1)\n",
        "        avg_center_loss = running_center_loss / (i+1)\n",
        "\n",
        "        print(f'Training loss: {avg_loss:.4f} (CE: {avg_ce_loss:.4f}, Center: {avg_center_loss*self.center_loss_weight:.4f})')\n",
        "\n",
        "        # Step the scheduler if available\n",
        "        if self.scheduler is not None:\n",
        "            self.scheduler.step()\n",
        "\n",
        "        # Update history\n",
        "        self.history['train_loss'].append(avg_loss)\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        \"\"\"Validate with improved metrics tracking\"\"\"\n",
        "        if self.openmax_model is None:\n",
        "            return self._validate_without_openmax()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        super_correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # For separate known/novel evaluation\n",
        "        novel_total = 0\n",
        "        known_total = 0\n",
        "        novel_correct = 0\n",
        "        known_correct = 0\n",
        "\n",
        "        all_super_unknown_probs = []\n",
        "        all_is_novel = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                super_labels = super_labels.to(self.device)\n",
        "                sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "                # Get features for loss calculation\n",
        "                features = self.model.get_features(inputs)\n",
        "                super_outputs, sub_outputs = self.model.get_logits(features)\n",
        "\n",
        "                # Standard cross-entropy loss\n",
        "                ce_loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "\n",
        "                # Center loss\n",
        "                c_loss = self._center_loss(features, super_labels)\n",
        "\n",
        "                # Combined loss\n",
        "                loss = ce_loss + self.center_loss_weight * c_loss\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, sub_preds, super_unknown_probs, _ = self.openmax_model.predict(inputs, self.device)\n",
        "\n",
        "                # Track overall accuracy\n",
        "                total += super_labels.size(0)\n",
        "                super_correct += (super_preds == super_labels).sum().item()\n",
        "                sub_correct += (sub_preds == sub_labels).sum().item()\n",
        "\n",
        "                # Track novel vs known separately\n",
        "                is_novel = super_labels >= self.openmax_model.num_superclasses\n",
        "                novel_total += is_novel.sum().item()\n",
        "                known_total += (~is_novel).sum().item()\n",
        "\n",
        "                # A novel sample is correctly classified if detected as novel\n",
        "                novel_correct += (is_novel & (super_preds == self.openmax_model.num_superclasses)).sum().item()\n",
        "\n",
        "                # A known sample is correctly classified if prediction matches label\n",
        "                known_correct += ((~is_novel) & (super_preds == super_labels)).sum().item()\n",
        "\n",
        "                # Store probabilities and labels for threshold analysis\n",
        "                all_super_unknown_probs.extend(super_unknown_probs)\n",
        "                all_is_novel.extend(is_novel.cpu().numpy())\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_loss = running_loss / (i+1)\n",
        "        super_acc = 100 * super_correct / total if total > 0 else 0\n",
        "        sub_acc = 100 * sub_correct / total if total > 0 else 0\n",
        "\n",
        "        novel_acc = 100 * novel_correct / novel_total if novel_total > 0 else 0\n",
        "        known_acc = 100 * known_correct / known_total if known_total > 0 else 0\n",
        "        balanced_acc = (novel_acc + known_acc) / 2 if novel_total > 0 and known_total > 0 else 0\n",
        "\n",
        "        print(f'Validation loss: {avg_loss:.4f}')\n",
        "        print(f'Overall superclass acc: {super_acc:.2f}%, subclass acc: {sub_acc:.2f}%')\n",
        "\n",
        "        if novel_total > 0:\n",
        "            print(f'Novel acc: {novel_acc:.2f}% ({novel_correct}/{novel_total})')\n",
        "        if known_total > 0:\n",
        "            print(f'Known acc: {known_acc:.2f}% ({known_correct}/{known_total})')\n",
        "        if novel_total > 0 and known_total > 0:\n",
        "            print(f'Balanced acc: {balanced_acc:.2f}%')\n",
        "\n",
        "        # Update history\n",
        "        self.history['val_loss'].append(avg_loss)\n",
        "        self.history['super_acc'].append(super_acc)\n",
        "        self.history['sub_acc'].append(sub_acc)\n",
        "        self.history['novel_acc'].append(novel_acc)\n",
        "        self.history['known_acc'].append(known_acc)\n",
        "        self.history['balanced_acc'].append(balanced_acc)\n",
        "\n",
        "        return {\n",
        "            'loss': avg_loss,\n",
        "            'super_acc': super_acc,\n",
        "            'sub_acc': sub_acc,\n",
        "            'novel_acc': novel_acc,\n",
        "            'known_acc': known_acc,\n",
        "            'balanced_acc': balanced_acc\n",
        "        }\n",
        "\n",
        "    def _validate_without_openmax(self):\n",
        "        \"\"\"Validate during pre-training (before OpenMax is initialized)\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        correct = 0\n",
        "        sub_correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.val_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                super_labels = super_labels.to(self.device)\n",
        "                sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                super_outputs, sub_outputs = self.model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "\n",
        "                # Get predictions\n",
        "                _, super_predicted = torch.max(super_outputs, 1)\n",
        "                _, sub_predicted = torch.max(sub_outputs, 1)\n",
        "\n",
        "                # Count correct\n",
        "                total += super_labels.size(0)\n",
        "                correct += (super_predicted == super_labels).sum().item()\n",
        "                sub_correct += (sub_predicted == sub_labels).sum().item()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = 100 * correct / total if total > 0 else 0\n",
        "        sub_accuracy = 100 * sub_correct / total if total > 0 else 0\n",
        "        avg_loss = running_loss / (i+1)\n",
        "\n",
        "        print(f'Pre-training validation - Loss: {avg_loss:.4f}, Super acc: {accuracy:.2f}%, Sub acc: {sub_accuracy:.2f}%')\n",
        "\n",
        "        # Update history\n",
        "        self.history['val_loss'].append(avg_loss)\n",
        "        self.history['super_acc'].append(accuracy)\n",
        "        self.history['sub_acc'].append(sub_accuracy)\n",
        "        self.history['novel_acc'].append(0)\n",
        "        self.history['known_acc'].append(accuracy)\n",
        "        self.history['balanced_acc'].append(0)\n",
        "\n",
        "        return {\n",
        "            'loss': avg_loss,\n",
        "            'super_acc': accuracy,\n",
        "            'sub_acc': sub_accuracy,\n",
        "            'novel_acc': 0,\n",
        "            'known_acc': accuracy,\n",
        "            'balanced_acc': 0\n",
        "        }\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False, output_file='optimized_openmax_predictions.csv'):\n",
        "        \"\"\"Test the model and generate predictions with detailed analysis\"\"\"\n",
        "        if not self.test_loader:\n",
        "            raise ValueError('test_loader not specified')\n",
        "\n",
        "        if self.openmax_model is None:\n",
        "            raise ValueError('OpenMax model not initialized')\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Create dictionaries for predictions\n",
        "        test_predictions = {\n",
        "            'image': [],\n",
        "            'superclass_index': [],\n",
        "            'subclass_index': [],\n",
        "            'unknown_probability': []\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(self.test_loader):\n",
        "                inputs, img_name = data[0].to(self.device), data[1]\n",
        "\n",
        "                # OpenMax prediction\n",
        "                super_preds, sub_preds, super_unknown_probs, sub_unknown_probs = self.openmax_model.predict(inputs, self.device)\n",
        "\n",
        "                for j in range(inputs.size(0)):\n",
        "                    img = img_name[j] if isinstance(img_name, list) else img_name[0]\n",
        "\n",
        "                    test_predictions['image'].append(img)\n",
        "                    test_predictions['superclass_index'].append(super_preds[j].item())\n",
        "                    test_predictions['subclass_index'].append(sub_preds[j].item())\n",
        "                    test_predictions['unknown_probability'].append(super_unknown_probs[j])\n",
        "\n",
        "                # Print progress for large datasets\n",
        "                if (i+1) % 100 == 0:\n",
        "                    print(f\"Processed {i+1} batches...\")\n",
        "\n",
        "        # Create full DataFrame with all information\n",
        "        full_predictions_df = pd.DataFrame(data=test_predictions)\n",
        "\n",
        "        # Create simplified DataFrame for output (compatible with original code)\n",
        "        simplified_predictions = {\n",
        "            'image': test_predictions['image'],\n",
        "            'superclass_index': test_predictions['superclass_index'],\n",
        "            'subclass_index': test_predictions['subclass_index']\n",
        "        }\n",
        "        simplified_predictions_df = pd.DataFrame(data=simplified_predictions)\n",
        "\n",
        "        # Print summary of novel predictions\n",
        "        novel_super_count = sum(1 for idx in test_predictions['superclass_index'] if idx == self.openmax_model.num_superclasses)\n",
        "        novel_sub_count = sum(1 for idx in test_predictions['subclass_index'] if idx == self.openmax_model.num_subclasses)\n",
        "\n",
        "        total_count = len(test_predictions['image'])\n",
        "        novel_super_perc = 100 * novel_super_count / total_count if total_count > 0 else 0\n",
        "        novel_sub_perc = 100 * novel_sub_count / total_count if total_count > 0 else 0\n",
        "\n",
        "        print(f'Test set predictions:')\n",
        "        print(f'Images predicted as novel superclass: {novel_super_count} ({novel_super_perc:.2f}%)')\n",
        "        print(f'Images predicted as novel subclass: {novel_sub_count} ({novel_sub_perc:.2f}%)')\n",
        "\n",
        "        # Print unknown probability statistics\n",
        "        unknown_probs = np.array(test_predictions['unknown_probability'])\n",
        "        print(f'Unknown probability statistics:')\n",
        "        print(f'  Mean: {np.mean(unknown_probs):.4f}')\n",
        "        print(f'  Std: {np.std(unknown_probs):.4f}')\n",
        "        print(f'  Min: {np.min(unknown_probs):.4f}')\n",
        "        print(f'  Max: {np.max(unknown_probs):.4f}')\n",
        "\n",
        "        # Print distribution of unknown probabilities\n",
        "        print(f'Unknown probability distribution:')\n",
        "        bins = [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
        "        for i in range(len(bins)-1):\n",
        "            count = sum(1 for p in unknown_probs if bins[i] <= p < bins[i+1])\n",
        "            print(f'  {bins[i]:.1f}-{bins[i+1]:.1f}: {count} ({100 * count / len(unknown_probs):.2f}%)')\n",
        "\n",
        "        if save_to_csv:\n",
        "            # Save in the same format as the original code\n",
        "            simplified_predictions_df.to_csv(output_file, index=False)\n",
        "            print(f\"Predictions saved to '{output_file}'\")\n",
        "\n",
        "            # Save detailed version with probabilities\n",
        "            full_predictions_df.to_csv(f'detailed_{output_file}', index=False)\n",
        "            print(f\"Detailed predictions saved to 'detailed_{output_file}'\")\n",
        "\n",
        "        if return_predictions:\n",
        "            # Return the predictions for further analysis\n",
        "            return full_predictions_df, unknown_probs\n",
        "\n",
        "\n",
        "# Complete integrated solution for training and optimizing OpenMax\n",
        "def optimize_openmax_model(train_loader, val_loader, test_loader=None, device='cuda', epochs=15):\n",
        "    \"\"\"\n",
        "    End-to-end workflow to train and optimize OpenMax model for both seen and unseen class accuracy\n",
        "\n",
        "    Args:\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        test_loader: Test data loader (optional)\n",
        "        device: Device to use (cuda or cpu)\n",
        "        epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (model, openmax_model, trainer)\n",
        "    \"\"\"\n",
        "    print(\"=== Starting Optimized OpenMax Training ===\")\n",
        "\n",
        "    # Phase 1: Initialize model and pre-train\n",
        "    print(\"\\nPhase 1: Initializing and pre-training classification model\")\n",
        "    model = OptimizedCNN(\n",
        "        input_size=64,\n",
        "        num_superclasses=4,\n",
        "        num_subclasses=88\n",
        "    ).to(device)\n",
        "\n",
        "    # Check if we need to handle class imbalance\n",
        "    class_counts = [0, 0, 0, 0]  # Count samples per superclass\n",
        "    for _, super_label, _, _, _ in train_loader.dataset:\n",
        "        if hasattr(super_label, 'item'):\n",
        "            super_label = super_label.item()\n",
        "        if super_label < len(class_counts):\n",
        "            class_counts[super_label] += 1\n",
        "\n",
        "    print(f\"Class distribution: {class_counts}\")\n",
        "\n",
        "    # Create weighted loss if there's imbalance\n",
        "    non_zero_counts = [c for c in class_counts if c > 0]\n",
        "    if len(non_zero_counts) > 1 and max(non_zero_counts) / min(non_zero_counts) > 1.5:\n",
        "        total = sum(class_counts)\n",
        "        weights = [total / (c * len(non_zero_counts)) if c > 0 else 0.0 for c in class_counts]\n",
        "        class_weights = torch.tensor(weights, device=device)\n",
        "        print(f\"Using class weights: {weights}\")\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        # Use label smoothing for regularization\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "\n",
        "    # Use AdamW optimizer with weight decay\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=0.001,\n",
        "        weight_decay=0.0001\n",
        "    )\n",
        "\n",
        "    # Initialize trainer with very small center loss weight\n",
        "    trainer = OptimizedOpenMaxTrainer(\n",
        "        model=model,\n",
        "        openmax_model=None,  # Will be created after pre-training\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        center_loss_weight=0.0005  # Critical fix: Reduce from 0.05 to 0.0005\n",
        "    )\n",
        "\n",
        "    # Pre-train for a few epochs\n",
        "    print(\"\\nPre-training classification model...\")\n",
        "    for epoch in range(5):\n",
        "        trainer.train_epoch()\n",
        "        metrics = trainer._validate_without_openmax()\n",
        "        print(f\"Pre-training Epoch {epoch+1}/5: Accuracy: {metrics['super_acc']:.2f}%\")\n",
        "\n",
        "    # Phase 2: Initialize and fit OpenMax\n",
        "    print(\"\\nPhase 2: Initializing OpenMax model\")\n",
        "    openmax_model = OptimizedOpenMaxModel(\n",
        "        model,\n",
        "        num_superclasses=3,\n",
        "        num_subclasses=87,\n",
        "        tailsize=40,                  # Larger tailsize for better statistics\n",
        "        alpha=4,                      # Fewer top activations to consider\n",
        "        threshold=0.6,                # Initial threshold (will be calibrated)\n",
        "        distance_multiplier=1.5       # Scale distances for better sensitivity\n",
        "    )\n",
        "\n",
        "    # Fit OpenMax parameters\n",
        "    openmax_model.fit(train_loader, device)\n",
        "\n",
        "    # Update trainer with OpenMax model\n",
        "    trainer.openmax_model = openmax_model\n",
        "\n",
        "    # Phase 3: Calibrate threshold for better novelty detection\n",
        "    print(\"\\nPhase 3: Calibrating novelty detection threshold\")\n",
        "    best_threshold, _ = openmax_model.calibrate_threshold(val_loader, device)\n",
        "\n",
        "    # Phase 4: Continue training with OpenMax\n",
        "    print(\"\\nPhase 4: Continuing training with OpenMax and optimized parameters\")\n",
        "    best_metrics = {'balanced_acc': 0}\n",
        "    best_state_dict = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        trainer.train_epoch()\n",
        "        metrics = trainer.validate_epoch()\n",
        "\n",
        "        # Save best model based on balanced accuracy\n",
        "        if metrics['balanced_acc'] > best_metrics['balanced_acc']:\n",
        "            best_metrics = metrics.copy()\n",
        "            best_state_dict = {\n",
        "                'model': model.state_dict().copy(),\n",
        "                'threshold': openmax_model.threshold\n",
        "            }\n",
        "            print(f\"New best model! Balanced accuracy: {best_metrics['balanced_acc']:.4f}\")\n",
        "\n",
        "        # Re-calibrate every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            openmax_model.calibrate_threshold(val_loader, device)\n",
        "\n",
        "    # Load best model\n",
        "    if best_state_dict is not None:\n",
        "        model.load_state_dict(best_state_dict['model'])\n",
        "        openmax_model.threshold = best_state_dict['threshold']\n",
        "        print(f\"Loaded best model with balanced accuracy: {best_metrics['balanced_acc']:.4f}\")\n",
        "        print(f\"Using threshold: {openmax_model.threshold:.4f}\")\n",
        "\n",
        "        # Re-fit OpenMax with best model\n",
        "        openmax_model.fit(train_loader, device)\n",
        "\n",
        "    return model, openmax_model, trainer\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# model, openmax_model, trainer = optimize_openmax_model(train_loader, val_loader, test_loader)\n",
        "# predictions, unknown_probs = trainer.test(save_to_csv=True)"
      ],
      "metadata": {
        "id": "JK0ki7M9zioP"
      },
      "id": "JK0ki7M9zioP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "model, openmax_model, trainer = optimize_openmax_model(train_loader, val_loader, test_loader)\n",
        "predictions, unknown_probs = trainer.test(save_to_csv=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4A2Idp2D47in",
        "outputId": "11af4a76-f96b-450d-a40e-ce95842a3de8"
      },
      "id": "4A2Idp2D47in",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Starting Optimized OpenMax Training ===\n",
            "\n",
            "Phase 1: Initializing and pre-training classification model\n",
            "Class distribution: [1657, 1878, 2125, 0]\n",
            "\n",
            "Pre-training classification model...\n",
            "Training loss: 3.7936 (CE: 3.7591, Center: 0.0345)\n",
            "Pre-training validation - Loss: 3.0374, Super acc: 96.82%, Sub acc: 38.54%\n",
            "Pre-training Epoch 1/5: Accuracy: 96.82%\n",
            "Training loss: 2.5728 (CE: 2.5312, Center: 0.0416)\n",
            "Pre-training validation - Loss: 2.3244, Super acc: 96.97%, Sub acc: 64.33%\n",
            "Pre-training Epoch 2/5: Accuracy: 96.97%\n",
            "Training loss: 2.1408 (CE: 2.0949, Center: 0.0459)\n",
            "Pre-training validation - Loss: 2.0217, Super acc: 98.73%, Sub acc: 73.25%\n",
            "Pre-training Epoch 3/5: Accuracy: 98.73%\n",
            "Training loss: 1.8796 (CE: 1.8317, Center: 0.0479)\n",
            "Pre-training validation - Loss: 1.9338, Super acc: 97.93%, Sub acc: 77.07%\n",
            "Pre-training Epoch 4/5: Accuracy: 97.93%\n",
            "Training loss: 1.7215 (CE: 1.6727, Center: 0.0488)\n",
            "Pre-training validation - Loss: 1.8164, Super acc: 98.57%, Sub acc: 79.46%\n",
            "Pre-training Epoch 5/5: Accuracy: 98.57%\n",
            "\n",
            "Phase 2: Initializing OpenMax model\n",
            "\n",
            "Fitting OpenMax parameters with distance_multiplier = 1.5\n",
            "Collecting class activations...\n",
            "Class 0 MAV stats: mean=-0.0795, std=1.6610\n",
            "Class 1 MAV stats: mean=-0.1239, std=1.5876\n",
            "Class 2 MAV stats: mean=-0.1769, std=1.4181\n",
            "Computing distances for Weibull fitting...\n",
            "Super distance stats: mean=0.2045, std=0.1782\n",
            "Sub distance stats: mean=0.0949, std=0.0660\n",
            "Fitting Weibull distributions...\n",
            "Fitted Weibull for superclass 0: shape=4.7755, scale=0.7602\n",
            "Fitted Weibull for superclass 1: shape=6.8909, scale=0.8532\n",
            "Fitted Weibull for superclass 2: shape=3.1401, scale=0.9563\n",
            "\n",
            "Phase 3: Calibrating novelty detection threshold\n",
            "\n",
            "Calibrating novelty detection threshold...\n",
            "Threshold 0.10: Known=0.0605, Novel=0.0000, Balanced=0.0303\n",
            "Threshold 0.15: Known=0.2006, Novel=0.0000, Balanced=0.1003\n",
            "Threshold 0.20: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.25: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.30: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.35: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.40: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.45: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.50: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.55: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.60: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.65: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.70: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.75: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.80: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.85: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.90: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Updated threshold from 0.60 to 0.20, Balanced accuracy: 0.5000\n",
            "\n",
            "Phase 4: Continuing training with OpenMax and optimized parameters\n",
            "\n",
            "Epoch 1/15\n",
            "Training loss: 1.5911 (CE: 1.5421, Center: 0.0490)\n",
            "Validation loss: 1.7029\n",
            "Overall superclass acc: 94.27%, subclass acc: 82.01%\n",
            "Known acc: 94.27% (592/628)\n",
            "\n",
            "Epoch 2/15\n",
            "Training loss: 1.4992 (CE: 1.4500, Center: 0.0491)\n",
            "Validation loss: 1.6318\n",
            "Overall superclass acc: 95.22%, subclass acc: 85.83%\n",
            "Known acc: 95.22% (598/628)\n",
            "\n",
            "Epoch 3/15\n",
            "Training loss: 1.4311 (CE: 1.3825, Center: 0.0486)\n",
            "Validation loss: 1.5946\n",
            "Overall superclass acc: 94.75%, subclass acc: 87.90%\n",
            "Known acc: 94.75% (595/628)\n",
            "\n",
            "Epoch 4/15\n",
            "Training loss: 1.3943 (CE: 1.3459, Center: 0.0484)\n",
            "Validation loss: 1.5652\n",
            "Overall superclass acc: 96.50%, subclass acc: 88.85%\n",
            "Known acc: 96.50% (606/628)\n",
            "\n",
            "Epoch 5/15\n",
            "Training loss: 1.3797 (CE: 1.3314, Center: 0.0483)\n",
            "Validation loss: 1.5546\n",
            "Overall superclass acc: 96.66%, subclass acc: 88.85%\n",
            "Known acc: 96.66% (607/628)\n",
            "\n",
            "Calibrating novelty detection threshold...\n",
            "Threshold 0.10: Known=0.0191, Novel=0.0000, Balanced=0.0096\n",
            "Threshold 0.15: Known=0.1465, Novel=0.0000, Balanced=0.0732\n",
            "Threshold 0.20: Known=0.9904, Novel=0.0000, Balanced=0.4952\n",
            "Threshold 0.25: Known=0.9984, Novel=0.0000, Balanced=0.4992\n",
            "Threshold 0.30: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.35: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.40: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.45: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.50: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.55: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.60: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.65: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.70: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.75: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.80: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.85: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.90: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Updated threshold from 0.20 to 0.30, Balanced accuracy: 0.5000\n",
            "\n",
            "Epoch 6/15\n",
            "Training loss: 1.3675 (CE: 1.3191, Center: 0.0484)\n",
            "Validation loss: 1.5570\n",
            "Overall superclass acc: 96.50%, subclass acc: 89.01%\n",
            "Known acc: 96.50% (606/628)\n",
            "\n",
            "Epoch 7/15\n",
            "Training loss: 1.3697 (CE: 1.3215, Center: 0.0482)\n",
            "Validation loss: 1.5534\n",
            "Overall superclass acc: 96.82%, subclass acc: 89.81%\n",
            "Known acc: 96.82% (608/628)\n",
            "\n",
            "Epoch 8/15\n",
            "Training loss: 1.3705 (CE: 1.3225, Center: 0.0480)\n",
            "Validation loss: 1.5502\n",
            "Overall superclass acc: 96.97%, subclass acc: 89.65%\n",
            "Known acc: 96.97% (609/628)\n",
            "\n",
            "Epoch 9/15\n",
            "Training loss: 1.3743 (CE: 1.3268, Center: 0.0475)\n",
            "Validation loss: 1.5636\n",
            "Overall superclass acc: 96.66%, subclass acc: 89.97%\n",
            "Known acc: 96.66% (607/628)\n",
            "\n",
            "Epoch 10/15\n",
            "Training loss: 1.3757 (CE: 1.3289, Center: 0.0468)\n",
            "Validation loss: 1.5835\n",
            "Overall superclass acc: 95.54%, subclass acc: 86.94%\n",
            "Known acc: 95.54% (600/628)\n",
            "\n",
            "Calibrating novelty detection threshold...\n",
            "Threshold 0.10: Known=0.0207, Novel=0.0000, Balanced=0.0104\n",
            "Threshold 0.15: Known=0.1051, Novel=0.0000, Balanced=0.0525\n",
            "Threshold 0.20: Known=0.9984, Novel=0.0000, Balanced=0.4992\n",
            "Threshold 0.25: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.30: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.35: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.40: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.45: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.50: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.55: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.60: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.65: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.70: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.75: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.80: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.85: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.90: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Updated threshold from 0.30 to 0.25, Balanced accuracy: 0.5000\n",
            "\n",
            "Epoch 11/15\n",
            "Training loss: 1.3837 (CE: 1.3376, Center: 0.0460)\n",
            "Validation loss: 1.6350\n",
            "Overall superclass acc: 92.83%, subclass acc: 85.99%\n",
            "Known acc: 92.83% (583/628)\n",
            "\n",
            "Epoch 12/15\n",
            "Training loss: 1.4051 (CE: 1.3598, Center: 0.0452)\n",
            "Validation loss: 1.6983\n",
            "Overall superclass acc: 96.18%, subclass acc: 86.46%\n",
            "Known acc: 96.18% (604/628)\n",
            "\n",
            "Epoch 13/15\n",
            "Training loss: 1.3980 (CE: 1.3537, Center: 0.0443)\n",
            "Validation loss: 1.7061\n",
            "Overall superclass acc: 94.59%, subclass acc: 83.12%\n",
            "Known acc: 94.59% (594/628)\n",
            "\n",
            "Epoch 14/15\n",
            "Training loss: 1.4112 (CE: 1.3683, Center: 0.0429)\n",
            "Validation loss: 1.7186\n",
            "Overall superclass acc: 95.54%, subclass acc: 83.92%\n",
            "Known acc: 95.54% (600/628)\n",
            "\n",
            "Epoch 15/15\n",
            "Training loss: 1.3868 (CE: 1.3452, Center: 0.0416)\n",
            "Validation loss: 1.6773\n",
            "Overall superclass acc: 96.02%, subclass acc: 85.03%\n",
            "Known acc: 96.02% (603/628)\n",
            "\n",
            "Calibrating novelty detection threshold...\n",
            "Threshold 0.10: Known=0.0398, Novel=0.0000, Balanced=0.0199\n",
            "Threshold 0.15: Known=0.1911, Novel=0.0000, Balanced=0.0955\n",
            "Threshold 0.20: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.25: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.30: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.35: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.40: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.45: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.50: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.55: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.60: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.65: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.70: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.75: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.80: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.85: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Threshold 0.90: Known=1.0000, Novel=0.0000, Balanced=0.5000\n",
            "Updated threshold from 0.25 to 0.20, Balanced accuracy: 0.5000\n",
            "Processed 100 batches...\n",
            "Processed 200 batches...\n",
            "Processed 300 batches...\n",
            "Processed 400 batches...\n",
            "Processed 500 batches...\n",
            "Processed 600 batches...\n",
            "Processed 700 batches...\n",
            "Processed 800 batches...\n",
            "Processed 900 batches...\n",
            "Processed 1000 batches...\n",
            "Processed 1100 batches...\n",
            "Processed 1200 batches...\n",
            "Processed 1300 batches...\n",
            "Processed 1400 batches...\n",
            "Processed 1500 batches...\n",
            "Processed 1600 batches...\n",
            "Processed 1700 batches...\n",
            "Processed 1800 batches...\n",
            "Processed 1900 batches...\n",
            "Processed 2000 batches...\n",
            "Processed 2100 batches...\n",
            "Processed 2200 batches...\n",
            "Processed 2300 batches...\n",
            "Processed 2400 batches...\n",
            "Processed 2500 batches...\n",
            "Processed 2600 batches...\n",
            "Processed 2700 batches...\n",
            "Processed 2800 batches...\n",
            "Processed 2900 batches...\n",
            "Processed 3000 batches...\n",
            "Processed 3100 batches...\n",
            "Processed 3200 batches...\n",
            "Processed 3300 batches...\n",
            "Processed 3400 batches...\n",
            "Processed 3500 batches...\n",
            "Processed 3600 batches...\n",
            "Processed 3700 batches...\n",
            "Processed 3800 batches...\n",
            "Processed 3900 batches...\n",
            "Processed 4000 batches...\n",
            "Processed 4100 batches...\n",
            "Processed 4200 batches...\n",
            "Processed 4300 batches...\n",
            "Processed 4400 batches...\n",
            "Processed 4500 batches...\n",
            "Processed 4600 batches...\n",
            "Processed 4700 batches...\n",
            "Processed 4800 batches...\n",
            "Processed 4900 batches...\n",
            "Processed 5000 batches...\n",
            "Processed 5100 batches...\n",
            "Processed 5200 batches...\n",
            "Processed 5300 batches...\n",
            "Processed 5400 batches...\n",
            "Processed 5500 batches...\n",
            "Processed 5600 batches...\n",
            "Processed 5700 batches...\n",
            "Processed 5800 batches...\n",
            "Processed 5900 batches...\n",
            "Processed 6000 batches...\n",
            "Processed 6100 batches...\n",
            "Processed 6200 batches...\n",
            "Processed 6300 batches...\n",
            "Processed 6400 batches...\n",
            "Processed 6500 batches...\n",
            "Processed 6600 batches...\n",
            "Processed 6700 batches...\n",
            "Processed 6800 batches...\n",
            "Processed 6900 batches...\n",
            "Processed 7000 batches...\n",
            "Processed 7100 batches...\n",
            "Processed 7200 batches...\n",
            "Processed 7300 batches...\n",
            "Processed 7400 batches...\n",
            "Processed 7500 batches...\n",
            "Processed 7600 batches...\n",
            "Processed 7700 batches...\n",
            "Processed 7800 batches...\n",
            "Processed 7900 batches...\n",
            "Processed 8000 batches...\n",
            "Processed 8100 batches...\n",
            "Processed 8200 batches...\n",
            "Processed 8300 batches...\n",
            "Processed 8400 batches...\n",
            "Processed 8500 batches...\n",
            "Processed 8600 batches...\n",
            "Processed 8700 batches...\n",
            "Processed 8800 batches...\n",
            "Processed 8900 batches...\n",
            "Processed 9000 batches...\n",
            "Processed 9100 batches...\n",
            "Processed 9200 batches...\n",
            "Processed 9300 batches...\n",
            "Processed 9400 batches...\n",
            "Processed 9500 batches...\n",
            "Processed 9600 batches...\n",
            "Processed 9700 batches...\n",
            "Processed 9800 batches...\n",
            "Processed 9900 batches...\n",
            "Processed 10000 batches...\n",
            "Processed 10100 batches...\n",
            "Processed 10200 batches...\n",
            "Processed 10300 batches...\n",
            "Processed 10400 batches...\n",
            "Processed 10500 batches...\n",
            "Processed 10600 batches...\n",
            "Processed 10700 batches...\n",
            "Processed 10800 batches...\n",
            "Processed 10900 batches...\n",
            "Processed 11000 batches...\n",
            "Processed 11100 batches...\n",
            "Test set predictions:\n",
            "Images predicted as novel superclass: 225 (2.01%)\n",
            "Images predicted as novel subclass: 0 (0.00%)\n",
            "Unknown probability statistics:\n",
            "  Mean: 0.1470\n",
            "  Std: 0.0339\n",
            "  Min: 0.0067\n",
            "  Max: 0.3695\n",
            "Unknown probability distribution:\n",
            "  0.0-0.2: 10955 (97.99%)\n",
            "  0.2-0.4: 225 (2.01%)\n",
            "  0.4-0.5: 0 (0.00%)\n",
            "  0.5-0.6: 0 (0.00%)\n",
            "  0.6-0.8: 0 (0.00%)\n",
            "  0.8-1.0: 0 (0.00%)\n",
            "Predictions saved to 'optimized_openmax_predictions.csv'\n",
            "Detailed predictions saved to 'detailed_optimized_openmax_predictions.csv'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-65eebb7fa65e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Default title text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenmax_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_openmax_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_to_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the CLIP package\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u01I0aeneNxY",
        "outputId": "861cbcf9-3177-4953-9de0-0791e1aa6bb6"
      },
      "id": "u01I0aeneNxY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-cs832wq9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-cs832wq9\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Novelty Detection with Enhanced Threshold Selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size=64, num_superclasses=4, num_subclasses=88):\n",
        "        super().__init__()\n",
        "\n",
        "        # Calculate feature size based on input size and pooling operations\n",
        "        # 3 max pooling layers with stride 2 each reduces size by factor of 2^3\n",
        "        self.feature_size = input_size // (2**3)\n",
        "\n",
        "        # First convolutional block\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Second convolutional block\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 64, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Third convolutional block\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, 3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        self.fc1 = nn.Linear(self.feature_size * self.feature_size * 128, 256)\n",
        "        self.dropout1 = nn.Dropout(0.2)  # Add dropout for regularization\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.2)  # Add dropout for regularization\n",
        "\n",
        "        # Classification heads\n",
        "        self.fc3a = nn.Linear(128, num_superclasses)  # 4 superclasses: bird, dog, reptile, novel\n",
        "        self.fc3b = nn.Linear(128, num_subclasses)    # All subclasses + novel\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network\"\"\"\n",
        "        # Pass through convolutional blocks\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        # Flatten for fully connected layers\n",
        "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)  # Apply dropout after activation\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)  # Apply dropout after activation\n",
        "\n",
        "        # Classification heads\n",
        "        super_out = self.fc3a(x)\n",
        "        sub_out = self.fc3b(x)\n",
        "\n",
        "        return super_out, sub_out\n",
        "\n",
        "    def get_features(self, x):\n",
        "        \"\"\"Extract features before the final classification layer\"\"\"\n",
        "        # Pass through convolutional blocks\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        # Flatten and pass through FC layers (without final classification)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)  # Apply dropout\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)  # Apply dropout\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class NoveltyDetectionTrainer:\n",
        "    def __init__(self, full_dataset, image_preprocessing, device='cuda', batch_size=64,\n",
        "                 min_known_acc=95, min_novel_acc=20):\n",
        "        self.full_dataset = full_dataset\n",
        "        self.image_preprocessing = image_preprocessing\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Store energy normalization parameters\n",
        "        self.energy_mean = 0\n",
        "        self.energy_std = 1\n",
        "\n",
        "        # Required accuracy thresholds\n",
        "        self.min_known_acc = min_known_acc\n",
        "        self.min_novel_acc = min_novel_acc\n",
        "\n",
        "        # Fixed energy threshold (based on cross-validation) for high known accuracy (≥95%)\n",
        "        # and reasonable novel detection (≥20%)\n",
        "        self.energy_threshold = 1.9  # Fixed threshold from cross-validation\n",
        "\n",
        "        # Get all unique superclass indices\n",
        "        self.superclass_indices = set()\n",
        "        for i in range(len(full_dataset)):\n",
        "            _, super_idx, _, _, _ = full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "            self.superclass_indices.add(super_idx)\n",
        "\n",
        "        self.superclass_indices = sorted(list(self.superclass_indices))\n",
        "        print(f\"Found superclasses with indices: {self.superclass_indices}\")\n",
        "\n",
        "    def cross_validate_novelty_detection(self, epochs=5, confidence_threshold=0.0):\n",
        "        \"\"\"Run cross-validation for novelty detection\"\"\"\n",
        "        results = []\n",
        "\n",
        "        # For each superclass, treat it as novel and others as known\n",
        "        for fold, novel_idx in enumerate(self.superclass_indices):\n",
        "            print(f\"\\n=== Fold {fold+1}/{len(self.superclass_indices)}: Treating superclass {novel_idx} as novel ===\")\n",
        "\n",
        "            # Create data splits\n",
        "            known_indices, novel_indices = self._split_by_superclass(novel_idx)\n",
        "\n",
        "            # Further split known indices into train/validation\n",
        "            np.random.shuffle(known_indices)\n",
        "            train_size = int(0.9 * len(known_indices))\n",
        "            train_indices = known_indices[:train_size]\n",
        "            val_known_indices = known_indices[train_size:]\n",
        "\n",
        "            # Create datasets\n",
        "            train_dataset = Subset(self.full_dataset, train_indices)\n",
        "            val_known_dataset = Subset(self.full_dataset, val_known_indices)\n",
        "            val_novel_dataset = Subset(self.full_dataset, novel_indices)\n",
        "\n",
        "            # Create dataloaders\n",
        "            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "            val_known_loader = DataLoader(val_known_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "            val_novel_loader = DataLoader(val_novel_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "            # Initialize model, loss, optimizer\n",
        "            model = CNN(input_size=64, num_superclasses=len(self.superclass_indices)+1).to(self.device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "            # Train the model\n",
        "            self._train_model(model, criterion, optimizer, train_loader, epochs)\n",
        "\n",
        "            # Calibrate energy statistics on training data\n",
        "            self._calibrate_energy_stats(model, train_loader)\n",
        "\n",
        "            # Evaluate novelty detection\n",
        "            metrics = self._evaluate_novelty_detection(model, val_known_loader, val_novel_loader, confidence_threshold)\n",
        "            results.append(metrics)\n",
        "\n",
        "            print(f\"Fold {fold+1} results:\")\n",
        "            for key, value in metrics.items():\n",
        "                print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "        # Calculate average results across folds\n",
        "        avg_results = {}\n",
        "        for key in results[0].keys():\n",
        "            avg_results[key] = sum(r[key] for r in results) / len(results)\n",
        "\n",
        "        print(\"\\n=== Cross-Validation Summary ===\")\n",
        "        for key, value in avg_results.items():\n",
        "            print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "        return avg_results, results\n",
        "\n",
        "    def find_optimal_threshold(self, fold_index=0, threshold_range=np.arange(-3.0, 3.0, 0.1)):\n",
        "        \"\"\"Find the optimal normalized energy threshold for a given fold\"\"\"\n",
        "        novel_idx = self.superclass_indices[fold_index]\n",
        "        print(f\"\\n=== Finding optimal threshold for fold {fold_index+1}: Superclass {novel_idx} as novel ===\")\n",
        "\n",
        "        # Create data splits\n",
        "        known_indices, novel_indices = self._split_by_superclass(novel_idx)\n",
        "\n",
        "        # Further split known indices into train/validation\n",
        "        np.random.shuffle(known_indices)\n",
        "        train_size = int(0.9 * len(known_indices))\n",
        "        train_indices = known_indices[:train_size]\n",
        "        val_known_indices = known_indices[train_size:]\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = Subset(self.full_dataset, train_indices)\n",
        "        val_known_dataset = Subset(self.full_dataset, val_known_indices)\n",
        "        val_novel_dataset = Subset(self.full_dataset, novel_indices)\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_known_loader = DataLoader(val_known_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        val_novel_loader = DataLoader(val_novel_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        # Initialize model, loss, optimizer\n",
        "        model = CNN(input_size=64, num_superclasses=len(self.superclass_indices)+1).to(self.device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "        # Train the model\n",
        "        self._train_model(model, criterion, optimizer, train_loader, epochs=5)\n",
        "\n",
        "        # Calibrate energy statistics on training data\n",
        "        self._calibrate_energy_stats(model, train_loader)\n",
        "\n",
        "        # Collect all normalized energy scores\n",
        "        known_energies, novel_energies = self._collect_energies(model, val_known_loader, val_novel_loader)\n",
        "\n",
        "        # Evaluate different thresholds\n",
        "        results = []\n",
        "        for threshold in threshold_range:\n",
        "            # For known classes, prediction is \"known\" if energy <= threshold\n",
        "            known_correct = sum(1 for e in known_energies if e <= threshold)\n",
        "            known_accuracy = known_correct / len(known_energies) if known_energies else 0\n",
        "\n",
        "            # For novel classes, prediction is \"novel\" if energy > threshold\n",
        "            novel_correct = sum(1 for e in novel_energies if e > threshold)\n",
        "            novel_accuracy = novel_correct / len(novel_energies) if novel_energies else 0\n",
        "\n",
        "            # Balanced accuracy (average of known and novel accuracies)\n",
        "            balanced_accuracy = (known_accuracy + novel_accuracy) / 2\n",
        "\n",
        "            results.append({\n",
        "                'threshold': threshold,\n",
        "                'known_accuracy': known_accuracy * 100,  # Convert to percentage\n",
        "                'novel_accuracy': novel_accuracy * 100,  # Convert to percentage\n",
        "                'balanced_accuracy': balanced_accuracy * 100  # Convert to percentage\n",
        "            })\n",
        "\n",
        "            print(f\"Threshold {threshold:.2f}: Known Acc={known_accuracy:.4f}, Novel Acc={novel_accuracy:.4f}, Balanced Acc={balanced_accuracy:.4f}\")\n",
        "\n",
        "        # Find thresholds that meet our criteria\n",
        "        valid_thresholds = []\n",
        "        for result in results:\n",
        "            # Check if thresholds meet minimum requirements\n",
        "            if result['known_accuracy'] >= self.min_known_acc and result['novel_accuracy'] >= self.min_novel_acc:\n",
        "                valid_thresholds.append(result)\n",
        "\n",
        "        if valid_thresholds:\n",
        "            # Choose the threshold with best balanced accuracy from valid ones\n",
        "            best_result = max(valid_thresholds, key=lambda x: x['balanced_accuracy'])\n",
        "            print(f\"\\nFound threshold meeting criteria (known ≥{self.min_known_acc}%, novel ≥{self.min_novel_acc}%):\")\n",
        "        else:\n",
        "            # No threshold meets criteria, use threshold 1.9 from cross-validation\n",
        "            print(f\"\\nNo threshold meets both criteria (known ≥{self.min_known_acc}%, novel ≥{self.min_novel_acc}%)\")\n",
        "            print(\"Using pre-selected threshold of 1.9 from cross-validation...\")\n",
        "\n",
        "            # Find result closest to threshold 1.9\n",
        "            best_result = min(results, key=lambda x: abs(x['threshold'] - 1.9))\n",
        "\n",
        "        print(f\"Best threshold: {best_result['threshold']:.2f}\")\n",
        "        print(f\"Known accuracy: {best_result['known_accuracy']:.4f}\")\n",
        "        print(f\"Novel accuracy: {best_result['novel_accuracy']:.4f}\")\n",
        "        print(f\"Balanced accuracy: {best_result['balanced_accuracy']:.4f}\")\n",
        "\n",
        "        return best_result['threshold'], results\n",
        "\n",
        "    def _calibrate_energy_stats(self, model, loader):\n",
        "      \"\"\"Calculate energy statistics on a dataset for normalization\"\"\"\n",
        "      model.eval()\n",
        "      all_energies = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for data in loader:\n",
        "              inputs = data[0].to(self.device)\n",
        "\n",
        "              # Get model outputs\n",
        "              super_outputs, _ = model(inputs)\n",
        "\n",
        "              # Calculate raw energy\n",
        "              energies = -torch.logsumexp(super_outputs, dim=1)\n",
        "              all_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "      # Compute mean and standard deviation\n",
        "      all_energies = np.array(all_energies)\n",
        "      self.energy_mean = float(np.mean(all_energies))\n",
        "      self.energy_std = float(np.std(all_energies) + 1e-6)  # Add epsilon to avoid division by zero\n",
        "\n",
        "      print(f\"Calibrated energy statistics: mean={self.energy_mean:.4f}, std={self.energy_std:.4f}\")\n",
        "\n",
        "    def _compute_normalized_energy(self, logits):\n",
        "      \"\"\"Compute normalized energy scores\"\"\"\n",
        "      # Calculate raw energy\n",
        "      raw_energy = -torch.logsumexp(logits, dim=1)\n",
        "\n",
        "      # Normalize using stored statistics\n",
        "      normalized_energy = (raw_energy - self.energy_mean) / self.energy_std\n",
        "\n",
        "      return normalized_energy\n",
        "\n",
        "    def _split_by_superclass(self, novel_superclass_idx):\n",
        "        \"\"\"Split dataset indices into known and novel based on superclass\"\"\"\n",
        "        known_indices = []\n",
        "        novel_indices = []\n",
        "\n",
        "        for i in range(len(self.full_dataset)):\n",
        "            _, super_idx, _, _, _ = self.full_dataset[i]\n",
        "            if hasattr(super_idx, 'item'):\n",
        "                super_idx = super_idx.item()\n",
        "\n",
        "            if super_idx == novel_superclass_idx:\n",
        "                novel_indices.append(i)\n",
        "            else:\n",
        "                known_indices.append(i)\n",
        "\n",
        "        return known_indices, novel_indices\n",
        "\n",
        "    def _train_model(self, model, criterion, optimizer, train_loader, epochs):\n",
        "        \"\"\"Train the model on known classes\"\"\"\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(train_loader):\n",
        "                inputs, super_labels, _, sub_labels, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                super_labels = super_labels.to(self.device)\n",
        "                sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                super_outputs, sub_outputs = model(inputs)\n",
        "                loss = criterion(super_outputs, super_labels) + criterion(sub_outputs, sub_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "    def _evaluate_novelty_detection(self, model, known_loader, novel_loader, threshold):\n",
        "      \"\"\"Evaluate novelty detection performance using balanced ensemble approach.\"\"\"\n",
        "      model.eval()\n",
        "\n",
        "      # First calibrate energy statistics on known data\n",
        "      self._calibrate_energy_stats(model, known_loader)\n",
        "\n",
        "      def eval_loader(loader, is_novel):\n",
        "          super_correct, sub_correct = 0, 0\n",
        "          super_total, sub_total = 0, 0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for data in loader:\n",
        "                  inputs, _, _, _, _ = data\n",
        "                  inputs = inputs.to(self.device)\n",
        "\n",
        "                  super_outputs, sub_outputs = model(inputs)\n",
        "\n",
        "                  # --- Energy-based detection with FIXED threshold ---\n",
        "                  super_energies = self._compute_normalized_energy(super_outputs)\n",
        "                  # Use the fixed threshold of 1.9 to maintain high known accuracy\n",
        "                  energy_novel = super_energies > self.energy_threshold  # self.energy_threshold is 1.9\n",
        "\n",
        "                  # --- Confidence-based detection ---\n",
        "                  super_probs = F.softmax(super_outputs, dim=1)\n",
        "                  super_confidences, _ = torch.max(super_probs, dim=1)\n",
        "                  confidence_novel = super_confidences < 0.7  # Adjust this threshold\n",
        "\n",
        "                  # --- Balanced approach - weight both signals ---\n",
        "                  # Energy gets 60% weight, confidence gets 40% weight\n",
        "                  energy_weight = 0.6\n",
        "                  confidence_weight = 0.4\n",
        "\n",
        "                  # Calculate weighted score (1 = novel, 0 = known)\n",
        "                  novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "\n",
        "                  # Consider novel if score > 0.5 (adjustable threshold)\n",
        "                  is_novel_super = novelty_score > 0.5\n",
        "\n",
        "                  # --- Subclass detection remains the same ---\n",
        "                  sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "                  sub_confidences, _ = torch.max(sub_probs, dim=1)\n",
        "                  is_novel_sub = sub_confidences < 0.5\n",
        "\n",
        "                  # Count correct predictions\n",
        "                  if is_novel:\n",
        "                      super_correct += is_novel_super.sum().item()\n",
        "                      sub_correct += is_novel_sub.sum().item()\n",
        "                  else:\n",
        "                      super_correct += (~is_novel_super).sum().item()\n",
        "                      sub_correct += (~is_novel_sub).sum().item()\n",
        "\n",
        "                  super_total += inputs.size(0)\n",
        "                  sub_total += inputs.size(0)\n",
        "\n",
        "          return (\n",
        "              super_correct / super_total if super_total else 0,\n",
        "              sub_correct / sub_total if sub_total else 0\n",
        "          )\n",
        "\n",
        "      # Evaluate known and novel sets\n",
        "      known_super_acc, known_sub_acc = eval_loader(known_loader, is_novel=False)\n",
        "      novel_super_acc, novel_sub_acc = eval_loader(novel_loader, is_novel=True)\n",
        "\n",
        "      balanced_super_acc = (known_super_acc + novel_super_acc) / 2\n",
        "      balanced_sub_acc = (known_sub_acc + novel_sub_acc) / 2\n",
        "\n",
        "      # Check if requirements are met\n",
        "      known_req_met = known_super_acc * 100 >= self.min_known_acc\n",
        "      novel_req_met = novel_super_acc * 100 >= self.min_novel_acc\n",
        "\n",
        "      if known_req_met and novel_req_met:\n",
        "          print(f\"✓ Requirements met: known={known_super_acc*100:.2f}%, novel={novel_super_acc*100:.2f}%\")\n",
        "      else:\n",
        "          print(f\"✗ Requirements not met:\")\n",
        "          if not known_req_met:\n",
        "              print(f\"  Known accuracy {known_super_acc*100:.2f}% < {self.min_known_acc}% requirement\")\n",
        "          if not novel_req_met:\n",
        "              print(f\"  Novel accuracy {novel_super_acc*100:.2f}% < {self.min_novel_acc}% requirement\")\n",
        "\n",
        "      return {\n",
        "          'known_superclass_accuracy': known_super_acc,\n",
        "          'novel_superclass_accuracy': novel_super_acc,\n",
        "          'balanced_superclass_accuracy': balanced_super_acc,\n",
        "          'known_subclass_accuracy': known_sub_acc,\n",
        "          'novel_subclass_accuracy': novel_sub_acc,\n",
        "          'balanced_subclass_accuracy': balanced_sub_acc\n",
        "      }\n",
        "\n",
        "    def _collect_energies(self, model, known_loader, novel_loader):\n",
        "        \"\"\"Collect normalized energy scores for known and novel classes\"\"\"\n",
        "        model.eval()\n",
        "\n",
        "        known_energies = []\n",
        "        novel_energies = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Known classes\n",
        "            for data in known_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                super_outputs, _ = model(inputs)\n",
        "                energies = self._compute_normalized_energy(super_outputs)\n",
        "                known_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "            # Novel classes\n",
        "            for data in novel_loader:\n",
        "                inputs, _, _, _, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                super_outputs, _ = model(inputs)\n",
        "                energies = self._compute_normalized_energy(super_outputs)\n",
        "                novel_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "        return known_energies, novel_energies\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader=None, device='cuda',\n",
        "                min_known_acc=95, min_novel_acc=20):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.device = device\n",
        "\n",
        "        # For energy normalization\n",
        "        self.energy_mean = 0\n",
        "        self.energy_std = 1\n",
        "        self.energy_calibrated = False\n",
        "\n",
        "        # Accuracy requirements\n",
        "        self.min_known_acc = min_known_acc\n",
        "        self.min_novel_acc = min_novel_acc\n",
        "\n",
        "        # Fixed energy threshold for high known accuracy (≥95%) and reasonable novel detection (≥20%)\n",
        "        # This comes from cross-validation\n",
        "        self.energy_threshold = 1.9\n",
        "\n",
        "        # Add scheduler\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "        )\n",
        "\n",
        "        # Store temperature parameter\n",
        "        self.temperature = 1.5\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            inputs, super_labels, sub_labels = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            super_outputs, sub_outputs = self.model(inputs)\n",
        "            loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Training loss: {running_loss/(i+1):.3f}')\n",
        "        avg_loss = running_loss/(i+1)\n",
        "        self.scheduler.step(avg_loss)\n",
        "\n",
        "        # Recalibrate energy statistics after each epoch\n",
        "        self._calibrate_energy_stats()\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "    def _calibrate_energy_stats(self):\n",
        "        \"\"\"Calculate energy statistics on training data for normalization\"\"\"\n",
        "        self.model.eval()\n",
        "        all_energies = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in self.train_loader:\n",
        "                inputs = data[0].to(self.device)\n",
        "\n",
        "                # Get model outputs\n",
        "                super_outputs, _ = self.model(inputs)\n",
        "\n",
        "                # Calculate raw energy\n",
        "                energies = -torch.logsumexp(super_outputs, dim=1)\n",
        "                all_energies.extend(energies.cpu().numpy())\n",
        "\n",
        "        # Compute mean and standard deviation\n",
        "        all_energies = np.array(all_energies)\n",
        "        self.energy_mean = float(np.mean(all_energies))\n",
        "        self.energy_std = float(np.std(all_energies) + 1e-6)  # Add epsilon to avoid division by zero\n",
        "        self.energy_calibrated = True\n",
        "\n",
        "        print(f\"Calibrated energy statistics: mean={self.energy_mean:.4f}, std={self.energy_std:.4f}\")\n",
        "\n",
        "    def compute_normalized_energy(self, logits):\n",
        "        \"\"\"Compute normalized energy scores\"\"\"\n",
        "        # Calculate raw energy\n",
        "        raw_energy = -torch.logsumexp(logits, dim=1)\n",
        "\n",
        "        # Normalize using stored statistics\n",
        "        if not self.energy_calibrated:\n",
        "            # If not calibrated, just return raw energy\n",
        "            print(\"Warning: Energy statistics not calibrated, using raw energy\")\n",
        "            return raw_energy\n",
        "\n",
        "        normalized_energy = (raw_energy - self.energy_mean) / self.energy_std\n",
        "\n",
        "        return normalized_energy\n",
        "\n",
        "    def validate_epoch(self, novel_superclass_idx=3, novel_subclass_idx=87):\n",
        "      \"\"\"\n",
        "      Validate the model with balanced ensemble novelty detection approach\n",
        "      using FIXED threshold of 1.9 for better known accuracy.\n",
        "      \"\"\"\n",
        "      # Make sure energy statistics are calibrated\n",
        "      if not self.energy_calibrated:\n",
        "          self._calibrate_energy_stats()\n",
        "\n",
        "      self.model.eval()\n",
        "\n",
        "      # Metrics to track\n",
        "      correct_with_novelty = 0\n",
        "      super_correct_standard = 0\n",
        "      sub_correct = 0\n",
        "\n",
        "      novel_total = 0\n",
        "      known_total = 0\n",
        "      novel_correct = 0\n",
        "      known_correct = 0\n",
        "\n",
        "      total = 0\n",
        "\n",
        "      novel_super_predictions = 0\n",
        "      novel_sub_predictions = 0\n",
        "\n",
        "      all_super_energies = []\n",
        "      all_sub_confidences = []\n",
        "\n",
        "      running_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for i, data in enumerate(self.val_loader):\n",
        "              inputs, super_labels, _, sub_labels, _ = data\n",
        "              inputs = inputs.to(self.device)\n",
        "              super_labels = super_labels.to(self.device)\n",
        "              sub_labels = sub_labels.to(self.device)\n",
        "\n",
        "              super_outputs, sub_outputs = self.model(inputs)\n",
        "\n",
        "              # Normalized energy scores with FIXED threshold (1.9)\n",
        "              super_energies = self.compute_normalized_energy(super_outputs)\n",
        "              energy_novel = super_energies > self.energy_threshold  # Using fixed threshold 1.9\n",
        "\n",
        "              # Confidence scores\n",
        "              super_probs = F.softmax(super_outputs, dim=1)\n",
        "              super_confidences, super_predicted = torch.max(super_probs, dim=1)\n",
        "\n",
        "              # Confidence threshold can be adjusted\n",
        "              conf_threshold = 0.7\n",
        "              confidence_novel = super_confidences < conf_threshold\n",
        "\n",
        "              # BALANCED APPROACH - weighted voting\n",
        "              # Adjust these weights to control balance\n",
        "              energy_weight = 0.6    # More weight to energy scores\n",
        "              confidence_weight = 0.4 # Less weight to confidence\n",
        "\n",
        "              # Calculate weighted novelty score (0-1 range)\n",
        "              novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "\n",
        "              # Decision threshold - adjust to control sensitivity\n",
        "              decision_threshold = 0.5  # 0.5 is balanced\n",
        "              novel_super_mask = novelty_score > decision_threshold\n",
        "\n",
        "              # Create final predictions\n",
        "              final_super_preds = torch.where(\n",
        "                  novel_super_mask,\n",
        "                  torch.full_like(super_predicted, novel_superclass_idx),\n",
        "                  super_predicted\n",
        "              )\n",
        "\n",
        "              # Subclass confidence-based detection\n",
        "              sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "              sub_confidences, sub_predicted = torch.max(sub_probs, dim=1)\n",
        "              sub_threshold = 0.5\n",
        "              novel_sub_mask = sub_confidences < sub_threshold\n",
        "\n",
        "              final_sub_preds = torch.where(\n",
        "                  novel_sub_mask,\n",
        "                  torch.full_like(sub_predicted, novel_subclass_idx),\n",
        "                  sub_predicted\n",
        "              )\n",
        "\n",
        "              # Count total\n",
        "              total += super_labels.size(0)\n",
        "\n",
        "              # Count correct predictions\n",
        "              correct_with_novelty += (final_super_preds == super_labels).sum().item()\n",
        "              super_correct_standard += (super_predicted == super_labels).sum().item()\n",
        "              sub_correct += (final_sub_preds == sub_labels).sum().item()\n",
        "\n",
        "              # Count novel vs known separately\n",
        "              is_novel_label = super_labels == novel_superclass_idx\n",
        "              novel_total += is_novel_label.sum().item()\n",
        "              known_total += (~is_novel_label).sum().item()\n",
        "\n",
        "              novel_correct += ((final_super_preds == super_labels) & is_novel_label).sum().item()\n",
        "              known_correct += ((final_super_preds == super_labels) & ~is_novel_label).sum().item()\n",
        "\n",
        "              # Count samples predicted as novel\n",
        "              novel_super_predictions += novel_super_mask.sum().item()\n",
        "              novel_sub_predictions += novel_sub_mask.sum().item()\n",
        "\n",
        "              # Store energy and confidence scores\n",
        "              all_super_energies.extend(super_energies.cpu().numpy())\n",
        "              all_sub_confidences.extend(sub_confidences.cpu().numpy())\n",
        "\n",
        "              # Calculate loss\n",
        "              loss = self.criterion(super_outputs, super_labels) + self.criterion(sub_outputs, sub_labels)\n",
        "              running_loss += loss.item()\n",
        "\n",
        "      # Calculate metrics\n",
        "      super_acc = 100 * correct_with_novelty / total if total > 0 else 0\n",
        "      sub_acc = 100 * sub_correct / total if total > 0 else 0\n",
        "\n",
        "      novel_acc = 100 * novel_correct / novel_total if novel_total > 0 else 0\n",
        "      known_acc = 100 * known_correct / known_total if known_total > 0 else 0\n",
        "      balanced_acc = (novel_acc + known_acc) / 2 if novel_total > 0 and known_total > 0 else 0\n",
        "\n",
        "      avg_super_energy = sum(all_super_energies) / len(all_super_energies) if all_super_energies else 0\n",
        "      avg_sub_conf = sum(all_sub_confidences) / len(all_sub_confidences) if all_sub_confidences else 0\n",
        "\n",
        "      novel_super_perc = 100 * novel_super_predictions / total if total > 0 else 0\n",
        "      novel_sub_perc = 100 * novel_sub_predictions / total if total > 0 else 0\n",
        "\n",
        "      # Display metrics\n",
        "      print(f'Validation loss: {running_loss/(i+1):.3f}')\n",
        "      print(f'Validation superclass acc: {super_acc:.2f}%')\n",
        "      print(f'Validation subclass acc: {sub_acc:.2f}%')\n",
        "      print(f'Novel superclass acc: {novel_acc:.2f}%, Known superclass acc: {known_acc:.2f}%')\n",
        "      print(f'Balanced superclass acc: {balanced_acc:.2f}%')\n",
        "      print(f'Average normalized superclass energy: {avg_super_energy:.4f}')\n",
        "      print(f'Average subclass confidence: {avg_sub_conf:.4f}')\n",
        "      print(f'Samples predicted as novel superclass: {novel_super_predictions} ({novel_super_perc:.2f}%)')\n",
        "      print(f'Samples predicted as novel subclass: {novel_sub_predictions} ({novel_sub_perc:.2f}%)')\n",
        "\n",
        "      # Check if requirements are met\n",
        "      requirements_met = known_acc >= self.min_known_acc and novel_acc >= self.min_novel_acc\n",
        "\n",
        "      if requirements_met:\n",
        "          print(f\"✓ REQUIREMENTS MET: known={known_acc:.2f}% ≥ {self.min_known_acc}%, novel={novel_acc:.2f}% ≥ {self.min_novel_acc}%\")\n",
        "      else:\n",
        "          print(f\"✗ REQUIREMENTS NOT MET:\")\n",
        "          if known_acc < self.min_known_acc:\n",
        "              print(f\"  Known accuracy {known_acc:.2f}% < {self.min_known_acc}% requirement\")\n",
        "          if novel_acc < self.min_novel_acc:\n",
        "              print(f\"  Novel accuracy {novel_acc:.2f}% < {self.min_novel_acc}% requirement\")\n",
        "\n",
        "      return {\n",
        "          'loss': running_loss/(i+1),\n",
        "          'accuracy': super_acc,\n",
        "          'novel_acc': novel_acc,\n",
        "          'known_acc': known_acc,\n",
        "          'balanced_acc': balanced_acc\n",
        "      }\n",
        "\n",
        "    def test(self, save_to_csv=False, return_predictions=False, output_file='example_test_predictions.csv'):\n",
        "      \"\"\"\n",
        "      Test the model with fixed threshold of 1.9 for higher known accuracy\n",
        "      \"\"\"\n",
        "      if not self.test_loader:\n",
        "          raise NotImplementedError('test_loader not specified')\n",
        "\n",
        "      # Make sure energy statistics are calibrated\n",
        "      if not self.energy_calibrated:\n",
        "          self._calibrate_energy_stats()\n",
        "\n",
        "      self.model.eval()\n",
        "      novel_superclass_idx = 3  # Index for novel superclass\n",
        "      novel_subclass_idx = 87   # Index for novel subclass\n",
        "\n",
        "      # Create full data structure for internal use\n",
        "      full_test_predictions = {\n",
        "          'image': [],\n",
        "          'superclass_index': [],\n",
        "          'subclass_index': [],\n",
        "          'superclass_energy': [],\n",
        "          'subclass_confidence': [],\n",
        "          'novelty_score': []\n",
        "      }\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for i, data in enumerate(self.test_loader):\n",
        "              inputs, img_name = data[0].to(self.device), data[1]\n",
        "\n",
        "              super_outputs, sub_outputs = self.model(inputs)\n",
        "\n",
        "              # Normalized energy with FIXED threshold (1.9)\n",
        "              super_energies = self.compute_normalized_energy(super_outputs)\n",
        "              energy_novel = super_energies > self.energy_threshold  # Fixed at 1.9\n",
        "\n",
        "              # Confidence scores for superclasses\n",
        "              super_probs = F.softmax(super_outputs, dim=1)\n",
        "              super_confidences, super_predicted = torch.max(super_probs, dim=1)\n",
        "\n",
        "              # Confidence threshold\n",
        "              conf_threshold = 0.7\n",
        "              confidence_novel = super_confidences < conf_threshold\n",
        "\n",
        "              # BALANCED APPROACH - weighted voting\n",
        "              energy_weight = 0.6\n",
        "              confidence_weight = 0.4\n",
        "\n",
        "              novelty_score = energy_weight * energy_novel.float() + confidence_weight * confidence_novel.float()\n",
        "              decision_threshold = 0.5\n",
        "              novel_super_mask = novelty_score > decision_threshold\n",
        "\n",
        "              # Subclass confidence-based detection\n",
        "              sub_probs = F.softmax(sub_outputs, dim=1)\n",
        "              sub_confidences, sub_predicted = torch.max(sub_probs, dim=1)\n",
        "              sub_threshold = 0.5\n",
        "              novel_sub_mask = sub_confidences < sub_threshold\n",
        "\n",
        "              for j in range(inputs.size(0)):\n",
        "                  img = img_name[j] if isinstance(img_name, list) else img_name[0]\n",
        "\n",
        "                  # Apply novelty detection\n",
        "                  super_pred = novel_superclass_idx if novel_super_mask[j] else super_predicted[j].item()\n",
        "                  sub_pred = novel_subclass_idx if novel_sub_mask[j] else sub_predicted[j].item()\n",
        "\n",
        "                  full_test_predictions['image'].append(img)\n",
        "                  full_test_predictions['superclass_index'].append(super_pred)\n",
        "                  full_test_predictions['subclass_index'].append(sub_pred)\n",
        "                  full_test_predictions['superclass_energy'].append(super_energies[j].item())\n",
        "                  full_test_predictions['subclass_confidence'].append(sub_confidences[j].item())\n",
        "                  full_test_predictions['novelty_score'].append(novelty_score[j].item())\n",
        "\n",
        "      # Create complete predictions dataframe\n",
        "      full_predictions_df = pd.DataFrame(data=full_test_predictions)\n",
        "\n",
        "      # Create simplified dataframe with only the columns that match the first method\n",
        "      simplified_test_predictions = {\n",
        "          'image': full_test_predictions['image'],\n",
        "          'superclass_index': full_test_predictions['superclass_index'],\n",
        "          'subclass_index': full_test_predictions['subclass_index']\n",
        "      }\n",
        "      simplified_predictions_df = pd.DataFrame(data=simplified_test_predictions)\n",
        "\n",
        "      # Summarize\n",
        "      novel_super_count = sum(1 for idx in full_test_predictions['superclass_index'] if idx == novel_superclass_idx)\n",
        "      novel_sub_count = sum(1 for idx in full_test_predictions['subclass_index'] if idx == novel_subclass_idx)\n",
        "\n",
        "      total_count = len(full_test_predictions['image'])\n",
        "      novel_super_perc = 100 * novel_super_count / total_count if total_count > 0 else 0\n",
        "      novel_sub_perc = 100 * novel_sub_count / total_count if total_count > 0 else 0\n",
        "\n",
        "      print(f'Test set predictions:')\n",
        "      print(f'Images predicted as novel superclass: {novel_super_count} ({novel_super_perc:.2f}%)')\n",
        "      print(f'Images predicted as novel subclass: {novel_sub_count} ({novel_sub_perc:.2f}%)')\n",
        "\n",
        "      # Print distribution of novelty scores to help with threshold tuning\n",
        "      print(f'Novelty score distribution:')\n",
        "      bins = [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
        "      for i in range(len(bins)-1):\n",
        "          count = sum(1 for score in full_test_predictions['novelty_score']\n",
        "                    if bins[i] <= score < bins[i+1])\n",
        "          print(f'  {bins[i]:.1f}-{bins[i+1]:.1f}: {count} ({100*count/total_count:.2f}%)')\n",
        "\n",
        "      if save_to_csv:\n",
        "          # Save in the same format as the first method\n",
        "          simplified_predictions_df.to_csv(output_file, index=False)\n",
        "          print(f\"Predictions saved to '{output_file}'\")\n",
        "\n",
        "      if return_predictions:\n",
        "          # Return the full predictions for internal use\n",
        "          return full_predictions_df\n",
        "\n",
        "\n",
        "# Helper function to run the cross-validation and find optimal threshold\n",
        "def train_with_novelty_detection(full_dataset, image_preprocessing, device='cuda', batch_size=64, epochs=5,\n",
        "                               min_known_acc=95, min_novel_acc=20):\n",
        "    # Initialize novelty detection trainer with requirements\n",
        "    novelty_trainer = NoveltyDetectionTrainer(\n",
        "        full_dataset=full_dataset,\n",
        "        image_preprocessing=image_preprocessing,\n",
        "        device=device,\n",
        "        batch_size=batch_size,\n",
        "        min_known_acc=min_known_acc,  # Minimum known accuracy requirement\n",
        "        min_novel_acc=min_novel_acc   # Minimum novel accuracy requirement\n",
        "    )\n",
        "\n",
        "    # Run cross-validation to evaluate novelty detection\n",
        "    print(\"Running cross-validation for novelty detection...\")\n",
        "    avg_results, fold_results = novelty_trainer.cross_validate_novelty_detection(epochs=epochs)\n",
        "\n",
        "    # Find optimal threshold\n",
        "    print(\"\\nFinding optimal energy threshold...\")\n",
        "    best_threshold, threshold_results = novelty_trainer.find_optimal_threshold()\n",
        "\n",
        "    return avg_results, best_threshold"
      ],
      "metadata": {
        "id": "QQlV0QbKjY0m"
      },
      "id": "QQlV0QbKjY0m",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run cross-validation to find best threshold\n",
        "avg_results, best_threshold = train_with_novelty_detection(\n",
        "    full_dataset=full_dataset,\n",
        "    image_preprocessing=image_preprocessing,\n",
        "    device=device,\n",
        "    batch_size=64,\n",
        "    epochs=5,\n",
        "    min_known_acc=95,\n",
        "    min_novel_acc=20\n",
        ")\n",
        "\n",
        "print(f\"Cross-validation found best threshold: {best_threshold}\")\n",
        "\n",
        "# Initialize model and trainer for main training\n",
        "model = CNN(input_size=64, num_superclasses=4, num_subclasses=88).to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00005)\n",
        "\n",
        "# Create trainer and SET THE BEST THRESHOLD\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    min_known_acc=95,\n",
        "    min_novel_acc=20\n",
        ")\n",
        "\n",
        "# Important - use the threshold found during cross-validation\n",
        "trainer.energy_threshold = best_threshold\n",
        "print(f\"Training with energy threshold: {trainer.energy_threshold}\")\n",
        "\n",
        "# Train for N epochs\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    trainer.train_epoch()\n",
        "    metrics = trainer.validate_epoch()\n",
        "\n",
        "# Test the final model\n",
        "test_results = trainer.test(save_to_csv=True, output_file='best_threshold_predictions.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBlWOAK-jZ7D",
        "outputId": "94465747-a326-4379-ce63-765399de9b0e"
      },
      "id": "kBlWOAK-jZ7D",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found superclasses with indices: [0, 1, 2]\n",
            "Running cross-validation for novelty detection...\n",
            "\n",
            "=== Fold 1/3: Treating superclass 0 as novel ===\n",
            "Epoch 1/5, Loss: 3.5607\n",
            "Epoch 2/5, Loss: 2.1836\n",
            "Epoch 3/5, Loss: 1.5421\n",
            "Epoch 4/5, Loss: 1.1159\n",
            "Epoch 5/5, Loss: 0.9707\n",
            "Calibrated energy statistics: mean=-5.9108, std=2.5504\n",
            "Calibrated energy statistics: mean=-5.7555, std=2.6191\n",
            "✗ Requirements not met:\n",
            "  Novel accuracy 0.43% < 20% requirement\n",
            "Fold 1 results:\n",
            "  known_superclass_accuracy: 0.9932\n",
            "  novel_superclass_accuracy: 0.0043\n",
            "  balanced_superclass_accuracy: 0.4988\n",
            "  known_subclass_accuracy: 0.6937\n",
            "  novel_subclass_accuracy: 0.6086\n",
            "  balanced_subclass_accuracy: 0.6512\n",
            "\n",
            "=== Fold 2/3: Treating superclass 1 as novel ===\n",
            "Epoch 1/5, Loss: 3.6545\n",
            "Epoch 2/5, Loss: 2.2190\n",
            "Epoch 3/5, Loss: 1.5874\n",
            "Epoch 4/5, Loss: 1.2041\n",
            "Epoch 5/5, Loss: 0.9888\n",
            "Calibrated energy statistics: mean=-7.3367, std=2.8228\n",
            "Calibrated energy statistics: mean=-7.0493, std=2.8571\n",
            "✗ Requirements not met:\n",
            "  Novel accuracy 7.92% < 20% requirement\n",
            "Fold 2 results:\n",
            "  known_superclass_accuracy: 0.9929\n",
            "  novel_superclass_accuracy: 0.0792\n",
            "  balanced_superclass_accuracy: 0.5360\n",
            "  known_subclass_accuracy: 0.7862\n",
            "  novel_subclass_accuracy: 0.4064\n",
            "  balanced_subclass_accuracy: 0.5963\n",
            "\n",
            "=== Fold 3/3: Treating superclass 2 as novel ===\n",
            "Epoch 1/5, Loss: 3.7027\n",
            "Epoch 2/5, Loss: 2.0003\n",
            "Epoch 3/5, Loss: 1.3826\n",
            "Epoch 4/5, Loss: 1.0512\n",
            "Epoch 5/5, Loss: 0.7654\n",
            "Calibrated energy statistics: mean=-7.4715, std=2.6481\n",
            "Calibrated energy statistics: mean=-7.3257, std=2.7530\n",
            "✗ Requirements not met:\n",
            "  Novel accuracy 3.57% < 20% requirement\n",
            "Fold 3 results:\n",
            "  known_superclass_accuracy: 0.9924\n",
            "  novel_superclass_accuracy: 0.0357\n",
            "  balanced_superclass_accuracy: 0.5140\n",
            "  known_subclass_accuracy: 0.8274\n",
            "  novel_subclass_accuracy: 0.4592\n",
            "  balanced_subclass_accuracy: 0.6433\n",
            "\n",
            "=== Cross-Validation Summary ===\n",
            "known_superclass_accuracy: 0.9928\n",
            "novel_superclass_accuracy: 0.0397\n",
            "balanced_superclass_accuracy: 0.5163\n",
            "known_subclass_accuracy: 0.7691\n",
            "novel_subclass_accuracy: 0.4914\n",
            "balanced_subclass_accuracy: 0.6303\n",
            "\n",
            "Finding optimal energy threshold...\n",
            "\n",
            "=== Finding optimal threshold for fold 1: Superclass 0 as novel ===\n",
            "Epoch 1/5, Loss: 3.6182\n",
            "Epoch 2/5, Loss: 2.0559\n",
            "Epoch 3/5, Loss: 1.4437\n",
            "Epoch 4/5, Loss: 1.0891\n",
            "Epoch 5/5, Loss: 0.8621\n",
            "Calibrated energy statistics: mean=-7.1983, std=2.8071\n",
            "Threshold -3.00: Known Acc=0.0045, Novel Acc=1.0000, Balanced Acc=0.5023\n",
            "Threshold -2.90: Known Acc=0.0045, Novel Acc=1.0000, Balanced Acc=0.5023\n",
            "Threshold -2.80: Known Acc=0.0045, Novel Acc=1.0000, Balanced Acc=0.5023\n",
            "Threshold -2.70: Known Acc=0.0045, Novel Acc=1.0000, Balanced Acc=0.5023\n",
            "Threshold -2.60: Known Acc=0.0045, Novel Acc=1.0000, Balanced Acc=0.5023\n",
            "Threshold -2.50: Known Acc=0.0045, Novel Acc=1.0000, Balanced Acc=0.5023\n",
            "Threshold -2.40: Known Acc=0.0045, Novel Acc=1.0000, Balanced Acc=0.5023\n",
            "Threshold -2.30: Known Acc=0.0045, Novel Acc=1.0000, Balanced Acc=0.5023\n",
            "Threshold -2.20: Known Acc=0.0068, Novel Acc=0.9995, Balanced Acc=0.5031\n",
            "Threshold -2.10: Known Acc=0.0158, Novel Acc=0.9989, Balanced Acc=0.5073\n",
            "Threshold -2.00: Known Acc=0.0180, Novel Acc=0.9984, Balanced Acc=0.5082\n",
            "Threshold -1.90: Known Acc=0.0225, Novel Acc=0.9984, Balanced Acc=0.5105\n",
            "Threshold -1.80: Known Acc=0.0338, Novel Acc=0.9978, Balanced Acc=0.5158\n",
            "Threshold -1.70: Known Acc=0.0428, Novel Acc=0.9978, Balanced Acc=0.5203\n",
            "Threshold -1.60: Known Acc=0.0473, Novel Acc=0.9978, Balanced Acc=0.5226\n",
            "Threshold -1.50: Known Acc=0.0631, Novel Acc=0.9978, Balanced Acc=0.5305\n",
            "Threshold -1.40: Known Acc=0.0743, Novel Acc=0.9962, Balanced Acc=0.5353\n",
            "Threshold -1.30: Known Acc=0.0833, Novel Acc=0.9941, Balanced Acc=0.5387\n",
            "Threshold -1.20: Known Acc=0.0991, Novel Acc=0.9924, Balanced Acc=0.5458\n",
            "Threshold -1.10: Known Acc=0.1149, Novel Acc=0.9881, Balanced Acc=0.5515\n",
            "Threshold -1.00: Known Acc=0.1374, Novel Acc=0.9854, Balanced Acc=0.5614\n",
            "Threshold -0.90: Known Acc=0.1622, Novel Acc=0.9822, Balanced Acc=0.5722\n",
            "Threshold -0.80: Known Acc=0.2005, Novel Acc=0.9795, Balanced Acc=0.5900\n",
            "Threshold -0.70: Known Acc=0.2252, Novel Acc=0.9784, Balanced Acc=0.6018\n",
            "Threshold -0.60: Known Acc=0.2748, Novel Acc=0.9735, Balanced Acc=0.6241\n",
            "Threshold -0.50: Known Acc=0.2995, Novel Acc=0.9659, Balanced Acc=0.6327\n",
            "Threshold -0.40: Known Acc=0.3333, Novel Acc=0.9595, Balanced Acc=0.6464\n",
            "Threshold -0.30: Known Acc=0.3761, Novel Acc=0.9476, Balanced Acc=0.6618\n",
            "Threshold -0.20: Known Acc=0.4054, Novel Acc=0.9346, Balanced Acc=0.6700\n",
            "Threshold -0.10: Known Acc=0.4392, Novel Acc=0.9227, Balanced Acc=0.6809\n",
            "Threshold 0.00: Known Acc=0.4797, Novel Acc=0.9043, Balanced Acc=0.6920\n",
            "Threshold 0.10: Known Acc=0.5225, Novel Acc=0.8892, Balanced Acc=0.7059\n",
            "Threshold 0.20: Known Acc=0.5608, Novel Acc=0.8719, Balanced Acc=0.7164\n",
            "Threshold 0.30: Known Acc=0.5968, Novel Acc=0.8530, Balanced Acc=0.7249\n",
            "Threshold 0.40: Known Acc=0.6441, Novel Acc=0.8173, Balanced Acc=0.7307\n",
            "Threshold 0.50: Known Acc=0.6779, Novel Acc=0.7881, Balanced Acc=0.7330\n",
            "Threshold 0.60: Known Acc=0.7117, Novel Acc=0.7649, Balanced Acc=0.7383\n",
            "Threshold 0.70: Known Acc=0.7590, Novel Acc=0.7276, Balanced Acc=0.7433\n",
            "Threshold 0.80: Known Acc=0.7860, Novel Acc=0.6962, Balanced Acc=0.7411\n",
            "Threshold 0.90: Known Acc=0.8221, Novel Acc=0.6470, Balanced Acc=0.7345\n",
            "Threshold 1.00: Known Acc=0.8423, Novel Acc=0.6038, Balanced Acc=0.7231\n",
            "Threshold 1.10: Known Acc=0.8604, Novel Acc=0.5616, Balanced Acc=0.7110\n",
            "Threshold 1.20: Known Acc=0.8716, Novel Acc=0.5124, Balanced Acc=0.6920\n",
            "Threshold 1.30: Known Acc=0.9009, Novel Acc=0.4535, Balanced Acc=0.6772\n",
            "Threshold 1.40: Known Acc=0.9189, Novel Acc=0.3968, Balanced Acc=0.6578\n",
            "Threshold 1.50: Known Acc=0.9347, Novel Acc=0.3254, Balanced Acc=0.6300\n",
            "Threshold 1.60: Known Acc=0.9369, Novel Acc=0.2697, Balanced Acc=0.6033\n",
            "Threshold 1.70: Known Acc=0.9505, Novel Acc=0.2168, Balanced Acc=0.5836\n",
            "Threshold 1.80: Known Acc=0.9527, Novel Acc=0.1546, Balanced Acc=0.5536\n",
            "Threshold 1.90: Known Acc=0.9640, Novel Acc=0.1032, Balanced Acc=0.5336\n",
            "Threshold 2.00: Known Acc=0.9730, Novel Acc=0.0616, Balanced Acc=0.5173\n",
            "Threshold 2.10: Known Acc=0.9775, Novel Acc=0.0346, Balanced Acc=0.5060\n",
            "Threshold 2.20: Known Acc=0.9865, Novel Acc=0.0205, Balanced Acc=0.5035\n",
            "Threshold 2.30: Known Acc=0.9887, Novel Acc=0.0070, Balanced Acc=0.4979\n",
            "Threshold 2.40: Known Acc=1.0000, Novel Acc=0.0038, Balanced Acc=0.5019\n",
            "Threshold 2.50: Known Acc=1.0000, Novel Acc=0.0022, Balanced Acc=0.5011\n",
            "Threshold 2.60: Known Acc=1.0000, Novel Acc=0.0005, Balanced Acc=0.5003\n",
            "Threshold 2.70: Known Acc=1.0000, Novel Acc=0.0000, Balanced Acc=0.5000\n",
            "Threshold 2.80: Known Acc=1.0000, Novel Acc=0.0000, Balanced Acc=0.5000\n",
            "Threshold 2.90: Known Acc=1.0000, Novel Acc=0.0000, Balanced Acc=0.5000\n",
            "\n",
            "Found threshold meeting criteria (known ≥95%, novel ≥20%):\n",
            "Best threshold: 1.70\n",
            "Known accuracy: 95.0450\n",
            "Novel accuracy: 21.6757\n",
            "Balanced accuracy: 58.3604\n",
            "Cross-validation found best threshold: 1.7000000000000046\n",
            "Training with energy threshold: 1.7000000000000046\n",
            "\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 4.063\n",
            "Calibrated energy statistics: mean=-3.4238, std=1.0056\n",
            "Validation loss: 2.935\n",
            "Validation superclass acc: 90.92%\n",
            "Validation subclass acc: 9.71%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 90.92%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.0961\n",
            "Average subclass confidence: 0.2777\n",
            "Samples predicted as novel superclass: 30 (4.78%)\n",
            "Samples predicted as novel subclass: 544 (86.62%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 90.92% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 2/15\n",
            "Training loss: 2.656\n",
            "Calibrated energy statistics: mean=-3.1093, std=1.0720\n",
            "Validation loss: 2.489\n",
            "Validation superclass acc: 90.29%\n",
            "Validation subclass acc: 25.00%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 90.29%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.0265\n",
            "Average subclass confidence: 0.4171\n",
            "Samples predicted as novel superclass: 14 (2.23%)\n",
            "Samples predicted as novel subclass: 423 (67.36%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 90.29% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 3/15\n",
            "Training loss: 2.097\n",
            "Calibrated energy statistics: mean=-3.5182, std=1.0519\n",
            "Validation loss: 2.010\n",
            "Validation superclass acc: 92.36%\n",
            "Validation subclass acc: 40.92%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.36%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1302\n",
            "Average subclass confidence: 0.5551\n",
            "Samples predicted as novel superclass: 34 (5.41%)\n",
            "Samples predicted as novel subclass: 294 (46.82%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 92.36% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 4/15\n",
            "Training loss: 1.800\n",
            "Calibrated energy statistics: mean=-3.7703, std=1.0846\n",
            "Validation loss: 1.899\n",
            "Validation superclass acc: 90.76%\n",
            "Validation subclass acc: 49.68%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 90.76%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1113\n",
            "Average subclass confidence: 0.6079\n",
            "Samples predicted as novel superclass: 37 (5.89%)\n",
            "Samples predicted as novel subclass: 255 (40.61%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 90.76% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 5/15\n",
            "Training loss: 1.610\n",
            "Calibrated energy statistics: mean=-3.4179, std=0.9845\n",
            "Validation loss: 1.721\n",
            "Validation superclass acc: 92.52%\n",
            "Validation subclass acc: 54.46%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.52%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1604\n",
            "Average subclass confidence: 0.6355\n",
            "Samples predicted as novel superclass: 38 (6.05%)\n",
            "Samples predicted as novel subclass: 224 (35.67%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 92.52% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 6/15\n",
            "Training loss: 1.430\n",
            "Calibrated energy statistics: mean=-3.3026, std=0.9202\n",
            "Validation loss: 1.690\n",
            "Validation superclass acc: 91.40%\n",
            "Validation subclass acc: 59.39%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 91.40%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1152\n",
            "Average subclass confidence: 0.6592\n",
            "Samples predicted as novel superclass: 34 (5.41%)\n",
            "Samples predicted as novel subclass: 191 (30.41%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 91.40% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 7/15\n",
            "Training loss: 1.326\n",
            "Calibrated energy statistics: mean=-3.4346, std=1.0886\n",
            "Validation loss: 1.702\n",
            "Validation superclass acc: 92.36%\n",
            "Validation subclass acc: 58.44%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.36%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.1414\n",
            "Average subclass confidence: 0.6850\n",
            "Samples predicted as novel superclass: 29 (4.62%)\n",
            "Samples predicted as novel subclass: 189 (30.10%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 92.36% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 8/15\n",
            "Training loss: 1.230\n",
            "Calibrated energy statistics: mean=-3.6955, std=1.0653\n",
            "Validation loss: 1.572\n",
            "Validation superclass acc: 91.56%\n",
            "Validation subclass acc: 64.81%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 91.56%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.2411\n",
            "Average subclass confidence: 0.7226\n",
            "Samples predicted as novel superclass: 41 (6.53%)\n",
            "Samples predicted as novel subclass: 139 (22.13%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 91.56% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 9/15\n",
            "Training loss: 1.156\n",
            "Calibrated energy statistics: mean=-3.6226, std=1.0353\n",
            "Validation loss: 1.599\n",
            "Validation superclass acc: 91.24%\n",
            "Validation subclass acc: 66.08%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 91.24%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.2111\n",
            "Average subclass confidence: 0.7336\n",
            "Samples predicted as novel superclass: 48 (7.64%)\n",
            "Samples predicted as novel subclass: 132 (21.02%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 91.24% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 10/15\n",
            "Training loss: 1.106\n",
            "Calibrated energy statistics: mean=-3.6176, std=1.0824\n",
            "Validation loss: 1.472\n",
            "Validation superclass acc: 92.99%\n",
            "Validation subclass acc: 67.52%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.99%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.2208\n",
            "Average subclass confidence: 0.7445\n",
            "Samples predicted as novel superclass: 35 (5.57%)\n",
            "Samples predicted as novel subclass: 142 (22.61%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 92.99% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 11/15\n",
            "Training loss: 1.052\n",
            "Calibrated energy statistics: mean=-3.5163, std=1.0140\n",
            "Validation loss: 1.459\n",
            "Validation superclass acc: 92.99%\n",
            "Validation subclass acc: 68.31%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.99%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.2715\n",
            "Average subclass confidence: 0.7290\n",
            "Samples predicted as novel superclass: 36 (5.73%)\n",
            "Samples predicted as novel subclass: 142 (22.61%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 92.99% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 12/15\n",
            "Training loss: 1.014\n",
            "Calibrated energy statistics: mean=-3.4513, std=0.9473\n",
            "Validation loss: 1.618\n",
            "Validation superclass acc: 92.52%\n",
            "Validation subclass acc: 63.22%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 92.52%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.2977\n",
            "Average subclass confidence: 0.7305\n",
            "Samples predicted as novel superclass: 34 (5.41%)\n",
            "Samples predicted as novel subclass: 149 (23.73%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 92.52% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 13/15\n",
            "Training loss: 0.977\n",
            "Calibrated energy statistics: mean=-3.4558, std=0.8894\n",
            "Validation loss: 1.456\n",
            "Validation superclass acc: 91.08%\n",
            "Validation subclass acc: 68.63%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 91.08%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.3259\n",
            "Average subclass confidence: 0.7382\n",
            "Samples predicted as novel superclass: 50 (7.96%)\n",
            "Samples predicted as novel subclass: 136 (21.66%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 91.08% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 14/15\n",
            "Training loss: 0.964\n",
            "Calibrated energy statistics: mean=-3.6606, std=0.9481\n",
            "Validation loss: 1.491\n",
            "Validation superclass acc: 89.65%\n",
            "Validation subclass acc: 67.04%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 89.65%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.3383\n",
            "Average subclass confidence: 0.7424\n",
            "Samples predicted as novel superclass: 62 (9.87%)\n",
            "Samples predicted as novel subclass: 142 (22.61%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 89.65% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "\n",
            "Epoch 15/15\n",
            "Training loss: 0.933\n",
            "Calibrated energy statistics: mean=-3.5067, std=0.8828\n",
            "Validation loss: 1.448\n",
            "Validation superclass acc: 90.61%\n",
            "Validation subclass acc: 67.20%\n",
            "Novel superclass acc: 0.00%, Known superclass acc: 90.61%\n",
            "Balanced superclass acc: 0.00%\n",
            "Average normalized superclass energy: 0.3437\n",
            "Average subclass confidence: 0.7320\n",
            "Samples predicted as novel superclass: 56 (8.92%)\n",
            "Samples predicted as novel subclass: 153 (24.36%)\n",
            "✗ REQUIREMENTS NOT MET:\n",
            "  Known accuracy 90.61% < 95% requirement\n",
            "  Novel accuracy 0.00% < 20% requirement\n",
            "Test set predictions:\n",
            "Images predicted as novel superclass: 2666 (23.85%)\n",
            "Images predicted as novel subclass: 6462 (57.80%)\n",
            "Novelty score distribution:\n",
            "  0.0-0.2: 8044 (71.95%)\n",
            "  0.2-0.4: 0 (0.00%)\n",
            "  0.4-0.5: 470 (4.20%)\n",
            "  0.5-0.6: 0 (0.00%)\n",
            "  0.6-0.8: 1294 (11.57%)\n",
            "  0.8-1.0: 0 (0.00%)\n",
            "Predictions saved to 'best_threshold_predictions.csv'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}